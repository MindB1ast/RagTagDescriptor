





Локальное объяснение интерпретируемой моделью | Машинное и глубокое обучение






[IMAGE]














Перейти к основному содержимому
[IMAGE]
Машинное обучение
Глубокое обучение
Обозначения
Лицензия
Машинное обучение
Введение
Основы машинного обучения
Подготовка данных
Классификаторы в общем виде
Метрические методы прогнозирования
Линейная регрессия и её обобщения
Оценка качества регрессии
Линейная классификация
Многоклассовая классификация набором бинарных классификаторов
Численная оптимизация
Оценка качества классификации
Решающие деревья
Переобучение и недообучение
Ансамбли моделей
Бустинг
Интерпретация простых моделей
Интерпретация сложных моделей
Интерпретация сложных моделей
Анализ ошибок модели
Прогнозы на типичных и нетипичных объектах
Влияние признаков на качество прогнозов
Значения Шепли
Локальное объяснение интерпретируемой моделью
Влияние фрагментов
Зависимость прогноза от признаков
Контрфактические объяснения
Влияние обучающих объектов
Вопросы
Заключение
Интерпретация сложных моделей
Локальное объяснение интерпретируемой моделью
Содержание этой страницы
Локальное объяснение интерпретируемой моделью


Алгоритм LIME
​


Метод 
LIME
 (local interpretable model-agnostic explanations 
[1]
) позволяет объяснить прогноз сложной модели 
$f(\mathbf{x})$
f
(
x
)
 для интересуемого объекта 
$\mathbf{x}$
x
 за счёт 
аппроксимации прогнозов этой модели другой простой и интерпретируемой моделью
 в окрестности точки 
$\mathbf{x}$
x
.




Например, в задаче кредитного скоринга нас может интересовать, почему для определённого клиента кредит не был одобрен. Пусть прогноз возврата кредита осуществляется многослойной нейросетью или сложным ансамблем моделей, поэтому напрямую неинтерпретируем. Но мы можем сгенерировать много синтетических клиентов, похожих на интересующего, посмотреть на прогнозы сложной модели для этих похожих клиентов и настроить 
простую интерпретируемую модель, аппроксимирующую прогнозы сложной модели в окрестности интересуемого клиента
. Простыми и интерпретируемыми моделями могут выступать решающее дерево, линейная модель, метод ближайших центроидов и т.д.




Простая модель поддаётся непосредственной интерпретации, поэтому можно напрямую изучить логику её работы, чтобы делать выводы о работе сложной модели в окрестности интересующего объекта.


Метод LIME работает следующим образом:






Выбрать объект 
$\mathbf{x}$
x
, для которого нужно объяснить прогноз сложной модели 
$f(\mathbf{x})$
f
(
x
)
.






Сгенерировать выборку, состоящую из локальных вариаций 
$\widetilde{\mathbf{x}}_{1},\widetilde{\mathbf{x}}_{2},...\widetilde{\mathbf{x}}_{K}$
x
1
​
,
x
2
​
,
...
x
K
​
 объекта 
$\mathbf{x}$
x
.






Построить для вариаций прогнозы сложной моделью 
$f\left(\cdot\right)$
f
(
⋅
)
, получив выборку:


$\left\{ \left[\widetilde{\mathbf{x}}_{1},f\left(\widetilde{\mathbf{x}}_{1}\right)\right];~\left[\widetilde{\mathbf{x}}_{2},f\left(\widetilde{\mathbf{x}}_{2}\right)\right];~...\left[\widetilde{\mathbf{x}}_{K},f\left(\widetilde{\mathbf{x}}_{K}\right)\right]\right\}$
{
[
x
1
​
,
f
(
x
1
​
)
]
;
 
[
x
2
​
,
f
(
x
2
​
)
]
;
 
...
[
x
K
​
,
f
(
x
K
​
)
]
}






Взвесить объекты по близости к 
$\mathbf{x}$
x
 (чем вариация ближе, тем её вес больше):


$\left\{ \left[w_{1},\widetilde{\mathbf{x}}_{1},f\left(\widetilde{\mathbf{x}}_{1}\right)\right];~\left[w_{2},\widetilde{\mathbf{x}}_{2},f\left(\widetilde{\mathbf{x}}_{2}\right)\right];~...\left[w_{K},\widetilde{\mathbf{x}}_{K},f\left(\widetilde{\mathbf{x}}_{K}\right)\right]\right\}$
{
[
w
1
​
,
x
1
​
,
f
(
x
1
​
)
]
;
 
[
w
2
​
,
x
2
​
,
f
(
x
2
​
)
]
;
 
...
[
w
K
​
,
x
K
​
,
f
(
x
K
​
)
]
}






Настроить интерпретируемую модель 
$g\left(\mathbf{x}\right)$
g
(
x
)
 по взвешенной выборке (чем вес выше, тем объект учитывается 
сильнее
).






Исследовать интерпретируемую модель, аппроксимирующую прогноз сложной модели для точки 
$\mathbf{x}$
x
.






Этапы работы алгоритма визуализированы ниже на графиках A,B,C,D 
[2]
:


[IMAGE]


Процесс сэмплирования объектов, похожих на заданный, зависит от характера объектов:






Для вектора вещественных чисел можно добавлять шум с небольшой дисперсией.






Для текста можно включать/исключать отдельные слова.






Для изображения - включать/исключать отдельные суперпиксели (области соседних пикселей, примерно похожих по цвету 
[3]
).






В качестве простой интерпретируемой модели, аппроксимирующей сложную, обычно используется:






Решающее дерево небольшой глубины.






Линейная модель с сильной 
L1-регуляризацией
.






L1-регуляризация используется для сокращения числа признаков. Альтернативно для этого можно использовать 
OMP-регрессию
 или другой метод 
отбора признаков
, такой как forward-selection (последовательный выбор самых важных признаков) или backward-selection (последовательное исключение самых незначимых).


Контроль ошибки аппроксимации
​


Cтоит помнить, что простая интерпретируемая модель является лишь 
аппроксимацией
 сложной модели, поэтому важно контролировать качество её аппроксимации. Например, для задачи регрессии это может быть ошибка аппроксимации относительно ошибки аппроксимации константой:


$\text{ApproxError}=\sqrt{\frac{\frac{1}{K}\sum_{k=1}^{K}\left(f\left(\mathbf{x}_{k}\right)-g\left(\mathbf{x}_{k}\right)\right)^{2}}{\frac{1}{K}\sum_{k=1}^{K}\left(f\left(\mathbf{x}_{k}\right)-\frac{1}{K}\sum_{k=1}^{K}f\left(\mathbf{x}_{k}\right)\right)^{2}}}$
ApproxError
=
K
1
​
∑
k
=
1
K
​
(
f
(
x
k
​
)
−
K
1
​
∑
k
=
1
K
​
f
(
x
k
​
)
)
2
K
1
​
∑
k
=
1
K
​
(
f
(
x
k
​
)
−
g
(
x
k
​
)
)
2
​
​


Если ошибка аппроксимации высока, нужно либо усложнять аппроксимирующую модель (увеличивая глубину дерева или увеличивая число признаков, ослабляя  L1-регуляризацию), либо уменьшить окрестность сэмплируемых вокруг интересующего объекта 
$\mathbf{x}$
x
 точек (которую при прочих равных лучше брать побольше, чтобы описать поведение исходной модели в более широкой области).


Примеры использования
​


Классификация текста
​


В 
[1]
 представлена интерпретация бинарного классификатора документа из подмножества документов датасета 20-news-groups 
[4]
, относящихся к классам christianity и atheism:


[IMAGE]


Самые значимые слова показаны слева, а начало документа - справа. Хотя классификация корректна, модель использовала нерелевантные слова Posting, Host и Re, не имеющие никакого отношения к темам христианства и атеизма! Подобный анализ позволяет 
выявить переобученные модели
, даже если они показывают хорошее качество прогнозов.


Классификация изображений
​


Метод LIME можно использовать и для интерпретации моделей, классифицирующих изображения по объектам, которые на них показаны. Для этого можно сложную модель, такую как 
Google inception network
, аппроксимировать линейным классификатором, использующим небольшое число суперпикселей. Пример подобной интерпретации для наиболее вероятных классов приведён ниже 
[1]
:


[IMAGE]


Как видим, наиболее сильно влияющие суперпиксели согласуются с классами, и модель не переобучилась.




Метод LIME на python реализован в 
одноимённой библиотеке
. Детальнее о методе можно почитать в 
[2]
, а также в оригинальной статье 
[1]
.


Литература
​






Ribeiro M. T., Singh S., Guestrin C. " Why should i trust you?" Explaining the predictions of any classifier //Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining. – 2016. – С. 1135-1144.






Molnar C. Interpretable machine learning. – Lulu. com, 2020: LIME.






Medium.com: superpixels and SLIC.






UC Irvine Machine Learning Repository: twenty newsgroups dataset.




Предыдущая страница
Значения Шепли
Следующая страница
Влияние фрагментов
Алгоритм LIME
Контроль ошибки аппроксимации
Примеры использования
Классификация текста
Классификация изображений
Литература
© 2023-25 
Виктор Китов.
 
Новости проекта.

