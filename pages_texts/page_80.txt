





Учёт пользовательской функции потерь | Машинное и глубокое обучение






[IMAGE]














Перейти к основному содержимому
[IMAGE]
Машинное обучение
Глубокое обучение
Обозначения
Лицензия
Машинное обучение
Введение
Основы машинного обучения
Подготовка данных
Классификаторы в общем виде
Метрические методы прогнозирования
Линейная регрессия и её обобщения
Оценка качества регрессии
Линейная классификация
Многоклассовая классификация набором бинарных классификаторов
Численная оптимизация
Оценка качества классификации
Решающие деревья
Решающие деревья
Особенности прогнозов решающего дерева
Настройка решающего дерева
Функции неопределённости
Учёт пользовательской функции потерь
Обрезка решающих деревьев
Обработка пропущенных значений
Важность признаков
Анализ решающих деревьев
Обобщения решающих деревьев
Дополнительная литература
Вопросы
Переобучение и недообучение
Ансамбли моделей
Бустинг
Интерпретация простых моделей
Интерпретация сложных моделей
Заключение
Решающие деревья
Учёт пользовательской функции потерь
Содержание этой страницы
Учёт пользовательской функции потерь


Функции неопределённости как минимальные значения потерь
​


Все представленные ранее 
функции неопределённости
 решающих деревьев получаются в результате расчёта 
средних потерь при оптимальном константном прогнозе для этих потерь
.


Пусть 
$I_t$
I
t
​
 - индексы объектов, попавших в узел 
$t$
t
, а 
$|I_t|$
∣
I
t
​
∣
 - количество таких объектов.


Тогда 
дисперсия откликов
 представляет собой минимально возможные 
среднеквадратичные потери
 при константном прогнозе 
$\hat{y}\in\mathbb{R}$
y
^
​
∈
R
. Оптимальной константой, минимизирующей средний квадрат ошибки, будет выборочное среднее (докажите!):


$\begin{aligned}
\phi(t)&=\min_{\widehat{y}}\frac{1}{\left|I_{t}\right|}\sum_{i\in I_{t}}\left(y_{i}-\widehat{y}\right)^{2}\\
&=\frac{1}{\left|I_{t}\right|}\sum_{i\in I_{t}}\left(y_{i}-\text{mean}_{i\in I_{t}}(y_{i})\right)^{2}
\end{aligned}$
ϕ
(
t
)
​
=
y
​
min
​
∣
I
t
​
∣
1
​
i
∈
I
t
​
∑
​
(
y
i
​
−
y
​
)
2
=
∣
I
t
​
∣
1
​
i
∈
I
t
​
∑
​
(
y
i
​
−
mean
i
∈
I
t
​
​
(
y
i
​
)
)
2
​


Среднее абсолютное отклонение от медианы
 минимизирует 
модуль отклонений
 от оптимальной константы 
$\hat{y}\in\mathbb{R}$
y
^
​
∈
R
, в качестве которой выступает медиана (докажите!):


$\begin{aligned}
\phi(t)&=\min_{\widehat{y}}\frac{1}{\left|I_{t}\right|}\sum_{i\in I_{t}}\vert y_{i}-\widehat{y}\vert\\
&=\frac{1}{\left|I_{t}\right|}\sum_{i\in I_{t}}\vert y_{i}-\text{median}_{i\in I_{t}}(y_{i})\vert
\end{aligned}$
ϕ
(
t
)
​
=
y
​
min
​
∣
I
t
​
∣
1
​
i
∈
I
t
​
∑
​
∣
y
i
​
−
y
​
∣
=
∣
I
t
​
∣
1
​
i
∈
I
t
​
∑
​
∣
y
i
​
−
median
i
∈
I
t
​
​
(
y
i
​
)
∣
​




Классификационная ошибка
 минимизирует 
частоту ошибок классификации
, когда класс всегда предсказывается также оптимальной константой. В качестве таковой выступает самый часто встречающийся класс (докажите!):


$\begin{aligned}
\phi(t)&=\min_{\widehat{y}}\frac{1}{\left|I_{t}\right|}\sum_{i\in I_{t}}\mathbb{I}[y_{i}\ne\widehat{y}]\\
       &=\frac{1}{\left|I_{t}\right|}\sum_{i\in I_{t}}\mathbb{I}[y_{i}\ne y_{\text{самый частый}}]\\
       &=1-\hat{p}_{max}
\end{aligned}$
ϕ
(
t
)
​
=
y
​
min
​
∣
I
t
​
∣
1
​
i
∈
I
t
​
∑
​
I
[
y
i
​

=
y
​
]
=
∣
I
t
​
∣
1
​
i
∈
I
t
​
∑
​
I
[
y
i
​

=
y
самый
 
частый
​
]
=
1
−
p
^
​
ma
x
​
​


Энтропия
 представляет собой наилучшее значение 
кросс-энропийных потерь
 (
cross-entropy loss
 между фактическими вероятностями классов и их предсказанными значениями для всех объектов узла 
$t$
t
. Оптимальными вероятностями оказываются при этом фактические частоты классов в узле 
$\widehat{p}_1,\widehat{p}_2,...\widehat{p}_C$
p
​
1
​
,
p
​
2
​
,
...
p
​
C
​
 (докажите!):


$\begin{aligned}
\phi(t)&=\min_{p:\sum_{c}p_{c}=1}-\frac{1}{\left|I_{t}\right|}\left(\sum_{i\in I_{t}}\sum_{c=1}^{C}\ln p_{c}^{\mathbb{I}[y_{i}=c]}\right)\\
&=\min_{p:\sum_{c}p_{c}=1}-\frac{1}{\left|I_{t}\right|}\left(\sum_{i\in I_{t}}\sum_{c=1}^{C}\mathbb{I}[y_{i}=c]\ln p_{c}\right)\\
&=-\sum_{i=1}^{C}\widehat{p}_{i}\ln\widehat{p}_{i}
\end{aligned}$
ϕ
(
t
)
​
=
p
:
∑
c
​
p
c
​
=
1
min
​
−
∣
I
t
​
∣
1
​
(
i
∈
I
t
​
∑
​
c
=
1
∑
C
​
ln
p
c
I
[
y
i
​
=
c
]
​
)
=
p
:
∑
c
​
p
c
​
=
1
min
​
−
∣
I
t
​
∣
1
​
(
i
∈
I
t
​
∑
​
c
=
1
∑
C
​
I
[
y
i
​
=
c
]
ln
p
c
​
)
=
−
i
=
1
∑
C
​
p
​
i
​
ln
p
​
i
​
​


Критерий Джини
 выступает в качестве оптимального значения 
функции потерь Бриера
 между фактическими и предсказываемыми вероятностями. Наилучшими вероятностями также выступают фактические частоты каждого класса среди объектов узла (докажите!):


$\begin{aligned}
\phi(t)&=\min_{p:\sum_{c}p_{c}=1}\frac{1}{\left|I_{t}\right|}\sum_{i\in I_{t}}\left\lVert \mathbf{p}-\mathbf{p}_{i}^{true}\right\rVert ^{2}\\
&=\min_{p:\sum_{c}p_{c}=1}\frac{1}{\left|I_{t}\right|}\sum_{i\in I_{t}}\sum_{c=1}^{C}\left(p_{c}-\mathbb{I}[y_{i}=c]\right)^{2}\\
&=\sum_{i=1}^{C}\widehat{p}_{i}(1-\widehat{p}_{i})=1-\sum_{i=1}^{C}\widehat{p}_{i}^{2}&
\end{aligned}$
ϕ
(
t
)
​
=
p
:
∑
c
​
p
c
​
=
1
min
​
∣
I
t
​
∣
1
​
i
∈
I
t
​
∑
​
​
p
−
p
i
t
r
u
e
​
​
2
=
p
:
∑
c
​
p
c
​
=
1
min
​
∣
I
t
​
∣
1
​
i
∈
I
t
​
∑
​
c
=
1
∑
C
​
(
p
c
​
−
I
[
y
i
​
=
c
]
)
2
=
i
=
1
∑
C
​
p
​
i
​
(
1
−
p
​
i
​
)
=
1
−
i
=
1
∑
C
​
p
​
i
2
​
​
​


Задача
Докажите, что энтропия и критерий Джини минимизируют соответствующие функции потерь. Для этого необходимо воспользоваться методом множителей Лагранжа 
[1]
, поскольку оптимизация по вектору 
$\mathbf{p}$
p
 будет производиться при условии, что 
$\sum_{c=1}^C p_c=1$
∑
c
=
1
C
​
p
c
​
=
1
.


Функции неопределённости для пользовательской функции потерь
​


При минимизации нестандартной функции потерь 
$\mathcal{L}(\hat{y},y)$
L
(
y
^
​
,
y
)
 оптимальной функцией неопределённости 
$\phi(t)$
ϕ
(
t
)
 будет минимальное среднее значение значение 
этой функции потерь
 при константном прогнозе 
$\hat{y}$
y
^
​
:


$\phi_{\text{opt}}(t) = \min_{\hat{y}}\frac{1}{|I_t|}\sum_{i\in I_t}\mathcal{L}(\hat{y},y_i)$
ϕ
opt
​
(
t
)
=
y
^
​
min
​
∣
I
t
​
∣
1
​
i
∈
I
t
​
∑
​
L
(
y
^
​
,
y
i
​
)




Именно такая функция неопределённости будет оптимальна для минимизации потерь 
$\mathcal{L}(\hat{y},y)$
L
(
y
^
​
,
y
)
.




Такой подход, несмотря на его оптимальность, не реализован в большинстве библиотек машинного обучения в связи с тем, что для стандартных функций потерь оптимальный 
$\hat{y}$
y
^
​
 вычислим аналитически и известен заранее, что позволяет рассчитать функцию неопределённости за 
$O(|I_t|)$
O
(
∣
I
t
​
∣
)
, в то время как для произвольной функции потерь необходимо производить каждый раз переборную оптимизацию 
$\hat{y}$
y
^
​
, из-за чего сложность вычисления 
$\phi(t)$
ϕ
(
t
)
 возрастает до 
$O(|I_t|^2)$
O
(
∣
I
t
​
∣
2
)
.




Стоит отметить, что сложность возрастает до квадратичной от числа объектов в узле 
только во время настройки
 дерева. Сложность 
построения прогнозов
 при этом не меняется, поскольку решающие правила узлов уже фиксированы.




Листовые прогнозы для пользовательской функции потерь
​


При использовании пользовательской функции потерь 
$\mathcal{L}(\hat{y},y)$
L
(
y
^
​
,
y
)
 прогнозы в листах также оптимально назначать как минимизаторы 
именно этой функции
:


$\hat{y} = \arg\min_{y}\frac{1}{|I_t|}\sum_{i\in I_t}\mathcal{L}(y,y_i)$
y
^
​
=
ar
g
y
min
​
∣
I
t
​
∣
1
​
i
∈
I
t
​
∑
​
L
(
y
,
y
i
​
)


В частном случае задачи регрессии с 
$\mathcal{L}(\hat{y},y)=(\hat{y}-y)^2$
L
(
y
^
​
,
y
)
=
(
y
^
​
−
y
)
2
 оптимально назначать прогнозом листа выборочное среднее, а при 
$\mathcal{L}(\hat{y},y)=|\hat{y}-y|$
L
(
y
^
​
,
y
)
=
∣
y
^
​
−
y
∣
 медиану (докажите!).


В задаче классификации оптимально назначать прогнозом самый часто встречающийся класс среди объектов листа только для функции потерь 
$\mathcal{L}(\hat{y},y)=\mathbb{I}\{\hat{y}\ne y\}$
L
(
y
^
​
,
y
)
=
I
{
y
^
​

=
y
}
  (докажите!). Для другой функции потерь это правило уже перестаёт быть оптимальным.


Литература
​




Wikipedia: метод множителей Лагранжа.


Предыдущая страница
Функции неопределённости
Следующая страница
Обрезка решающих деревьев
Функции неопределённости как минимальные значения потерь
Функции неопределённости для пользовательской функции потерь
Листовые прогнозы для пользовательской функции потерь
Литература
© 2023-25 
Виктор Китов.
 
Новости проекта.

