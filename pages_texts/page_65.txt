





Метод Ньютона | Машинное и глубокое обучение






[IMAGE]














Перейти к основному содержимому
[IMAGE]
Машинное обучение
Глубокое обучение
Обозначения
Лицензия
Машинное обучение
Введение
Основы машинного обучения
Подготовка данных
Классификаторы в общем виде
Метрические методы прогнозирования
Линейная регрессия и её обобщения
Оценка качества регрессии
Линейная классификация
Многоклассовая классификация набором бинарных классификаторов
Численная оптимизация
Численные методы оптимизации
Метод градиентного спуска
Метод стохастического градиентного спуска
Мониторинг сходимости
Стохастический градиентный спуск с инерцией
Метод Ньютона
Вопросы
Оценка качества классификации
Решающие деревья
Переобучение и недообучение
Ансамбли моделей
Бустинг
Интерпретация простых моделей
Интерпретация сложных моделей
Заключение
Численная оптимизация
Метод Ньютона
Содержание этой страницы
Метод Ньютона


Метод оптимизации Ньютона
 (Newton's optimization method) позволяет ускорить метод 
градиентного спуска
 за счёт использования информации не только о градиенте, 
но и о матрице вторых производных
, называемой 
матрицей Гессе
 (Hessian):


$\nabla^2 L(\mathbf{w})=\begin{pmatrix}
\frac{\partial L(\mathbf{w})}{\partial w_1 \partial w_1} & \cdots & \frac{\partial L(\mathbf{w})}{\partial w_1 \partial w_D} \\
\cdots & \cdots & \cdots \\
\frac{\partial L(\mathbf{w})}{\partial w_D \partial w_1} & \cdots & \frac{\partial L(\mathbf{w})}{\partial w_D \partial w_D}
\end{pmatrix} \in \mathbb{R}^{D \times D}$
∇
2
L
(
w
)
=
​
∂
w
1
​
∂
w
1
​
∂
L
(
w
)
​
⋯
∂
w
D
​
∂
w
1
​
∂
L
(
w
)
​
​
⋯
⋯
⋯
​
∂
w
1
​
∂
w
D
​
∂
L
(
w
)
​
⋯
∂
w
D
​
∂
w
D
​
∂
L
(
w
)
​
​
​
∈
R
D
×
D


Использование вторых производный позволяет ускорить сходимость, поскольку содержит 
информацию о скорости изменения градиента
. Там, где градиент меняется медленно, можно увеличить шаг обучения, а там, где быстро, - замедлить.


Метод Ньютона выглядит следующим образом:




инициализируем настраиваемые веса 
$\mathbf{w}$
w
 случайно


пока не выполнено условие остановки:


             
$\mathbf{w}:=\mathbf{w}-[\nabla_{\mathbf{w}}^2 L(\mathbf{w})]^{-1}\nabla_{\mathbf{w}}L(\mathbf{w})$
w
:=
w
−
[
∇
w
2
​
L
(
w
)
]
−
1
∇
w
​
L
(
w
)




Как видим, за счёт использования информации о вторых производных удалось избавиться от явной спецификации шага обучения - он настраивается автоматически по скорости изменения градиента домножением на матрицу Гессе, состоящую из всех вторых производных функции потерь.




Обратим внимание, что домножение на матрицу подстраивает скорость изменения 
вдоль каждой оси
, используя градиенты 
вдоль всех осей
.




Геометрически метод Ньютона в каждой точке строит параболу (в многомерном пространстве - параболоид), после чего смещает оценку весов в точку минимума этой параболы:


[IMAGE]


Обоснование метода Ньютона
​


Рассмотрим минимизацию дважды дифференцируемой функции потерь 
$L(\mathbf{w})\to\min_{\mathbf{w}}$
L
(
w
)
→
min
w
​


Пусть 
$\mathbf{w}^{*}=\arg\min_{\mathbf{w}}L(\mathbf{w})$
w
∗
=
ar
g
min
w
​
L
(
w
)


Тогда 
$\nabla L(\mathbf{w}^{*})=\mathbf{0}$
∇
L
(
w
∗
)
=
0


Разложение Тейлора 
$\nabla L(\mathbf{w})$
∇
L
(
w
)
 относительно 
$\mathbf{w}$
w
 в точке 
$\mathbf{w}^{*}$
w
∗
:


$\nabla L(\mathbf{w}^{*})=0=\nabla L(\mathbf{w})+\nabla^{2}L(\mathbf{w})(\mathbf{w}^{*}-\mathbf{w})+o(\left\lVert \mathbf{w}-\mathbf{w}^{*}\right\rVert ),$
∇
L
(
w
∗
)
=
0
=
∇
L
(
w
)
+
∇
2
L
(
w
)
(
w
∗
−
w
)
+
o
(
∥
w
−
w
∗
∥
)
,


откуда


$\mathbf{w}^{*}-\mathbf{w}=-\left[\nabla^{2}L(\mathbf{w})\right]^{-1}\nabla L(\mathbf{w})+o(\left\lVert \mathbf{w}-\mathbf{w}^{*}\right\rVert )$
w
∗
−
w
=
−
[
∇
2
L
(
w
)
]
−
1
∇
L
(
w
)
+
o
(
∥
w
−
w
∗
∥
)


Получаем итоговое правило обновления весов, чтобы (приближённо!) переместиться в точку минимума:


$\mathbf{w} := \mathbf{w}-\left[\nabla^{2}L(\mathbf{w})\right]^{-1}\nabla L(\mathbf{w})$
w
:=
w
−
[
∇
2
L
(
w
)
]
−
1
∇
L
(
w
)


При минимизации 
квадратичной функции
 погрешности 
$o(\left\lVert \mathbf{w}-\mathbf{w}^{*}\right\rVert )$
o
(
∥
w
−
w
∗
∥
)
, её квадратичная аппроксимация будет точной, поэтому метод Ньютона сойдётся за один шаг.


Достоинства и недостатки метода
​


Метод Ньютона обладает более высокой скоростью сходимости, чем метод градиентного спуска. В частности, он находит минимум квадратичного функционала всего лишь за одну итерацию. Однако практическому применению этого метода в машинному обучении мешают два аспекта:






Необходимость хранить в памяти матрицу Гессе, размера 
$D\times D$
D
×
D
, а 
$D$
D
 обычно велико при большом числе признаков и при использовании таких перепараметризованных моделей, как нейросети.






Необходимость обращать матрицу Гессе на каждой итерации, что имеет вычислительную сложность 
$O(D^3)$
O
(
D
3
)
.








Детальнее о проблемах применения метода Ньютона в машинном обучении можно прочитать обсуждении 
[1]
. Из-за перечисленных проблем на практике чаще используют квазиньютоновские методы 
[2]
, которые используют вычислительно эффективную аппроксимацию метода Ньютона.


Детальнее о методе Ньютона, условиях сходимости и альтернативных методах второго порядка рекомендуется прочитать в учебнике ШАД 
[3]
.


Литература
​






Stats.stackexchange: Why is Newton's method not widely used in machine learning?






Wikipedia: Quasi-Newton method.






Учебник ШАД: методы второго порядка.




Предыдущая страница
Стохастический градиентный спуск с инерцией
Следующая страница
Вопросы
Обоснование метода Ньютона
Достоинства и недостатки метода
Литература
© 2023-25 
Виктор Китов.
 
Новости проекта.

