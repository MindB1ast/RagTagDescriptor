





Интерпретация логистической регрессии | Машинное и глубокое обучение






[IMAGE]














Перейти к основному содержимому
[IMAGE]
Машинное обучение
Глубокое обучение
Обозначения
Лицензия
Машинное обучение
Введение
Основы машинного обучения
Подготовка данных
Классификаторы в общем виде
Метрические методы прогнозирования
Линейная регрессия и её обобщения
Оценка качества регрессии
Линейная классификация
Многоклассовая классификация набором бинарных классификаторов
Численная оптимизация
Оценка качества классификации
Решающие деревья
Переобучение и недообучение
Ансамбли моделей
Бустинг
Интерпретация простых моделей
Интерпретируемое машинное обучение
Интерпретация метрических методов
Метод наивного Байеса
Интерпретация линейной регрессии
Интерпретация логистической регрессии
Интерпретация решающего дерева
Вопросы
Интерпретация сложных моделей
Заключение
Интерпретация простых моделей
Интерпретация логистической регрессии
Интерпретация логистической регрессии


Рассмотрим модель 
логистической регрессии
 для решения задачи бинарной классификации, когда 
$y\in\left\{ -1,+1\right\}$
y
∈
{
−
1
,
+
1
}
. В модели предполагается, что


$\begin{aligned}
p\left(y=+1|\mathbf{x}\right) &= \frac{1}{1+e^{-\mathbf{w}^{T}\mathbf{x}}} \\
p\left(y=-1|\mathbf{x}\right) &= 1-p\left(y=+1|\mathbf{x}\right) = \frac{1}{1+e^{\mathbf{w}^{T}\mathbf{x}}}
\end{aligned}$
p
(
y
=
+
1∣
x
)
p
(
y
=
−
1∣
x
)
​
=
1
+
e
−
w
T
x
1
​
=
1
−
p
(
y
=
+
1∣
x
)
=
1
+
e
w
T
x
1
​
​


Здесь так же, как и для линейной регрессии, по знаку коэффициента можно судить о направлении влияния признака на прогноз:






признак с положительным коэффициентом увеличивает вероятность положительного класса,






признак с отрицательным коэффициентом - уменьшает.






Величину коэффициента можно проинтерпретировать следующим образом:


$\begin{gathered}
1+e^{-\mathbf{w}^{T}\mathbf{x}}=\frac{1}{p\left(y=+1|\mathbf{x}\right)} \\
e^{-\mathbf{w}^{T}\mathbf{x}}=\frac{1}{p\left(y=+1|\mathbf{x}\right)}-\frac{p\left(y=+1|\mathbf{x}\right)}{p\left(y=+1|\mathbf{x}\right)}=\frac{p\left(y=-1|\mathbf{x}\right)}{p\left(y=+1|\mathbf{x}\right)}\\
e^{\mathbf{w}^{T}\mathbf{x}}=\frac{p\left(y=+1|\mathbf{x}\right)}{p\left(y=-1|\mathbf{x}\right)}=\text{odds ratio}
\end{gathered}$
1
+
e
−
w
T
x
=
p
(
y
=
+
1∣
x
)
1
​
e
−
w
T
x
=
p
(
y
=
+
1∣
x
)
1
​
−
p
(
y
=
+
1∣
x
)
p
(
y
=
+
1∣
x
)
​
=
p
(
y
=
+
1∣
x
)
p
(
y
=
−
1∣
x
)
​
e
w
T
x
=
p
(
y
=
−
1∣
x
)
p
(
y
=
+
1∣
x
)
​
=
odds ratio
​


Последняя величина (отношение вероятностей классов) называется 
odds ratio
, и увеличение i-го признака на 1 приводит к увеличению её значения в 
$e^{w_{i}}$
e
w
i
​
 раз.
Предыдущая страница
Интерпретация линейной регрессии
Следующая страница
Интерпретация решающего дерева
© 2023-25 
Виктор Китов.
 
Новости проекта.

