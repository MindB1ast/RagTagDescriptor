





Специальные меры качества для бинарной классификации | Машинное и глубокое обучение






[IMAGE]














Перейти к основному содержимому
[IMAGE]
Машинное обучение
Глубокое обучение
Обозначения
Лицензия
Машинное обучение
Введение
Основы машинного обучения
Подготовка данных
Классификаторы в общем виде
Метрические методы прогнозирования
Линейная регрессия и её обобщения
Оценка качества регрессии
Линейная классификация
Многоклассовая классификация набором бинарных классификаторов
Численная оптимизация
Оценка качества классификации
Базовые меры качества многоклассовой классификации
Специальные меры качества для бинарной классификации
Обобщение бинарных мер качества на многоклассовый случай
ROC-кривая
Лучший классификатор на ROC кривой
Эквивалентное определение AUC
Контроль качества предсказания вероятностей
Дополнительная литература
Вопросы
Решающие деревья
Переобучение и недообучение
Ансамбли моделей
Бустинг
Интерпретация простых моделей
Интерпретация сложных моделей
Заключение
Оценка качества классификации
Специальные меры качества для бинарной классификации
Содержание этой страницы
Специальные меры качества для бинарной классификации


В случае бинарной классификации 
$y\in\{+1,-1\}$
y
∈
{
+
1
,
−
1
}
, а соответствующие классы называются положительными и отрицательными. Положительному классу обычно сопоставляют более редкий целевой класс, который мы стремимся обнаружить.




Например, при распознавании болезни пациентов по симптомам положительным классом будет наличие заболевания, а отрицательным - отсутствие.




Матрица ошибок будет размера 
$2 \times 2$
2
×
2
 и каждый элемент этой матрицы имеет своё название:


$\hat{y}=+1$
y
^
​
=
+
1
$\hat{y}=-1$
y
^
​
=
−
1
$y=+1$
y
=
+
1
TP (true positives)
FN (false negatives)
$y=-1$
y
=
−
1
FP (false positives)
TN (true negatives)


Второе слово в названии обозначает прогноз, а первое - за его корректность. Например, ложно-положительные объекты (FP штук) - это объекты, ошибочно предсказанные как положительные, в то время как истинный класс был отрицательный. А ложно-отрицательные объекты (FN штук) были предсказаны как отрицательные, в то время как на самом деле они принадлежали положительному классу.


Как по значениям TP,TN,FP, FN вычислить точность и частоту ошибок классификации?
$\text{accuracy}=\frac{TP+TN}{N}=\frac{TP+TN}{TP+TN+FP+FN}$
accuracy
=
N
TP
+
TN
​
=
TP
+
TN
+
FP
+
FN
TP
+
TN
​
$\text{error rate}=\frac{FP+FN}{N}=\frac{FP+FN}{TP+TN+FP+FN}$
error rate
=
N
FP
+
FN
​
=
TP
+
TN
+
FP
+
FN
FP
+
FN
​


Меры качества для несбалансированных классов
​


Точность и полнота
​


В случае 
несбалансированных классов
 (unbalanced classes), когда положительный класс встречается 
существенно реже
, чем отрицательный, предложенные меры недостаточны для оценки модели!




Например, если положительный класс встречается в 1% случаев, а отрицательный - в оставшихся 99%, то константный прогноз, всегда назначающий отрицательный класс, будет показывать точность 99%, а частоту ошибок - всего 1%. Однако это никак не будет свидетельствовать об адекватности модели, поскольку она даже не пыталась выделить положительный класс!




Поэтому для таких ситуаций используются специальные меры качества - 
точность
 (precision, не путать с 
accuracy
!) и 
полнота
 (recall):


$\text{precision}=\frac{TP}{\hat{N}_+}=\frac{TP}{TP+FP}$
precision
=
N
^
+
​
TP
​
=
TP
+
FP
TP
​


$\text{recall}=\frac{TP}{N_+}=\frac{TP}{TP+FN}$
recall
=
N
+
​
TP
​
=
TP
+
FN
TP
​


где мы использовали обозначения:






$N_+=TP+FN$
N
+
​
=
TP
+
FN
 - общее число положительный объектов;






$\hat{N}_+=TP+FP$
N
^
+
​
=
TP
+
FP
 - общее число объектов, 
предсказанных
 как положительные.






Precision показывает долю верно-положительных объектов 
среди всех объектов, предсказанных как положительные
. Например, при классификации болезни - это доля действительно больных пациентов среди всех предсказанных как больные. Precision важен, если мы хотим минимизировать число ложных срабатываний классификатора (предсказаний болезни для здоровых пациентов).


Recall показывает долю верно-положительных объектов среди всех объектов, 
в действительности принадлежащих положительному классу
. В примере выше recall важен, если мы хотим обнаружить всех больных пациентов, пусть и с некоторой долей ложных срабатываний.


Таким образом, precision и recall измеряют различные аспекты качества модели.


F-мера
​


На практике важен и precision, и recall, поэтому считают их среднее гармоническое 
[1]
, называемое F-мерой (F-measure, 
$F_1$
F
1
​
-score):


$F_1\text{-score}=\frac{1}{\frac{1}{2}\frac{1}{\text{Precision}}+\frac{1}{2}\frac{1}{\text{Recall}}}=\frac{2\cdot \text{Precision}\times\text{Recall}}{\text{Precision}+\text{Recall}}$
F
1
​
-score
=
2
1
​
Precision
1
​
+
2
1
​
Recall
1
​
1
​
=
Precision
+
Recall
2
⋅
Precision
×
Recall
​


Почему не используют среднее арифметическое?
Преимущество F-меры по сравнению с обычным усреднением заключается в том, что F-мера будет штрафовать как низкие значения precision, так и низкие значения recall 
одновременно
. В частности, она будет равна нулю, если 
хотя бы один
 из показателей равен нулю.


Обычное же усреднение будет давать 0.5 в случае плохо настроенной модели, когда






Precision 
$\approx$
≈
 1, Recall = 0 (когда только одного пациента, в болезни которого мы максимально уверены, считаем больным);






Precision 
$\approx$
≈
 0, Recall = 1 (когда назначаем больными всех пациентов без разбора).






Будет ли этому свойству удовлетворять среднее геометрическое?
Да, будет, поскольку зависит от произведения этих величин.


Взвешенная F-мера
​


На практике точность и полнота имеют 
разную важность
 для конечной задачи. При идентификации болезни по симптомам важнее полнота (хотим отпустить минимальное число больных с диагнозом "здоров"), а в интернет-поиске важнее точность (хотим вернуть в поисковой выдаче только те страницы, которые действительно релевантны поисковому запросу, пусть и не все релевантные, поскольку их слишком много). Поэтому используется 
взвешенная F-мера
 (weighted F-measure, 
$F_{\beta}$
F
β
​
-score):


$F_\beta\text{-score}=\frac{1}{\frac{\beta^2}{1+\beta^2}\frac{1}{\text{Precision}}+\frac{1}{1+\beta^2}\frac{1}{\text{Recall}}}=(1+\beta^2)\frac{\text{Precision}\times\text{Recall}}{\beta^2\text{Precision}+\text{Recall}}$
F
β
​
-score
=
1
+
β
2
β
2
​
Precision
1
​
+
1
+
β
2
1
​
Recall
1
​
1
​
=
(
1
+
β
2
)
β
2
Precision
+
Recall
Precision
×
Recall
​


Более высокое значение 
$\beta$
β
 будет повышать значение Precision и занижать вклад Recall при агрегации.


Использование мер качества в ранжировании
​


Классификатор часто используется не для прогнозов меток классов, а для сортировки объектов по степени уверенности модели в том, что они принадлежат положительному классу.




Например, компания в первую очередь обзванивает тех клиентов, кому спецпреложение будет максимально интересно. В информационном поиске документы соритруются от более релевантных к менее релевантным.




Поскольку каждый бинарный классификатор 
представим в виде
:


$\hat{y}(\mathbf{x})=\text{sign}(g(\mathbf{x})),$
y
^
​
(
x
)
=
sign
(
g
(
x
))
,


то относительная дискриминантная функция 
$g(\mathbf{x})$
g
(
x
)
 как раз и будет служить оценкой уверенности классификатора в положительном классе для объекта 
$\mathbf{x}$
x
, по которой можно отранжировать объекты. Рассмотрим для определённости информационный поиск, в котором по запросу пользователя возвращаются релевантные документы. В подобных задачах задают некоторый порог 
$K$
K
 (предельное число документов, которые пользователь просмотрит в поисковой выдаче), после чего считают меры:






precision@K - долю релевантных документов среди первых 
$K$
K
 в выдаче;






recall@K - долю показанных релевантных документов среди всех релевантных.






Далее можно по ним можно считать взвешенную F-меру. Примеры расчётов precision@K и recall@K можно прочитать в 
[2]
.


Недостатком precision@K и recall@K является то, что эти меры 
никак не зависят
 от порядка следования корректных классификаций среди 
$K$
K
 представленных. А нам бы хотелось, чтобы релевантные объекты (положительного класса) шли именно в начале списка. Мерой, поощряющей выдачу релевантных объектов именно в начале отранжированного списка является 
average precision
.


Average Precision
​


Если варьировать K, то получим 
зависимость точности от полноты
 (precision-recall curve), пример которой приведён ниже:


[IMAGE]


Мы бы хотели получить выше precision при каждом уровне recall, поэтому чем выше этот график, тем качественнее работает классификатор. Агрегированной мерой качества классификации служит площадь под графиком зависимости точности от полноты, которая называется 
Average Precision
 (AP). Она поощряет ситуацию, когда при ранжировании объектов по степени принадлежности положительному классу 
в начале списка идут именно представители положительного класса
.




В случае информационного поиска это будут документы, релевантные поисковому запросу. Поскольку нас интересует качество ранжирующей системы для разных пользователей и поисковых запросов, то величины Average Precision 
усредняют по пользователям и запросам
, получая величину 
Mean Average Precision
.




Пример расчёта Average Precision и Mean Average Precision можно посмотреть в 
[3]
.


Литература
​






Википедия: среднее гармоническое.






EvidentlyAI: Precision and recall at K in ranking and recommendations.






EvidentlyAI: Mean Average Precision (MAP) in ranking and recommendations.




Предыдущая страница
Базовые меры качества многоклассовой классификации
Следующая страница
Обобщение бинарных мер качества на многоклассовый случай
Меры качества для несбалансированных классов
Точность и полнота
F-мера
Взвешенная F-мера
Использование мер качества в ранжировании
Average Precision
Литература
© 2023-25 
Виктор Китов.
 
Новости проекта.

