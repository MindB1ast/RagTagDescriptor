





Ансамбли рандомизированных деревьев | Машинное и глубокое обучение






[IMAGE]














Перейти к основному содержимому
[IMAGE]
Машинное обучение
Глубокое обучение
Обозначения
Лицензия
Машинное обучение
Введение
Основы машинного обучения
Подготовка данных
Классификаторы в общем виде
Метрические методы прогнозирования
Линейная регрессия и её обобщения
Оценка качества регрессии
Линейная классификация
Многоклассовая классификация набором бинарных классификаторов
Численная оптимизация
Оценка качества классификации
Решающие деревья
Переобучение и недообучение
Ансамбли моделей
Ансамбли моделей
Математическое обоснование ансамблей
Простая агрегация в ансамблях
Методы построения базовых моделей
Настройка на разных фрагментах обучающей выборки
Ансамбли рандомизированных деревьев
Стэкинг
Дополнительная литература
Вопросы
Бустинг
Интерпретация простых моделей
Интерпретация сложных моделей
Заключение
Ансамбли моделей
Ансамбли рандомизированных деревьев
Содержание этой страницы
Ансамбли рандомизированных деревьев


Как известно из 
разложения неоднозначности
 (ambiguity decomposition), ансамбли моделей дают более точные прогнозы, если базовые модели оказываются 
более непохожими друг на друга
, производя более рассогласованные прогнозы. Когда строится ансамбль над решающими деревьями, то для повышения разнообразия предлагается строить деревья 
рандомизированным способом
, то есть включая элемент случайности при их построении. На этом основаны алгоритмы 
решающего леса
 (random forest) и 
особо случайных деревьев
 (extra random trees), представляющие собой сильные базовые решения (baseline) для работы с табличными данными.


Настройка решающих правил в деревьях
​


При 
настройке классических решающих деревьев
 в каждом внутреннем узле нужно выбрать правило вида 
$x^i\le h$
x
i
≤
h
: если 
$i$
i
-й признак больше порога 
$h$
h
, то происходит переход в правую дочернюю вершину, иначе - в левую. Выбор номера признака и порога в узле 
$t$
t
 осуществляется по правилу:


$\widehat{i},\widehat{h}=\arg\max_{f,h\in P(t)}\Delta\phi(t),  \tag{1}$
i
,
h
=
ar
g
f
,
h
∈
P
(
t
)
max
​
Δ
ϕ
(
t
)
,
(
1
)


где 
$\Delta \phi(t)$
Δ
ϕ
(
t
)
 - изменение функции неопределённости в вершине 
$t$
t
 после разбиения.


Обычное дерево
​


В формуле (1) 
$P(t)$
P
(
t
)
 представляет собой набор пар (признак, порог), формирующийся перебором всех возможных пар (признак, порог):




$P=\{\}$
P
=
{
}
 
для каждого 
$i$
i
 in 
$\{1,...,D\}$
{
1
,
...
,
D
}
 




для каждого 
$h$
h
 из 
$\text{unique}\left\{ x_{n}^{i}\right\} _{n:\mathbf{x}_{n\in t}}$
unique
{
x
n
i
​
}
n
:
x
n
∈
t
​
​




$P:=P\cup(i,h)$
P
:=
P
∪
(
i
,
h
)








Оператор 
$\text{unique}\left\{ x_{n}^{i}\right\} _{n:\mathbf{x}_{n}\in t}$
unique
{
x
n
i
​
}
n
:
x
n
​
∈
t
​
 возвращает все уникальные значения признака 
$x^i$
x
i
 для объектов, попавших в узел 
$t$
t
.


Настройка правил в случайном лесе
​


Случайный лес
 (random forest 
[1]
) строится как бэггинг над решающими деревьями, но правила в каждом узле (1) каждого дерева настраиваются 
по случайному подмножеству признаков и всем допустимым порогам для них
:




$P=\{\}$
P
=
{
}
, 
$K=\alpha D$
K
=
α
D
 
сэмплируем без возвращения 
${i_1,...i_K}$
i
1
​
,
...
i
K
​
 случайно из 
$\{1,...,D\}$
{
1
,
...
,
D
}
 
для каждого 
$i$
i
 из 
$\{i_1,...i_K\}$
{
i
1
​
,
...
i
K
​
}
:




для каждого 
$h$
h
 из 
$\text{unique}\left\{ x_{n}^{i}\right\}_{n:\mathbf{x}_{n}\in t}$
unique
{
x
n
i
​
}
n
:
x
n
​
∈
t
​
:




$P:=P\cup(i,h)$
P
:=
P
∪
(
i
,
h
)








$\alpha\in (0,1]$
α
∈
(
0
,
1
]
 - гиперпараметр метода, характеризующий долю признаков, по которым происходит настройка правила в узле.




Например, при 
$\alpha=0.5$
α
=
0.5
 для каждого узла дерева будет сэмплироваться только половина всех признаков, по которым будет настраиваться правило (1).




Обратим внимание, что в каждом узле дерева сэмплируется своё случайное подмножество признаков, поэтому дерево как целое может зависеть от всех признаков. Это отличает деревья случайного леса от деревьев в 
методе случайных подпространств
 (random subspaces), в котором каждое дерево целиком будет зависеть от случайного подмножества признаков.




Гиперпараметр 
$\alpha$
α
 - 
основной гиперпараметр случайного леса
, контролирующий выразительную сложность модели!




Чем ниже 
$\alpha$
α
, тем меньше свободы у каждого решающего дерева в подборе оптимального правила, и тем оно получается менее гибким и смещённым (недообученным). Зато прогнозы деревьев получаются менее похожими за счёт большего разнообразия правил, по которым строятся решающие деревья.


Если же увеличивать 
$\alpha$
α
, то деревья гибче подстраиваются под данные и становятся более переобученными и менее разнообразными.


Оптимальная точность ансамбля достигается при некотором балансе между точностью отдельных моделей и разнообразием их прогнозов.


Нужен ли бэггинг?
В силу случайности, заложенной в построение каждого дерева случайного леса, использование 
бэггинга
 становится необязательным, так как даже на одинаковых выборках базовые алгоритмы будут получаться различными. Поэтому бэггинг можно как использовать, так и нет, в зависимости от того, какой вариант себя лучше покажет на валидационной выборке.


Настройка правил в особо случайных деревьях
​


Особо случайные деревья
 (extra trees, extremely randomized trees 
[2]
) также представляют собой бэггинг над решающими деревьями. Но если в деревьях решающего леса правила настраивались по подмножеству признаков 
и всем допустимым для них порогам
, то в особо случайном дереве 
пороги сэмплируются случайно вместе с признаками
, и настройка по порогу не производится. Это делает оптимизацию правил в каждом узле дерева еще более ограниченной, в результате чего деревья получаются ещё менее гибкими, зато дополнительно повышается их разнообразие.


Ниже приведён псевдокод для формирования пар (признак, порог), по которым производится настройка решающих правил в узле по правилу (1) каждого особо случайного дерева:




$S=\{\}$
S
=
{
}
, 
$K=\alpha D$
K
=
α
D
 
сэмплируем без возвращения 
${i_1,...i_K}$
i
1
​
,
...
i
K
​
 случайно из 
$\{1,...,D\}$
{
1
,
...
,
D
}
 
для каждого 
$i$
i
 из 
$\{i_1,...i_K\}$
{
i
1
​
,
...
i
K
​
}
:




сэмплируем 
$h$
h
 случайно без возвращения из 
$\text{unique}\left\{ x_{n}^{i}\right\}_{n:\mathbf{x}_{n}\in t}$
unique
{
x
n
i
​
}
n
:
x
n
​
∈
t
​
 


$P:=P\cup(f,h)$
P
:=
P
∪
(
f
,
h
)






Так же, как и в случае решающего леса, 
$\alpha \in (0,1]$
α
∈
(
0
,
1
]
 - главный гиперпараметр метода, контролирующий сложность модели.




Поскольку деревья используют случайность при настройке, они будут получаться разнообразными даже при перенастройке на одной и той же обучающей выборке, поэтому процедура бэггинга для генерации разных обучающих псевдовыборок необязательна. Использовать её или нет определяется тем, какой вариант лучше себя покажет на валидационной выборке.




Пример запуска на Python
​


Случайный лес для классификации:


from
 sklearn
.
ensemble 
import
 RandomForestClassifier
from
 sklearn
.
metrics 
import
 accuracy_score
from
 sklearn
.
metrics 
import
 brier_score_loss
X_train
,
 X_test
,
 Y_train
,
 Y_test 
=
 get_demo_classification_data
(
)
  
# инициализация модели:
model 
=
 RandomForestClassifier
(
n_estimators
=
100
,
  
# число базовых моделей 
                               bootstrap
=
True
,
    
# обучать на подвыборках или всей выборке 
                               max_features
=
0.5
,
  
# доля случайных признаков для обучения 
                               n_jobs
=
-
1
)
         
# использовать все ядра процессора  
model
.
fit
(
X_train
,
 Y_train
)
    
# обучение модели   
Y_hat 
=
 model
.
predict
(
X_test
)
  
# построение прогнозов
print
(
f'Точность прогнозов: 
{
100
*
accuracy_score
(
Y_test
,
 Y_hat
)
:
.1f
}
%'
)
P_hat 
=
 model
.
predict_proba
(
X_test
)
  
# можно предсказывать вероятности классов
loss 
=
 brier_score_loss
(
Y_test
,
 P_hat
[
:
,
1
]
)
  
# мера Бриера на вероятности положительного класса
print
(
f'Мера Бриера ошибки прогноза вероятностей: 
{
loss
:
.2f
}
'
)






Случайный лес для регрессии:


from
 sklearn
.
ensemble 
import
 RandomForestRegressor
from
 sklearn
.
metrics 
import
 mean_absolute_error
X_train
,
 X_test
,
 Y_train
,
 Y_test 
=
 get_demo_regression_data
(
)
# инициализация модели:
model 
=
 RandomForestRegressor
(
n_estimators
=
100
,
  
# число базовых моделей
                              bootstrap
=
True
,
    
# обучать на подвыборках или всей выборке
                              max_features
=
0.5
,
  
# доля случайных признаков для обучения
                              n_jobs
=
-
1
)
         
# использовать все ядра процессора
model
.
fit
(
X_train
,
 Y_train
)
      
# обучение модели   
Y_hat 
=
 model
.
predict
(
X_test
)
    
# построение прогнозов
print
(
f'Средний модуль ошибки (MAE): 
{
mean_absolute_error
(
Y_test
,
 Y_hat
)
:
.2f
}
'
)
  




Больше информации
. 
Полный код
.


Особо случайные деревья для классификации:


from
 sklearn
.
ensemble 
import
 ExtraTreesClassifier
from
 sklearn
.
metrics 
import
 accuracy_score
from
 sklearn
.
metrics 
import
 brier_score_loss
X_train
,
 X_test
,
 Y_train
,
 Y_test 
=
 get_demo_classification_data
(
)
  
# инициализация модели:
model 
=
 ExtraTreesClassifier
(
n_estimators
=
100
,
  
# число базовых моделей 
                             bootstrap
=
True
,
    
# обучать на подвыборках или всей выборке 
                             max_features
=
1.0
,
  
# доля случайных признаков для обучения 
                             n_jobs
=
-
1
)
         
# использовать все ядра процессора 
model
.
fit
(
X_train
,
 Y_train
)
       
# обучение модели   
Y_hat 
=
 model
.
predict
(
X_test
)
     
# построение прогнозов
print
(
f'Точность прогнозов: 
{
100
*
accuracy_score
(
Y_test
,
 Y_hat
)
:
.1f
}
%'
)
P_hat 
=
 model
.
predict_proba
(
X_test
)
  
# можно предсказывать вероятности классов
loss 
=
 brier_score_loss
(
Y_test
,
 P_hat
[
:
,
1
]
)
  
# мера Бриера на вероятности положительного класса
print
(
f'Мера Бриера ошибки прогноза вероятностей: 
{
loss
:
.2f
}
'
)






Особо случайные деревья для регрессии:


from
 sklearn
.
ensemble 
import
 ExtraTreesRegressor
from
 sklearn
.
metrics 
import
 mean_absolute_error
X_train
,
 X_test
,
 Y_train
,
 Y_test 
=
 get_demo_regression_data
(
)
 
# инициализация модели:
model 
=
 ExtraTreesRegressor
(
n_estimators
=
100
,
  
# число базовых моделей  
                            bootstrap
=
True
,
    
# обучать на подвыборках или всей выборке  
                            max_features
=
0.5
,
  
# доля случайных признаков для обучения
                            n_jobs
=
-
1
)
         
# использовать все ядра процессора
model
.
fit
(
X_train
,
 Y_train
)
       
# обучение модели   
Y_hat 
=
 model
.
predict
(
X_test
)
     
# построение прогнозов
print
(
f'Средний модуль ошибки (MAE): 
{
mean_absolute_error
(
Y_test
,
 Y_hat
)
:
.2f
}
'
)
  




Больше информации
. 
Полный код
.


Литература
​






Breiman L. Random forests //Machine learning. – 2001. – Т. 45. – С. 5-32.






Geurts P., Ernst D., Wehenkel L. Extremely randomized trees //Machine learning. – 2006. – Т. 63. – С. 3-42.




Предыдущая страница
Настройка на разных фрагментах обучающей выборки
Следующая страница
Стэкинг
Настройка решающих правил в деревьях
Обычное дерево
Настройка правил в случайном лесе
Настройка правил в особо случайных деревьях
Пример запуска на Python
Литература
© 2023-25 
Виктор Китов.
 
Новости проекта.

