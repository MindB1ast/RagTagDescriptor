





Метод наивного Байеса | Машинное и глубокое обучение






[IMAGE]














Перейти к основному содержимому
[IMAGE]
Машинное обучение
Глубокое обучение
Обозначения
Лицензия
Машинное обучение
Введение
Основы машинного обучения
Подготовка данных
Классификаторы в общем виде
Метрические методы прогнозирования
Линейная регрессия и её обобщения
Оценка качества регрессии
Линейная классификация
Многоклассовая классификация набором бинарных классификаторов
Численная оптимизация
Оценка качества классификации
Решающие деревья
Переобучение и недообучение
Ансамбли моделей
Бустинг
Интерпретация простых моделей
Интерпретируемое машинное обучение
Интерпретация метрических методов
Метод наивного Байеса
Интерпретация линейной регрессии
Интерпретация логистической регрессии
Интерпретация решающего дерева
Вопросы
Интерпретация сложных моделей
Заключение
Интерпретация простых моделей
Метод наивного Байеса
Содержание этой страницы
Метод наивного Байеса


Генеративные и дискриминативные модели
​


Прогностические модели машинного обучения бывают генеративные и дискриминативные.


Дискриминативные модели
 (discriminative models) моделируют только 
условное распределение отклика при условии вектора признаков
 
$p(y|\mathbf{x})$
p
(
y
∣
x
)
.


Генеративные модели
 (generative models) моделируют 
совместное распределение признаков и откликов
 
$p(\mathbf{x},y)$
p
(
x
,
y
)
. Из генеративной модели можно легко получить дискриминативную по правилу условных вероятностей:


$p\left(y|\mathbf{x}\right)=\frac{p(\mathbf{x},y)}{p(\mathbf{x})}=\frac{p\left(y\right)p\left(\mathbf{x}|y\right)}{p\left(\mathbf{x}\right)}\propto p\left(y\right)p\left(\mathbf{x}|y\right),$
p
(
y
∣
x
)
=
p
(
x
)
p
(
x
,
y
)
​
=
p
(
x
)
p
(
y
)
p
(
x
∣
y
)
​
∝
p
(
y
)
p
(
x
∣
y
)
,


где 
$\propto$
∝
 означает пропорционально с точностью до константы, которой выступает деление на 
$p(\mathbf{x})$
p
(
x
)
, не зависящей от класса, а потому не влияющей на прогноз его метки.


Идея метода наивного Байеса
​


Метод наивного Байеса
 (naive Bayes) представляет собой генеративную модель классификации, в которой дополнительно используется 
предположение наивного Байеса
 (naive Bayes assumption), состоящее в том, что 
признаки предполагаются распределёнными независимо при условии отклика
:


$p(\mathbf{x}|y) = p\left(x^{1}|y\right)p\left(x^{2}|y\right)...p\left(x^{D}|y\right)$
p
(
x
∣
y
)
=
p
(
x
1
∣
y
)
p
(
x
2
∣
y
)
...
p
(
x
D
∣
y
)




Например, в задаче классификации писем на полезные и спам по встречаемости разных слов в письме это предположение означает, что 
при условии класса письма
 (полезное или спам) встречаемости слов независимы друг от друга.




Предположение наивного Байеса на практике не выполняется, зато оно позволяет существенно упростить оценку многомерного распределения 
$p(\mathbf{x}|y)$
p
(
x
∣
y
)
, сведя его к оценке набора одномерных распределений! В результате снижается дисперсия оценки сложной модели за счёт её аппроксимации более простой моделью, имеющей более высокое смещение в терминах 
разложения на смещение и разброс
.


Условная и безусловная независимость
Условная независимость 
не эквивалентна
 безусловной независимости, записываемой как:
$p(\mathbf{x}) = p\left(x^{1}\right)p\left(x^{2}\right)...p\left(x^{D}\right)$
p
(
x
)
=
p
(
x
1
)
p
(
x
2
)
...
p
(
x
D
)


Например, в задаче классификации писем на полезные и спам безусловная независимость означает, что P("скидка" и "купи") = P("скидка") × P("купи"). Однако эти два слова однозначно зависимы: при появлении слова "купи" с большой вероятностью встретится и слово "скидка"!


Предположение же условной независимости признаёт, что вероятности слов могут синхронно меняться с классом письма (спам или полезное), однако 
при условии выбранного класса
 присутствие одного из слов уже не влияет на вероятность встречи другого.




Сделав предположение наивного Байеса, получим итоговый вид 
дискриминантных функций классификатора
:


$p\left(y|\mathbf{x}\right)=\frac{p\left(y\right)p\left(\mathbf{x}|y\right)}{p\left(\mathbf{x}\right)}\propto p\left(y\right)p\left(\mathbf{x}|y\right)=p\left(y\right)p\left(x^{1}|y\right)p\left(x^{2}|y\right)...p\left(x^{D}|y\right)$
p
(
y
∣
x
)
=
p
(
x
)
p
(
y
)
p
(
x
∣
y
)
​
∝
p
(
y
)
p
(
x
∣
y
)
=
p
(
y
)
p
(
x
1
∣
y
)
p
(
x
2
∣
y
)
...
p
(
x
D
∣
y
)


Прогноз осуществляется назначением класса, обладающего максимальным значением дискриминантной функции.


Интерпретация
​


Если общее число признаков 
$D$
D
 невелико, то метод наивного Байеса интерпретируем, поскольку вклад каждого признака вносится мультипликативно, и для аномально высокой и низкой вероятности можно отследить, из-за каких признаков и соответствующих слагаемых 
$p\left(x^{i}|y\right)$
p
(
x
i
∣
y
)
 мы получили именно такой прогноз. Отсортировав признаки по этой величине, можно выделить признаки, сильнее всего сдвигающие прогноз в пользу принятия или отказа от заданного класса.
Предыдущая страница
Интерпретация метрических методов
Следующая страница
Интерпретация линейной регрессии
Генеративные и дискриминативные модели
Идея метода наивного Байеса
Интерпретация
© 2023-25 
Виктор Китов.
 
Новости проекта.

