





Локально-постоянная регрессия | Машинное и глубокое обучение






[IMAGE]














Перейти к основному содержимому
[IMAGE]
Машинное обучение
Глубокое обучение
Обозначения
Лицензия
Машинное обучение
Введение
Основы машинного обучения
Подготовка данных
Классификаторы в общем виде
Метрические методы прогнозирования
Метрические методы
Метод ближайших центроидов
Метод K ближайших соседей
Анализ метода K ближайших соседей
Обобщение метода K ближайших соседей с весами
Веса в метрических методах
Локально-постоянная регрессия
Функции расстояния
Вопросы
Линейная регрессия и её обобщения
Оценка качества регрессии
Линейная классификация
Многоклассовая классификация набором бинарных классификаторов
Численная оптимизация
Оценка качества классификации
Решающие деревья
Переобучение и недообучение
Ансамбли моделей
Бустинг
Интерпретация простых моделей
Интерпретация сложных моделей
Заключение
Метрические методы прогнозирования
Локально-постоянная регрессия
Содержание этой страницы
Локально-постоянная регрессия


Локально-постоянная регрессия
 (local constant regression), известная также как 
ядерная регрессия
 (kernel regression 
[1]
) и 
регрессия Надарая-Ватсона
 (Nadaraya–Watson regression), представляет собой непараметрический метод для моделирования сложных регрессионных зависимостей. Она была впервые предложена в работах [2] и [3].


Допустим, нам нужно моделировать некоторую нелинейную зависимость 
$y(\mathbf{x})$
y
(
x
)
, показанную ниже:


[IMAGE]


Можно было бы искать наилучший в среднеквадратичном смысле константный прогноз:


$\widehat{y}=\arg\min_{\widehat{y}\in\mathbb{R}}\sum_{i=1}^{N}(\widehat{y}-y_{i})^{2}=\frac{1}{N}\sum_{n=1}^{N}y_{n}$
y
​
=
ar
g
y
​
∈
R
min
​
i
=
1
∑
N
​
(
y
​
−
y
i
​
)
2
=
N
1
​
n
=
1
∑
N
​
y
n
​


Задание
Докажите, что оптимальный константный прогноз, минимизирующий средний квадрат ошибки, это действительно выборочное среднее.
Подсказка: поскольку оптимизационный критерий по 
$\hat{y}$
y
^
​
 является 
выпуклым
, то не только необходимым, но и 
достаточным
 условием оптимальности будет равенство нулю его производной.


Однако константный прогноз слишком прост и нам не подходит, поскольку целевая зависимость нелинейная. Поэтому для построения прогноза в точке 
$\mathbf{x}$
x
 будем использовать 
локально-постоянный прогноз
, получаемый в результате минимизации квадратов ошибок 
в локальной окрестности от целевой точки
:


[IMAGE]




Усредняя лишь по близлежащим точкам к целевой получим нелинейную аппроксимацию общего вида, показанную на рисунке выше красной линией.




В общем случае локально-постоянная регрессия ищет наилучший константный прогноз, усредняя 
по всем объектам
 обучающей выборки, 
но с весами
 - чем объект более удалён от целевой точки, тем вес его меньше, и тем слабее он влияет на прогноз:


$\widehat{y}(\mathbf{x})=\arg\min_{\widehat{y}\in\mathbb{R}}\sum_{i=1}^{N}{\color{red}w_{i}(\mathbf{x})}(\widehat{y}-y_{i})^{2}=\frac{\sum_{i=1}^{N}y_{i}{\color{red}w_{i}(\mathbf{x})}}{\sum_{i=1}^{N}{\color{red}w_{i}(\mathbf{x})}}$
y
​
(
x
)
=
ar
g
y
​
∈
R
min
​
i
=
1
∑
N
​
w
i
​
(
x
)
(
y
​
−
y
i
​
)
2
=
∑
i
=
1
N
​
w
i
​
(
x
)
∑
i
=
1
N
​
y
i
​
w
i
​
(
x
)
​


Задание
Докажите, что оптимальный константный прогноз, минимизирующий квадраты ошибок с весами, это действительно взвешенное среднее.


Веса 
$w_i(\mathbf{x})\ge 0$
w
i
​
(
x
)
≥
0
 определяются так же, как и в случае взвешенного учета ближайших соседей, через убывающую функцию ядра (kernel) 
$K(\cdot)$
K
(
⋅
)
 от расстояния, нормированного на ширину окна 
$h>0$
h
>
0
 (bandwidth):


$w_i(\mathbf{x})=K\left(\frac{\rho(\mathbf{x},\mathbf{x}_i)}{h}\right)$
w
i
​
(
x
)
=
K
(
h
ρ
(
x
,
x
i
​
)
​
)


Разница лишь в том, что теперь это веса для всех объектов обучающей выборки, а не только для ближайших соседей. Типовые ядра 
$K(\cdot)$
K
(
⋅
)
 такие же, как раньше:


Ядро
Формула
top-hat
$\mathbb{I}[\vert u \vert<1]$
I
[
∣
u
∣
<
1
]
линейное
$\max\{0,1-\vert u \vert\}$
max
{
0
,
1
−
∣
u
∣
}
Епанечникова
$\max\{0,1-u^{2}\}$
max
{
0
,
1
−
u
2
}
экспоненциальное
$e^{-\vert u \vert}$
e
−
∣
u
∣
Гауссово
$e^{-\frac{1}{2}u^{2}}$
e
−
2
1
​
u
2
квартическое
$(1-u^{2})^{2}\mathbb{I}[\vert u \vert<1]$
(
1
−
u
2
)
2
I
[
∣
u
∣
<
1
]


а гиперпараметр ширины окна 
$h$
h
 задаёт ширину усреднения. Для ядер top-hat, линейного, Епанечникова и квартического прогноз будет получаться усреднением только по точкам, лежащим на расстоянии не более чем 
$h$
h
 от целевой. Для Гауссова и экспоненциального ядер усреднение всегда будет производиться по всем объектам, но основной вклад также будут давать объекты, лежащие в той же окрестности.


Вид ядра характеризует гладкость получаемой зависимости (рекомендуется Гауссово ядро и квартическое), однако на точность приближения больше всего влияет ширина окна 
$h$
h
 - чем она выше, тем более плавной будет получаться моделируемая зависимость, которая при 
$h\to\infty$
h
→
∞
 будет стремиться к константе (обоснуйте!).


Целесообразно варьировать гиперпараметр 
$h$
h
 для разных частей признакового пространства: чем гуще лежат обучающие объекты, тем меньше мы можем взять 
$h$
h
, производя полноценное усреднение по всё еще большому числу объектов. Поэтому в общем виде этот гиперпараметр также можно сделать зависящим от 
$\mathbf{x}$
x
:


$h=h(\mathbf{x})$
h
=
h
(
x
)


Задание
При каком выборе 
$K(\cdot)$
K
(
⋅
)
 и 
$h(\mathbf{x})$
h
(
x
)
 локально-постоянная регрессия превратится в 
метод K ближайших соседей
?


Обобщение
Поскольку прогноз локально-постоянной регрессии зависит от объектов только через расстояния до них, то это метрический метод, который мы можем обобщать, выбирая различные функции расстояния!


Этот метод, будучи метрическим, наследует преимущества и недостатки метода ближайших соседей.


Литература
​






Wikipedia: kernel regression.






Nadaraya E. A. On estimating regression //Theory of Probability & Its Applications. – 1964. – Т. 9. – №. 1. – С. 141-142.






Watson G. S. Smooth regression analysis //Sankhyā: The Indian Journal of Statistics, Series A. – 1964. – С. 359-372.




Предыдущая страница
Веса в метрических методах
Следующая страница
Функции расстояния
Литература
© 2023-25 
Виктор Китов.
 
Новости проекта.

