





Веса в метрических методах | Машинное и глубокое обучение






[IMAGE]














Перейти к основному содержимому
[IMAGE]
Машинное обучение
Глубокое обучение
Обозначения
Лицензия
Машинное обучение
Введение
Основы машинного обучения
Подготовка данных
Классификаторы в общем виде
Метрические методы прогнозирования
Метрические методы
Метод ближайших центроидов
Метод K ближайших соседей
Анализ метода K ближайших соседей
Обобщение метода K ближайших соседей с весами
Веса в метрических методах
Локально-постоянная регрессия
Функции расстояния
Вопросы
Линейная регрессия и её обобщения
Оценка качества регрессии
Линейная классификация
Многоклассовая классификация набором бинарных классификаторов
Численная оптимизация
Оценка качества классификации
Решающие деревья
Переобучение и недообучение
Ансамбли моделей
Бустинг
Интерпретация простых моделей
Интерпретация сложных моделей
Заключение
Метрические методы прогнозирования
Веса в метрических методах
Содержание этой страницы
Веса в метрических методах


Веса 
$w_i$
w
i
​
, с которыми учитываются ближайшие соседи, должны быть неотрицательными и убывать с ростом расстояния до ближайшего соседа.


Их можно сделать зависимыми от порядкового номера 
$k\in\{1,2,...K\}$
k
∈
{
1
,
2
,
...
K
}
 ближайшего соседа:


$w_{k}=\alpha^{k},\quad\alpha\in(0,1) - \text{гиперпараметр}$
w
k
​
=
α
k
,
α
∈
(
0
,
1
)
−
гиперпараметр


Тогда они будут убывать по экспоненциальному закону от порядкового номера соседа.


Также их можно сделать убывающими по линейному закону:


$w_{k}=\frac{K+1-k}{K}$
w
k
​
=
K
K
+
1
−
k
​


Однако более естественно и правильно сделать веса зависящими от расстояний между ближайшим соседом и целевым объектом 
$\rho(\mathbf{x},\tilde{\mathbf{x}}_k)$
ρ
(
x
,
x
~
k
​
)
, а не от его порядкового номера. Пусть 
$(\tilde{\mathbf{x}}_{1},\tilde{y}_{1}),(\tilde{\mathbf{x}}_{2},\tilde{y}_{2}),...(\tilde{\mathbf{x}}_{K},\tilde{y}_{K})$
(
x
~
1
​
,
y
~
​
1
​
)
,
(
x
~
2
​
,
y
~
​
2
​
)
,
...
(
x
~
K
​
,
y
~
​
K
​
)
 - ближайшие соседи в обучающей выборке для целевого объекта 
$\mathbf{x}$
x
, для которого мы строим прогноз.


Веса можно сделать убывающими по гиперболическому закону:


$w_{k}=\frac{1}{\rho(\tilde{\mathbf{x}}_{k},\mathbf{x})}$
w
k
​
=
ρ
(
x
~
k
​
,
x
)
1
​


В чём недостаток такого выбора весов и как его исправить?
При приближении к соседу его вес будет неограниченно возрастать, в результате чего отклик на этом объекте начнёт перевешивать отклики на других соседях. Чтобы воспрепятствовать неограниченному возрастанию весов, нужно его ограничить сверху некоторой константой 
$M>0$
M
>
0
:
$w_{k}=\min\left\{ M; \frac{1}{\rho(\tilde{\mathbf{x}}_{k},\mathbf{x})} \right\}$
w
k
​
=
min
{
M
;
ρ
(
x
~
k
​
,
x
)
1
​
}


Также можно сделать веса убывающими по линейному закону:


$w_{i}=\begin{cases}
\frac{\rho(\tilde{\mathbf{x}}_{K},\mathbf{x})-\rho(\tilde{\mathbf{x}}_{i},\mathbf{x})}{\rho(\tilde{\mathbf{x}}_{K},\mathbf{x})-\rho(\tilde{\mathbf{x}}_{1},\mathbf{x})}, & \rho(\tilde{\mathbf{x}}_{K},\mathbf{x})\ne\rho(\tilde{\mathbf{x}}_{1},\mathbf{x})\\
1, & \rho(\tilde{\mathbf{x}}_{K},\mathbf{x})=\rho(\tilde{\mathbf{x}}_{1},\mathbf{x})
\end{cases}$
w
i
​
=
{
ρ
(
x
~
K
​
,
x
)
−
ρ
(
x
~
1
​
,
x
)
ρ
(
x
~
K
​
,
x
)
−
ρ
(
x
~
i
​
,
x
)
​
,
1
,
​
ρ
(
x
~
K
​
,
x
)

=
ρ
(
x
~
1
​
,
x
)
ρ
(
x
~
K
​
,
x
)
=
ρ
(
x
~
1
​
,
x
)
​


В более общем виде веса определяются по формуле:


$w_i=K\left(\frac{\rho(\mathbf{x},\tilde{\mathbf{x}}_i)}{h}\right)$
w
i
​
=
K
(
h
ρ
(
x
,
x
~
i
​
)
​
)


для некоторой убывающей функции 
$K(\cdot)$
K
(
⋅
)
, называемой 
ядром
 (kernel) и зависящей от расстояния между объектами 
$\rho(\mathbf{x},\tilde{\mathbf{x}}_i)$
ρ
(
x
,
x
~
i
​
)
, нормированной на 
параметр ширины
 окна 
$h>0$
h
>
0
 (bandwidth), который в общем случае может представлять собой не константу, а тоже функцию от 
$\mathbf{x}$
x
.


Графики популярных ядер 
$K(\cdot)$
K
(
⋅
)
 приведены ниже вместе с формулами для их расчёта 
[1]
:


[IMAGE]






Ядро
Формула
top-hat
$\mathbb{I}[\vert u \vert<1]$
I
[
∣
u
∣
<
1
]
линейное
$\max\{0,1-\vert u \vert\}$
max
{
0
,
1
−
∣
u
∣
}
Епанечникова
$\max\{0,1-u^{2}\}$
max
{
0
,
1
−
u
2
}
экспоненциальное
$e^{-\vert u \vert}$
e
−
∣
u
∣
Гауссово
$e^{-\frac{1}{2}u^{2}}$
e
−
2
1
​
u
2
квартическое
$(1-u^{2})^{2}\mathbb{I}[\vert u \vert<1]$
(
1
−
u
2
)
2
I
[
∣
u
∣
<
1
]


Гиперпараметр ширины окна 
$h$
h
 определяет, насколько сильно меняются веса при изменении расстояний до объектов. Чем 
$h$
h
 выше, тем слабее веса зависят от расстояний, а при 
$h\to\infty$
h
→
∞
 веса стремятся к равномерным, и взвешенный метод ближайших соседей стремится к обычному (без весов).


Ширину окна можно варьировать в зависимости от целевого объекта, поэтому в общем случае она записывается как 
$h(\mathbf{x})$
h
(
x
)
.


Литература
​




Документация sklearn: simple 1D kernel density estimation.


Предыдущая страница
Обобщение метода K ближайших соседей с весами
Следующая страница
Локально-постоянная регрессия
Литература
© 2023-25 
Виктор Китов.
 
Новости проекта.

