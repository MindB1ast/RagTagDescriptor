





Обобщение метода K ближайших соседей с весами | Машинное и глубокое обучение






[IMAGE]














Перейти к основному содержимому
[IMAGE]
Машинное обучение
Глубокое обучение
Обозначения
Лицензия
Машинное обучение
Введение
Основы машинного обучения
Подготовка данных
Классификаторы в общем виде
Метрические методы прогнозирования
Метрические методы
Метод ближайших центроидов
Метод K ближайших соседей
Анализ метода K ближайших соседей
Обобщение метода K ближайших соседей с весами
Веса в метрических методах
Локально-постоянная регрессия
Функции расстояния
Вопросы
Линейная регрессия и её обобщения
Оценка качества регрессии
Линейная классификация
Многоклассовая классификация набором бинарных классификаторов
Численная оптимизация
Оценка качества классификации
Решающие деревья
Переобучение и недообучение
Ансамбли моделей
Бустинг
Интерпретация простых моделей
Интерпретация сложных моделей
Заключение
Метрические методы прогнозирования
Обобщение метода K ближайших соседей с весами
Обобщение метода K ближайших соседей с весами


В методе 
K ближайших соседей
 отклики ближайших объектов учитываются равномерно, с одинаковыми весами. Можно дополнительно повысить качество прогнозов, если позволить более близким ближайшим соседям влиять сильнее на прогноз, чем более далёким. Для этого равномерное усреднение по ближайшим соседям 
заменяется на взвешенное усреднение
, где больший вес будет соответствовать более близким соседям.


Обозначим 
$(\tilde{\mathbf{x}}_{1},\tilde{y}_{1}),(\tilde{\mathbf{x}}_{2},\tilde{y}_{2}),...(\tilde{\mathbf{x}}_{K},\tilde{y}_{K})$
(
x
~
1
​
,
y
~
​
1
​
)
,
(
x
~
2
​
,
y
~
​
2
​
)
,
...
(
x
~
K
​
,
y
~
​
K
​
)
 - ближайшие соседи в обучающей выборке для целевого объекта 
$\mathbf{x}$
x
, для которого мы строим прогноз. Причем эти соседи упорядочены по возрастанию расстояния от них до целевого объекта 
$\mathbf{x}$
x
:


$\rho(\mathbf{x},\tilde{\mathbf{x}}_{1})\le\rho(\mathbf{x},\tilde{\mathbf{x}}_{2})\le...\le\rho(\mathbf{x},\tilde{\mathbf{x}}_{K})$
ρ
(
x
,
x
~
1
​
)
≤
ρ
(
x
,
x
~
2
​
)
≤
...
≤
ρ
(
x
,
x
~
K
​
)


Регрессионный прогноз базового метода строится простым усреднением по откликам ближайших соседей:


$\widehat{y}(\mathbf{x})=\frac{1}{K}\sum_{k=1}^{K}\tilde{y}_{k}$
y
​
(
x
)
=
K
1
​
k
=
1
∑
K
​
y
~
​
k
​


Взвешенное обобщение строится уже взвешенным усреднением откликов:


$\widehat{y}(\mathbf{x})=\frac{\sum_{k=1}^{K}w_k \tilde{y}_{k}}{\sum_{k=1}^{K}w_k}$
y
​
(
x
)
=
∑
k
=
1
K
​
w
k
​
∑
k
=
1
K
​
w
k
​
y
~
​
k
​
​


Аналогично в задаче классификации базовым методом вероятности классов строятся по формуле:


$\frac{1}{K}\begin{pmatrix}
   \sum_{k=1}^{K}\mathbb{I}[\tilde{y}_{k}=1] \\
   \sum_{k=1}^{K}\mathbb{I}[\tilde{y}_{k}=2] \\
   \cdots \\
   \sum_{k=1}^{K}\mathbb{I}[\tilde{y}_{k}=C] \\
\end{pmatrix}$
K
1
​
​
∑
k
=
1
K
​
I
[
y
~
​
k
​
=
1
]
∑
k
=
1
K
​
I
[
y
~
​
k
​
=
2
]
⋯
∑
k
=
1
K
​
I
[
y
~
​
k
​
=
C
]
​
​


Взвешенное обобщение учитывает каждого ближайшего соседа со своим весом:


$\frac{1}{\sum_{k=1}^K w_k}\begin{pmatrix}
   \sum_{k=1}^{K}w_k\mathbb{I}[\tilde{y}_{k}=1] \\
   \sum_{k=1}^{K}w_k\mathbb{I}[\tilde{y}_{k}=2] \\
   \cdots \\
   \sum_{k=1}^{K}w_k\mathbb{I}[\tilde{y}_{k}=C] \\
\end{pmatrix}$
∑
k
=
1
K
​
w
k
​
1
​
​
∑
k
=
1
K
​
w
k
​
I
[
y
~
​
k
​
=
1
]
∑
k
=
1
K
​
w
k
​
I
[
y
~
​
k
​
=
2
]
⋯
∑
k
=
1
K
​
w
k
​
I
[
y
~
​
k
​
=
C
]
​
​


О популярных методах расчета весов можно прочитать в 
следующей главе
.
Предыдущая страница
Анализ метода K ближайших соседей
Следующая страница
Веса в метрических методах
© 2023-25 
Виктор Китов.
 
Новости проекта.

