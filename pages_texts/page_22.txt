





Преобразование целевой переменной | Машинное и глубокое обучение






[IMAGE]














Перейти к основному содержимому
[IMAGE]
Машинное обучение
Глубокое обучение
Обозначения
Лицензия
Машинное обучение
Введение
Основы машинного обучения
Подготовка данных
Фильтрация выбросов
Заполнение пропусков
Обработка временного признака
Обработка категориальных признаков
Нормализация признаков
Генерация признаков
Сокращение числа признаков
Преобразование целевой переменной
Вопросы
Классификаторы в общем виде
Метрические методы прогнозирования
Линейная регрессия и её обобщения
Оценка качества регрессии
Линейная классификация
Многоклассовая классификация набором бинарных классификаторов
Численная оптимизация
Оценка качества классификации
Решающие деревья
Переобучение и недообучение
Ансамбли моделей
Бустинг
Интерпретация простых моделей
Интерпретация сложных моделей
Заключение
Подготовка данных
Преобразование целевой переменной
Содержание этой страницы
Преобразование целевой переменной


Помимо предобработки признаков можно анализировать и обрабатывать и 
саму целевую переменную
 
$y$
y
. Например, удалять объекты, у которых отклик получился аномально большим или малым. Аномальные отклики могут быть вызваны ошибками в измерениях или нетипичными случаями, удаление которых повысит стабильность настройки модели.


Также часто проще что проще прогнозировать не исходный отклик 
$y$
y
, а 
некоторую его преобразованную версию
 
$g(y)$
g
(
y
)
. Например, при построении линейной регрессии может выясниться, что признаки и отклик связаны не линейно, а по экспоненциальному закону.


Тогда нужно нелинейно преобразовывать целевую переменную:


$y\to g(y),$
y
→
g
(
y
)
,


после чего 
обучать модель
 на выборке


$\{(\mathbf{x}_1,g(y_1),(\mathbf{x}_2,g(y_2),... (\mathbf{x}_N,g(y_N)\}.$
{(
x
1
​
,
g
(
y
1
​
)
,
(
x
2
​
,
g
(
y
2
​
)
,
...
(
x
N
​
,
g
(
y
N
​
)}
.


После применения модели
 её прогнозы нужно возвращать в исходную шкалу изменения отклика:


$\hat{y}(\mathbf{x}) \to g^{-1}(\hat{y}(\mathbf{x})),$
y
^
​
(
x
)
→
g
−
1
(
y
^
​
(
x
))
,


где 
$g^{-1}(\cdot)$
g
−
1
(
⋅
)
 - обратная функция к 
$g(\cdot)$
g
(
⋅
)
.


Для откликов, принимающих только положительные значения, популярным преобразованием является логарифмирование и последующее экспоненцирование:


$g(u)=\ln(u),\quad g^{-1}(u)=e^u.$
g
(
u
)
=
ln
(
u
)
,
g
−
1
(
u
)
=
e
u
.


Более общий способ
: подобрать такое преобразование целевой переменной, при котором отклик становится распределённым по 
нормальному закону распределения
.


Если 
$F(\cdot)$
F
(
⋅
)
 - 
функция распределения
 
$y$
y
, а 
$\Phi(\cdot)$
Φ
(
⋅
)
 - функция распределения стандартного нормального распределения, то преобразование 
$\Phi^{-1}(F(y))$
Φ
−
1
(
F
(
y
))
 будет монотонным, а преобразованный таким образом отклик - нормально распределённым со средним 0 и дисперсией 1. Такое преобразование реализовано в библиотеке sklearn 
[1]
.


Литература
​




Sklearn: TransformedTargetRegressor.


Предыдущая страница
Сокращение числа признаков
Следующая страница
Вопросы
Литература
© 2023-25 
Виктор Китов.
 
Новости проекта.

