





Популярные реализации | Машинное и глубокое обучение






[IMAGE]














Перейти к основному содержимому
[IMAGE]
Машинное обучение
Глубокое обучение
Обозначения
Лицензия
Машинное обучение
Введение
Основы машинного обучения
Подготовка данных
Классификаторы в общем виде
Метрические методы прогнозирования
Линейная регрессия и её обобщения
Оценка качества регрессии
Линейная классификация
Многоклассовая классификация набором бинарных классификаторов
Численная оптимизация
Оценка качества классификации
Решающие деревья
Переобучение и недообучение
Ансамбли моделей
Бустинг
Бустинг
Сравнение бустинга с другими ансамблями моделей
Алгоритм AdaBoost
Градиентный бустинг
Алгоритм градиентного бустинга
Улучшения градиентного бустинга
Иллюстрация работы
Градиентный бустинг второго порядка
Популярные реализации
Точность градиентного бустинга
Дополнительная литература
Вопросы
Интерпретация простых моделей
Интерпретация сложных моделей
Заключение
Бустинг
Популярные реализации
Содержание этой страницы
Популярные реализации


На языке python алгоритм градиентного бустинга над решающими деревьями реализован в библиотеке 
sklearn
 классами 
GradientBoostingClassifier
 и 
GradientBoostingRegressor
 для решения задач классификации и регрессии соответственно.


Там также реализованы ускоренные версии алгоритма 
HistGradientBoostingClassifier
 и 
HistGradientBoostingRegressor
, производящие настройку правил во внутренних узлах решающих деревьев за счёт перебора не по всем допустимым порогам, а по более грубой сетке значений. Также эти реализации допускают автоматическую обработку пропущенных значений  признаков, отправляя все объекты с пропущенным признаком либо в левое, либо в правое поддерево, в зависимости от того, что приводит к большему снижению функции неопределённости.


Тремя самыми продвинутыми реализациями бустинга являются:


Название
Документация
Разработчики
xgBoost
сайт
XGBoost Contributors
LightGBM
сайт
Microsoft
CatBoost
сайт
Яндекс


Все реализации допускают параллелизацию настройки базовых моделей, которые поддерживаются как на процессоре, так и на видеокарте.


Реализации используют расширенный набор гиперпараметров, влияющих на точность ансамбля. При настройке правил в узлах деревьев допускается оптимизация не по всем порогам, а по более грубой сетке значений для ускорения обучения модели на больших данных. При этом






xgBoost 
[1]
 использует 
оптимизацию второго порядка
 за счёт квадратичной аппроксимации функции потерь. Добавлены возможности регуляризации каждого решающего дерева.






LightGBM 
[2]
 настраивает каждую базовую модель не на всех данных, а на подмножестве объектов, сильнее всего влияющих на прогнозы ансамбля, что существенно ускоряет настройку модели. Также метод использует специальную технику для эффективной работы с разреженными данными (содержащими много нулевых значений).






CatBoost 
[3]
 строится не над традиционными, а над небрежными решающими деревьями (oblivious decision trees), в которых на каждом уровне дерева во всех узлах проверяется условие над одним и тем же признаком. Это позволяет настраивать деревья быстрее без потери качества всего ансамбля. Также CatBoost использует специальную обработку категориальных признаков.






Литература
​




Chen T., Guestrin C. Xgboost: A scalable tree boosting system //Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining. – 2016. – С. 785-794.


Ke G. et al. Lightgbm: A highly efficient gradient boosting decision tree //Advances in neural information processing systems. – 2017. – Т. 30.


Prokhorenkova L. et al. CatBoost: unbiased boosting with categorical features //Advances in neural information processing systems. – 2018. – Т. 31.


Предыдущая страница
Градиентный бустинг второго порядка
Следующая страница
Точность градиентного бустинга
Литература
© 2023-25 
Виктор Китов.
 
Новости проекта.

