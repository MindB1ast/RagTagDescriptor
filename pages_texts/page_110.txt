





Точность градиентного бустинга | Машинное и глубокое обучение






[IMAGE]














Перейти к основному содержимому
[IMAGE]
Машинное обучение
Глубокое обучение
Обозначения
Лицензия
Машинное обучение
Введение
Основы машинного обучения
Подготовка данных
Классификаторы в общем виде
Метрические методы прогнозирования
Линейная регрессия и её обобщения
Оценка качества регрессии
Линейная классификация
Многоклассовая классификация набором бинарных классификаторов
Численная оптимизация
Оценка качества классификации
Решающие деревья
Переобучение и недообучение
Ансамбли моделей
Бустинг
Бустинг
Сравнение бустинга с другими ансамблями моделей
Алгоритм AdaBoost
Градиентный бустинг
Алгоритм градиентного бустинга
Улучшения градиентного бустинга
Иллюстрация работы
Градиентный бустинг второго порядка
Популярные реализации
Точность градиентного бустинга
Дополнительная литература
Вопросы
Интерпретация простых моделей
Интерпретация сложных моделей
Заключение
Бустинг
Точность градиентного бустинга
Содержание этой страницы
Точность градиентного бустинга


Градиентный бустинг над решающими деревьями - 
один из самых точных алгоритмов классического машинного обучения
 для большинства задач, если не брать в расчёт нейросетевые методы. Но даже по сравнению с нейросетями градиентный бустинг показывает сопоставимое, а часто даже более высокое качество на табличных данных, в которых признаки представлены в виде небольшого вектора признаков. Этот алгоритм особенно хорошо работает, когда признаки имеют разные типы - вещественные, бинарные, категориальные и порядковые, поскольку, в отличие от нейросетей, решающие деревья особенно хорошо справляются с разнородными типами данных.




Делаются попытки догнать качество бустинга на табличных данных с помощью нейросетей, см. Tabular Deep Learning, например, 
[1]
 и 
[2]
.




Точность градиентного бустинга не снижает важности других методов. Для каждого прогнозирующего алгоритма существует ситуация, когда именно он будет оказываться лучшим из всех возможных (например, генерируя сами данные согласно предложенному алгоритму).




Это явно иллюстрируется в no free lunch теоремах 
[3]
, 
[4]
, доказывающих невозможность доминирования одного алгоритма оптимизации над всеми остальными в общем случае.




Изученные методы могут улучшать точность прогнозов, работая в 
ансамбле
 с градиентным бустингом.


Также для всех изученных алгоритмов существуют ситуации, когда именно они будут оказываться наилучшими, например:






Когда классов много, а представителей каждого класса мало, то метод K ближайших соседей часто оказывается лучшим решением, поскольку начинает распознавать класс всего по нескольким примерам.






Когда признаков много по сравнению с числом наблюдений, то линейная модель с регуляризацией может показывать наилучший результат как модель, учитывающая одновременное влияние сразу всех признаков. Поскольку влияние моделируется самой простой (линейной) зависимостью, эта модель наименее склонна переобучаться на малых выборках.






Также базовые модели классического машинного обучения, будучи простыми в настройке, часто используются в качестве референса (бейзлайна) относительно которого уже улучшается качество более продвинутыми моделями, такими как градиентный бустинг и нейросети. Если же сложные модели дают минимальный прирост в точности, но работают существенно медленнее, то из соображений эффективности целесообразен возврат к базовым моделям.


Литература
​






Arik S. Ö., Pfister T. Tabnet: Attentive interpretable tabular learning //Proceedings of the AAAI conference on artificial intelligence. – 2021. – Т. 35. – №. 8. – С. 6679-6687.






Gorishniy Y., Kotelnikov A., Babenko A. Tabm: Advancing tabular deep learning with parameter-efficient ensembling //arXiv preprint arXiv:2410.24210. – 2024.






Wikipedia: no free lunch theorem.






Wolpert D. H., Macready W. G. No free lunch theorems for optimization //IEEE transactions on evolutionary computation. – 1997. – Т. 1. – №. 1. – С. 67-82.




Предыдущая страница
Популярные реализации
Следующая страница
Дополнительная литература
Литература
© 2023-25 
Виктор Китов.
 
Новости проекта.

