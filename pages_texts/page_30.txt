





Анализ метода K ближайших соседей | Машинное и глубокое обучение






[IMAGE]














Перейти к основному содержимому
[IMAGE]
Машинное обучение
Глубокое обучение
Обозначения
Лицензия
Машинное обучение
Введение
Основы машинного обучения
Подготовка данных
Классификаторы в общем виде
Метрические методы прогнозирования
Метрические методы
Метод ближайших центроидов
Метод K ближайших соседей
Анализ метода K ближайших соседей
Обобщение метода K ближайших соседей с весами
Веса в метрических методах
Локально-постоянная регрессия
Функции расстояния
Вопросы
Линейная регрессия и её обобщения
Оценка качества регрессии
Линейная классификация
Многоклассовая классификация набором бинарных классификаторов
Численная оптимизация
Оценка качества классификации
Решающие деревья
Переобучение и недообучение
Ансамбли моделей
Бустинг
Интерпретация простых моделей
Интерпретация сложных моделей
Заключение
Метрические методы прогнозирования
Анализ метода K ближайших соседей
Содержание этой страницы
Анализ метода K ближайших соседей


Достоинства метода
​


Метод K ближайших соседей легко реализовать, и он является интерпретируемым методом: всегда можно обосновать его прогноз, сославшись на похожие объекты в обучающей выборке, отклик для которых известен.




Свойство интерпретируемости важно в приложениях, в которых цена ошибочного прогноза велика, например, в медицине, где ценой ошибки может быть жизнь пациента.




Метод не требует обучения - нужно лишь сохранить обучающие объекты в памяти, поэтому качество его работы можно оценивать 
кросс-валидацией
 с большим числом блоков и даже скользящим контролем (когда число блоков совпадает с числом объектов выборки).


Метод может применяться в 
онлайн-обучении
 (online machine learning 
[1]
), когда данные поступают динамическим потоком и быстро устаревают, например, при автоматической торговле на бирже. Для этого в качестве обучающей выборки нужно учитывать только те объекты, которые мы недавно пронаблюдали.


Метод легко подхватывает основные паттерны в данных, запоминая примеры каждого случая. Поэтому метод легко может оказаться лучшим, например, в классификации на 
очень большое число классов
, когда число представителей каждого класса мало.


Недостатки метода
​


Для работы метода необходимо хранить все объекты обучающей выборки, поскольку эти объекты по сути и составляют 
параметры метода
.


Параметрические и непараметрические методы
Методы, в которых число параметров растёт с ростом объема выборки, называются 
непараметрическими
 в противоположность 
параметрическим
  методам, в которых число параметров 
заранее фиксировано
.
Например, распределение случайной величины мы можем моделировать Гауссовым распределением - тогда, с ростом выборки наблюдений, число параметров (среднее и матрица ковариации) будет оставаться прежним. Если же мы оцениваем распределение гистограммой, то с ростом числа наблюдений, если параллельно делать гистограмму более точной, 
увеличивая число её столбцов
, этот метод становится уже непараметрическим.
Непараметрические методы не делают таких существенных предположений о распределении данных, как параметрические, поэтому обычно работают точнее, когда накоплено большое количество наблюдений.


При построении прогноза даже для одного объекта требуется вычисление расстояний от этого объекта 
до всех объектов обучающей выборки
, чтобы найти K ближайших соседей. Поэтому в простейшей реализации метод применим лишь для малых выборок или в ситуациях, когда скорость построения прогнозов не важна.


Ускорение поиска ближайших соседей
Существуют подходы ускорения прогнозов метода K ближайших соседей, усложняющие его обучение и снижающие его применимость в онлайн-обучении:




Можно ускорить вычисление расстояний между объектами, снизив число признаков.






Можно уменьшить размер выборки, отобрав только эталонные объекты для каждого класса и существенные объекты на границах между классами, отбросив малоинформативные.






Можно упорядочить объекты по определённой структуре признакового пространства. Поиск похожих объектов вместо полного перебора тогда сведётся к направленному поиску по этой структуре (KD-trees, ball-trees 
[2]
 либо направленный поиск по 
графу близости между объектами
 (proximity graph)).






Можно использовать 
локально-чувствительное хэширование
 (locality sensitive hashing), при котором строится хэш-функция, отображающая метрически близкие объекты в одинаковые значения. Тогда, отобразив целевой объект в какое-то значение хэш-функции, можно сразу найти похожие объекты. Это будут объекты обучающей выборки, для которых хэш-функция приняла такое же значение.




Хороший обзор методов ускоренного поиска ближайших соседей представлен в учебнике ШАД 
[3]
.


Проклятие размерности
​


Также метод подвержен так называемому 
проклятию размерности
 (curse of dimensionality). Суть проклятия размерности заключается в том, что с ростом размерности признакового пространства 
$D$
D
 для обеспечения определённого уровня точности методу необходим экспоненциальный рост числа наблюдений (относительно 
$D$
D
). Если же рост числа наблюдений не происходит, то точность работы метода снижается.


Примеры проклятия размерности из разных областей можно прочитать в 
[4]
, а здесь мы сосредоточимся на её интуиции для метода K ближайших соседей.


Пусть нам нужно построить прогноз для объекта 
$\mathbf{x}\in\mathbb{R}^D$
x
∈
R
D
. Поместим этот объект в центр двух вложенных друг в друга  
$D$
D
-мерных кубов со сторонами 
$S$
S
 и 
$S-\varepsilon$
S
−
ε
, где 
$\varepsilon$
ε
 - какая-то малая фиксированная константа, например 0.001. Тогда объем внутреннего куба будет 
$(S-\varepsilon)^D$
(
S
−
ε
)
D
, а объём внешнего - 
$S^D$
S
D
. Отношение двух объёмов будет экспоненциально быстро стремиться к нулю с ростом 
$D$
D
, и уже при  небольшом значении 
$D$
D
 будет пренебрежимо малым:


$\frac{(S-\varepsilon)^D}{S^D}=\left(\frac{S-\varepsilon}{S}\right)^D \to 0 \text{ при } D\to\infty.$
S
D
(
S
−
ε
)
D
​
=
(
S
S
−
ε
​
)
D
→
0
 
при
 
D
→
∞.


Это означает, что доля объема, покрываемого внутренним кубом, от объема внешнего куба будет становиться пренебрежимо малой, а почти весь объем внешнего куба будет концентрироваться на его границе. Аналогичные рассуждения можно провести и для двух сфер вокруг целевого объекта - с ростом 
$D$
D
 почти весь объем внешней сферы будет концентрироваться на её границе.


При равномерном распределении объектов в признаковом пространстве вероятность пронаблюдать объект в определённой области будет пропорциональна объёму области. Значит, при поиске ближайших объектов к заданному эти объекты, скорее всего, 
будут располагаться не рядом, а на удалении
 от него.




Ближайшие соседи перестанут оказываться метрически близкими и перестанут репрезентативно описывать отклик целевого объекта!




Это можно понять и проще - чем большим числом признаков описывается каждый  объект, тем в среднем более удалёнными объекты будут оказываться друг от друга при вычислении расстояний между ними. Таким образом, 
ближайшие соседи перестают быть на самом деле близкими
, и предсказательная сила их откликов для целевого объекта снижается.


Не всё так плохо
Проклятие размерности - скорее теоретическая проблема. На практике, несмотря на то, что мы можем рассматривать все больше и больше признаков, они скорее всего будут скоррелированы друг с другом. Поэтому даже при увеличении размерности признакового пространства объекты будут заполнять его не равномерно, а на некотором многообразии меньшей размерности, вследствие чего метод будет работать лучше ожиданий.


Однако об этой особенности следует помнить, чтобы не добавлять лишних признаков, слабо влияющих на отклик, иначе они могут снизить качество прогнозов.




Литература
​






Wikipedia: Online machine learning.






Документация sklearn: nearest neighbor algorithms.






Учебник ШАД: метрические методы.






Wikipedia: Curse of dimensionality.




Предыдущая страница
Метод K ближайших соседей
Следующая страница
Обобщение метода K ближайших соседей с весами
Достоинства метода
Недостатки метода
Проклятие размерности
Литература
© 2023-25 
Виктор Китов.
 
Новости проекта.

