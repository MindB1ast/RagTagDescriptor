





Предсказание вероятностей и преобразование SoftMax | Машинное и глубокое обучение






[IMAGE]














Перейти к основному содержимому
[IMAGE]
Машинное обучение
Глубокое обучение
Обозначения
Лицензия
Машинное обучение
Введение
Основы машинного обучения
Подготовка данных
Классификаторы в общем виде
Общий вид прогнозирующих функций
Отступ классификации
Предсказание вероятностей и преобразование SoftMax
Метрические методы прогнозирования
Линейная регрессия и её обобщения
Оценка качества регрессии
Линейная классификация
Многоклассовая классификация набором бинарных классификаторов
Численная оптимизация
Оценка качества классификации
Решающие деревья
Переобучение и недообучение
Ансамбли моделей
Бустинг
Интерпретация простых моделей
Интерпретация сложных моделей
Заключение
Классификаторы в общем виде
Предсказание вероятностей и преобразование SoftMax
Содержание этой страницы
Предсказание вероятностей и преобразование SoftMax


Часто в прикладных задачах требуется хорошо предсказывать не только 
метку класса
 
$\hat{y}\in\{1,2,...C\}$
y
^
​
∈
{
1
,
2
,
...
C
}
, но и 
вероятность каждого класса
 
$p(c), c=1,2,...C.$
p
(
c
)
,
c
=
1
,
2
,
...
C
.
 Например, в задачах прогноза погоды важно не только предсказать, будет ли ясная погода или дождь, но и вероятность дождя. Ведь даже если она меньше 50%, но близка к ней, то имеет смысл брать зонтик.


Для предсказания вероятностей классов нужно использовать их 
дискриминантные функции
 
$g_c(\mathbf{x}), c=1,2,...C,$
g
c
​
(
x
)
,
c
=
1
,
2
,
...
C
,
 характеризующие степень уверенности классификатора в том или ином классе. Поскольку дискриминантные функции могут принимать произвольные значения - как положительные, так и отрицательные - а сумма дискриминантных функций не обязательно равна единице, то к ним необходимо применить некоторое монотонно-возрастающее преобразование 
$F_{\tau}$
F
τ
​
, обеспечивающее два свойства:






выходы должны быть неотрицательными;






выходы должны суммироваться в единицу.






В качестве такого преобразования используют операцию SoftMax, используемую для перевода ненормированных рейтингов классов в их вероятности:


$\begin{pmatrix}
   g_1(\mathbf{x}) \\
   g_2(\mathbf{x}) \\
   \cdots \\
   g_C(\mathbf{x}) \\ 
\end{pmatrix}
\longrightarrow 
\begin{pmatrix}
   \cfrac{e^{g_1(\mathbf{x})/\tau}}{\sum_{i=1}^C e^{g_i(\mathbf{x})/\tau}} \\
   \cfrac{e^{g_2(\mathbf{x})/\tau}}{\sum_{i=1}^C e^{g_i(\mathbf{x})/\tau}} \\
   \cdots \\
   \cfrac{e^{g_C(\mathbf{x})/\tau}}{\sum_{i=1}^C e^{g_i(\mathbf{x})/\tau}} \\
\end{pmatrix}
=
\begin{pmatrix}
   \hat{p}(y=1|\mathbf{x}) \\
   \hat{p}(y=2|\mathbf{x}) \\
   \cdots \\
   \hat{p}(y=C|\mathbf{x}) \\ 
\end{pmatrix}$
​
g
1
​
(
x
)
g
2
​
(
x
)
⋯
g
C
​
(
x
)
​
​
⟶
​
∑
i
=
1
C
​
e
g
i
​
(
x
)
/
τ
e
g
1
​
(
x
)
/
τ
​
∑
i
=
1
C
​
e
g
i
​
(
x
)
/
τ
e
g
2
​
(
x
)
/
τ
​
⋯
∑
i
=
1
C
​
e
g
i
​
(
x
)
/
τ
e
g
C
​
(
x
)
/
τ
​
​
​
=
​
p
^
​
(
y
=
1∣
x
)
p
^
​
(
y
=
2∣
x
)
⋯
p
^
​
(
y
=
C
∣
x
)
​
​


Как вы думаете, на что влияет параметр 
$\tau$
τ
?
Параметр 
$\tau$
τ
, называемый 
температурой
 (temperature), определяет контрастность получаемых вероятностей. Чем 
$\tau$
τ
 выше, тем метод становится менее чувствительным к индивидуальным различиям дискриминантных функций, результирующее распределение вероятностей становится более близким к равномерному.
И наоборот, чем 
$\tau$
τ
 ниже, тем сильнее SoftMax реагирует на изменения рейтингов, а выходные вероятности становятся более контрастными.
Эти случаи показаны на рисунке:
[IMAGE]


Обобщение SoftMax-преобразования
SoftMax-преобразование можно обобщить, применяя любую монотонно возрастающую функцию 
$F_{\tau}(\cdot)$
F
τ
​
(
⋅
)
, принимающую неотрицательные значения:
$\begin{pmatrix}
   g_1(\mathbf{x}) \\
   g_2(\mathbf{x}) \\
   \cdots \\
   g_C(\mathbf{x}) \\ 
\end{pmatrix}
\longrightarrow 
\begin{pmatrix}
   \cfrac{F_{\tau}(g_1(\mathbf{x}))}{\sum_{i=1}^C F_{\tau}(g_i(\mathbf{x}))} \\
   \cfrac{F_{\tau}(g_2(\mathbf{x}))}{\sum_{i=1}^C F_{\tau}(g_i(\mathbf{x}))} \\
   \cdots \\
   \cfrac{F_{\tau}(g_C(\mathbf{x}))}{\sum_{i=1}^C F_{\tau}(g_i(\mathbf{x}))} \\
\end{pmatrix}
=
\begin{pmatrix}
   \hat{p}(y=1|\mathbf{x}) \\
   \hat{p}(y=2|\mathbf{x}) \\
   \cdots \\
   \hat{p}(y=C|\mathbf{x}) \\ 
\end{pmatrix}$
​
g
1
​
(
x
)
g
2
​
(
x
)
⋯
g
C
​
(
x
)
​
​
⟶
​
∑
i
=
1
C
​
F
τ
​
(
g
i
​
(
x
))
F
τ
​
(
g
1
​
(
x
))
​
∑
i
=
1
C
​
F
τ
​
(
g
i
​
(
x
))
F
τ
​
(
g
2
​
(
x
))
​
⋯
∑
i
=
1
C
​
F
τ
​
(
g
i
​
(
x
))
F
τ
​
(
g
C
​
(
x
))
​
​
​
=
​
p
^
​
(
y
=
1∣
x
)
p
^
​
(
y
=
2∣
x
)
⋯
p
^
​
(
y
=
C
∣
x
)
​
​
В частном случае 
$F_\tau(g)=e^{g/\tau}$
F
τ
​
(
g
)
=
e
g
/
τ
 получим операцию SoftMax.


Детальнее о SoftMax-преобразовании можно прочитать в 
[1]
.


Литература
​




Wikipedia: softmax function.


Предыдущая страница
Отступ классификации
Следующая страница
Метрические методы прогнозирования
Литература
© 2023-25 
Виктор Китов.
 
Новости проекта.

