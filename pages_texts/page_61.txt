





Метод градиентного спуска | Машинное и глубокое обучение






[IMAGE]














Перейти к основному содержимому
[IMAGE]
Машинное обучение
Глубокое обучение
Обозначения
Лицензия
Машинное обучение
Введение
Основы машинного обучения
Подготовка данных
Классификаторы в общем виде
Метрические методы прогнозирования
Линейная регрессия и её обобщения
Оценка качества регрессии
Линейная классификация
Многоклассовая классификация набором бинарных классификаторов
Численная оптимизация
Численные методы оптимизации
Метод градиентного спуска
Метод стохастического градиентного спуска
Мониторинг сходимости
Стохастический градиентный спуск с инерцией
Метод Ньютона
Вопросы
Оценка качества классификации
Решающие деревья
Переобучение и недообучение
Ансамбли моделей
Бустинг
Интерпретация простых моделей
Интерпретация сложных моделей
Заключение
Численная оптимизация
Метод градиентного спуска
Содержание этой страницы
Метод градиентного спуска


Идея метода
​


Метод градиентного спуска
 (gradient descent 
[1]
) минимизирует функцию потерь, 
итеративно сдвигая веса на антиградиент этой функции с небольшим весом
.


Псевдокод метода:




инициализируем 
$\mathbf{w}$
w
 случайно


пока не выполнено условие остановки:


                    
$\mathbf{w}:=\mathbf{w}-\varepsilon\nabla_{\mathbf{w}}L(\mathbf{w})$
w
:=
w
−
ε
∇
w
​
L
(
w
)




Здесь 
$\varepsilon>0$
ε
>
0
 - гиперпараметр, характеризующий 
шаг обновления весов
 (learning rate). Он выбирается небольшой константой.


В качестве 
условия остановки
 обычно выбирается условие, что от итерации к итерации функция потерь 
перестаёт существенно меняться
. Также можно допустить досрочное окончание оптимизации, если достигнуто максимальное число итераций.


Аналогия с путником в горах
Поскольку антиградиент 
$-\nabla_{\mathbf{w}}L(\mathbf{w})$
−
∇
w
​
L
(
w
)
 показывает 
локальное направление максимального уменьшения функции
, метод градиентного спуска можно уподобить путнику, заблудившемуся ночью в горах, который, пытаясь спуститься вниз как можно быстрее, каждый раз делает шаг в направлении наиболее крутого склона.
Кстати, так можно и скатиться с обрыва! Чтобы не улететь, путник использует альпинистское снаряжение, чтобы упасть не больше чем на длину страхующей веревки. Аналогично и в методе градиентного спуска, если рельеф функции потерь резко меняется, то можно допускать сдвиги по величие не больше, чем на заранее заданный порог безопасности, ограничивая  норму градиента, на который осуществляют сдвиг:
$\mathbf{w}:=
\begin{cases}
\mathbf{w}-\varepsilon\nabla_{\mathbf{w}}L(\mathbf{w}), \text{ если }\|\nabla_{\mathbf{w}}L(\mathbf{w})\|<h \\
\mathbf{w}-\varepsilon\nabla_{\mathbf{w}}h L(\mathbf{w})/\|L(\mathbf{w})\|, \text{ иначе. }
\end{cases}$
w
:=
{
w
−
ε
∇
w
​
L
(
w
)
,
 
если
 
∥
∇
w
​
L
(
w
)
∥
<
h
w
−
ε
∇
w
​
h
L
(
w
)
/∥
L
(
w
)
∥
,
 
иначе
. 
​
Такой подход называется 
обрезкой градиента
 (gradient clipping 
[1]
) и часто используется в настройке нейросетей, где функция потерь имеет сложный рельеф с резкими перепадами. Эта проблема особенно актуальна при настройке сложных нейросетевых моделей.


Выбор шага обучения
​


Гиперпараметр 
$\varepsilon>0$
ε
>
0
 выбирается пользователем и представляет собой небольшую величину, влияющую на характер сходимости. Если 
$\varepsilon$
ε
 выбрать слишком малым, то потребуется очень много шагов оптимизации, чтобы дойти до минимума. Если, наоборот, его выбрать слишком большим, то метод может расходиться. Эти ситуации показаны на рисунке слева и справа соответственно:


[IMAGE]


Для подбора 
$\varepsilon$
ε
 необходимо построить зависимость функции потерь 
$L(\mathbf{w}_t)$
L
(
w
t
​
)
 от номера итерации  
$t$
t
  и выбрать 
$\varepsilon$
ε
 таким, чтобы сходимость была максимально быстрой, и не возникало расходимости:


[IMAGE]


Ускорение сходимости
Дополнительно ускорить сходимость может 
нормализация признаков
 (приведение их к одному масштабу). Это позволяет отчасти решить проблему "вытянутых долин" в рельефе функции потерь, из-за которых приходится выбирать маленький шаг, чтобы не разойтись вдоль быстро меняющихся направлений.


Выбор начального приближения
​


В методе градиентного спуска начальное значение весов 
$\mathbf{w}$
w
 инициализируется случайно. В случае выпуклой функции потерь любой глобальный минимум является глобальным (докажите!), поэтому старт из разных начальных приближений приведёт к решению того же качества.




Если же функция потерь невыпукла, то она может содержать много локальных минимумов, и выбор начального приближения 
может влиять на то, к какому минимуму алгоритм оптимизации в итоге сойдётся
! Поэтому для невыпуклых потерь предлагается запускать алгоритм несколько раз из разных начальных приближений, а потом выбрать наилучшее решение (обеспечивающее минимальное значение функции потерь).




Также стоит отметить, что выбор начального приближения ближе к предполагаемому минимуму уменьшит число итераций до сходимости и ускорит настройку модели.


Условия сходимости
​


Для сходимости метода достаточно, чтобы функция потерь была непрерывно-дифференцируема, выпукла вниз в окрестности оптимума, удовлетворяла условию Липшица (имела ограниченную производную), а шаг обучения был достаточно мал. Детальнее об методе градиентного спуска, условиях и скорости его сходимости можно прочитать в 
[3]
.


Литература
​






Wikipedia: gradient descent.






Geeksforgeeks: understanding gradient clipping.






Ахмеров Р.Р. Методы оптимизации гладких функций.




Предыдущая страница
Численные методы оптимизации
Следующая страница
Метод стохастического градиентного спуска
Идея метода
Выбор шага обучения
Выбор начального приближения
Условия сходимости
Литература
© 2023-25 
Виктор Китов.
 
Новости проекта.

