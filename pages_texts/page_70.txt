





ROC-кривая | Машинное и глубокое обучение






[IMAGE]














Перейти к основному содержимому
[IMAGE]
Машинное обучение
Глубокое обучение
Обозначения
Лицензия
Машинное обучение
Введение
Основы машинного обучения
Подготовка данных
Классификаторы в общем виде
Метрические методы прогнозирования
Линейная регрессия и её обобщения
Оценка качества регрессии
Линейная классификация
Многоклассовая классификация набором бинарных классификаторов
Численная оптимизация
Оценка качества классификации
Базовые меры качества многоклассовой классификации
Специальные меры качества для бинарной классификации
Обобщение бинарных мер качества на многоклассовый случай
ROC-кривая
Лучший классификатор на ROC кривой
Эквивалентное определение AUC
Контроль качества предсказания вероятностей
Дополнительная литература
Вопросы
Решающие деревья
Переобучение и недообучение
Ансамбли моделей
Бустинг
Интерпретация простых моделей
Интерпретация сложных моделей
Заключение
Оценка качества классификации
ROC-кривая
Содержание этой страницы
ROC-кривая


Управление готовностью назначать положительный класс
​


Рассмотрим 
бинарный классификатор
:


$\hat{y}(\mathbf{x}) = \text{sign}(g(\mathbf{x}))  =
\begin{cases} 
+1, & g(\mathbf{x})\ge 0 \\
-1, & g(\mathbf{x})< 0 \\
\end{cases}$
y
^
​
(
x
)
=
sign
(
g
(
x
))
=
{
+
1
,
−
1
,
​
g
(
x
)
≥
0
g
(
x
)
<
0
​


Его относительная дискриминантная функция 
$g(\mathbf{x})$
g
(
x
)
 характеризует степень уверенности классификатора в том, что объект принадлежит положительному классу.


Решающее правило можно обобщить, введя порог 
$\alpha$
α
, с которым производится сравнение:


$\hat{y}(\mathbf{x}) = \text{sign}(g(\mathbf{x})-\alpha)  =
\begin{cases} 
+1, & g(\mathbf{x})\ge \alpha \\
-1, & g(\mathbf{x})< \alpha \\
\end{cases} \tag{1}$
y
^
​
(
x
)
=
sign
(
g
(
x
)
−
α
)
=
{
+
1
,
−
1
,
​
g
(
x
)
≥
α
g
(
x
)
<
α
​
(
1
)


Чем 
$\alpha$
α
 выше, тем более осторожно классификатор начинает предсказывать положительный класс для тех же самых объектов, и наоборот, для более низких 
$\alpha$
α
 положительный класс назначается более охотно. Это полезный параметр, чтобы управлять готовностью назначать объектам положительный класс 
без перенастройки самой модели
.




Рассмотрим задачу кредитного скоринга, в которой для клиентов (объектов 
$\mathbf{x}$
x
) нужно предсказывать, вернут они кредит 
$(y=+1)$
(
y
=
+
1
)
 или нет 
$(y=-1)$
(
y
=
−
1
)
, на основе чего принимается решение о выдаче им кредита. Для низких 
$\alpha$
α
 мы более склонны давать кредиты, и это обосновано в периоды экономического подъёма. В периоды же экономического спада разумно выдавать кредиты более осторожно, повысив гиперпараметр 
$\alpha$
α
, причём саму модель при этом перенастраивать не нужно!




Меры TPR и FPR
​


Определим две меры качества бинарной классификации - true positive rate (TPR) и false positive rate (FPR):


$TPR=\frac{TP}{N_+},\qquad FPR=\frac{FP}{N_-},$
TPR
=
N
+
​
TP
​
,
FPR
=
N
−
​
FP
​
,


где 
$N_+,N_-$
N
+
​
,
N
−
​
 - число объектов положительного и отрицательного класса соответственно.


Мера TPR
 совпадает с 
ранее изученной
 мерой Recall. Также её называют recognition rate, поскольку по смыслу она характеризует долю верных (положительных) детекций 
среди объектов положительного класса
.


Мера FRP
 также называется false alarm rate, поскольку она характеризует долю неверных детекций (положительным классом) 
среди объектов отрицательного класса
.




В примере с кредитным скорингом TPR будет измерять долю клиентов, которые могли бы вернуть кредит, и мы действительно сочли их кредитоспособными. FPR же будет измерять долю клиентов, которые не способны вернуть кредит, но которых мы ошибочно сочли кредитоспособными.




ROC-кривая
​


ROC-кривая
 (ROC-curve, receiver operating characteristic) показывает зависимость 
$TPR$
TPR
 от 
$FPR$
FPR
 при изменении порога 
$\alpha$
α
. Обе величины изменяются от нуля до единицы по неубывающему закону, как показано на графике:


[IMAGE]


При высоком 
$\alpha$
α
 классификатор будет очень редко назначать положительный класс - только в тех случаях, когда он очень уверен в положительности класса. Тогда:






TPR будет близок к нулю (будет мало верно распознанных объектов положительного класса)






FPR также будет мал (поскольку положительный класс будет назначаться редко).






Если же уменьшать 
$\alpha$
α
, то частота назначения положительного класса будет увеличиваться, увеличивая и TPR, и FPR, 
в результате чего мы прочертим ROC-кривую
.


ROC-кривая для случайного угадывания
Рассмотрим классификатор, который случайно назначает классы, независимо от вектора признаков 
$\mathbf{x}$
x
, по формуле
$\hat{y}(\mathbf{x})=\text{sign}(\xi-\alpha),$
y
^
​
(
x
)
=
sign
(
ξ
−
α
)
,
где 
$\xi$
ξ
 - равномерно распределённая случайная величина на отрезке 
$[0,1]$
[
0
,
1
]
. Докажите, что ROC-кривая, соответствующая этому классификатору, будет диагональю TPR=FPR. Существуют ли классификаторы, для которых ROC-кривая проходит ниже этой прямой? Можем ли мы извлечь пользу из таких классификаторов?


Построение ROC-кривой по данным
​


При практическом построении ROC-кривой нам не нужно перебирать все вещественные пороги 
$\alpha$
α
, поскольку мы работаем с конечной выборкой


$(\mathbf{x}_1,y_1),(\mathbf{x}_2,y_2),...(\mathbf{x}_N,y_N)$
(
x
1
​
,
y
1
​
)
,
(
x
2
​
,
y
2
​
)
,
...
(
x
N
​
,
y
N
​
)


Мы рассчитываем относительную дискриминантную функцию нашего классификатора 
$g(\mathbf{x})$
g
(
x
)
 для каждого объекта и отсортируем все объекты по возрастанию этой функции:


$g\left(\mathbf{x}_{(1)}\right)\ge g\left(\mathbf{x}_{(2)}\right)\ge...\ge g\left(\mathbf{x}_{(N)}\right)$
g
(
x
(
1
)
​
)
≥
g
(
x
(
2
)
​
)
≥
...
≥
g
(
x
(
N
)
​
)


Далее ROC-кривая строится итеративно, стартуя из точки (
$FPR_0,TPR_0$
FP
R
0
​
,
TP
R
0
​
)=(0,0) при движении вдоль значений 
$g(\mathbf{x})$
g
(
x
)
 от больших значений к меньшим, перебирая в качестве пороговых 
$\alpha$
α
 только следующие значения:


$g\left(\mathbf{x}_{(N)}\right), g\left(\mathbf{x}_{(N-1)}\right),... g\left(\mathbf{x}_{(1)}\right), g\left(\mathbf{x}_{(1)}\right)-1$
g
(
x
(
N
)
​
)
,
g
(
x
(
N
−
1
)
​
)
,
...
g
(
x
(
1
)
​
)
,
g
(
x
(
1
)
​
)
−
1


При сдвиге порога 
$\alpha$
α
 от 
$g\left(\mathbf{x}_{(i+1)}\right)$
g
(
x
(
i
+
1
)
​
)
 до 
$g\left(\mathbf{x}_{(i)}\right)$
g
(
x
(
i
)
​
)
 прогноз для объект 
$\mathbf{x}_{(i)}$
x
(
i
)
​
 по правилу (1) поменяется с отрицательного на положительный класс. В зависимости от реального класса 
$y_{(i)}$
y
(
i
)
​
 это будет соответствовать двум разным изменениям на ROC-кривой:






При 
$y_{(i)}=+1$
y
(
i
)
​
=
+
1
 следующая точка ROC-кривой получается сдвигом вверх на 
$\frac{1}{N_+}$
N
+
​
1
​
, поскольку при этом на один верно-положительный объект станет больше при том же уровне FPR.






При 
$y_{(i)}=+1$
y
(
i
)
​
=
+
1
 следующая точка ROC-кривой получается сдвигом вправо на 
$\frac{1}{N_-}$
N
−
​
1
​
, поскольку стало на один объект больше среди ложно-положительных срабатываний. FPR увеличится, а TPR останется неизменным.






Пример построения ROC-кривой показан на рисунке:


[IMAGE]




Далее ROC-кривая может сглаживаться линейной интерполяцией между точками.




Площадь под ROC-кривой
​


Чем выше ROC-кривая, тем 
лучше
 классификатор, поскольку нам бы хотелось иметь более высокий TPR при том же уровне FPR.


Агрегированной мерой качества классификации служит 
площадь под ROC-кривой
, называемая 
AUC
 (area under curve).




Классификатору, назначающему классы случайно независимо от входа, будет соответствовать диагональная ROC-кривая, площадь под которой равна 0.5 - см. ROC-кривую для случайного угадывания.






Поскольку ROC-кривая описывает не один классификатор, а целое 
семейство классификаторов
, параметризованных гиперпараметром 
$\alpha$
α
, далее мы рассмотрим 
нахождение наилучшего классификатора
 из этого семейства.


Мы также изучим эквивалентное 
определение величины AUC и способ её численной оптимизации
.
Предыдущая страница
Обобщение бинарных мер качества на многоклассовый случай
Следующая страница
Лучший классификатор на ROC кривой
Управление готовностью назначать положительный класс
Меры TPR и FPR
ROC-кривая
Построение ROC-кривой по данным
Площадь под ROC-кривой
© 2023-25 
Виктор Китов.
 
Новости проекта.

