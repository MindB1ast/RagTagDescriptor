





Аналитическое решение для гребневой регрессии | Машинное и глубокое обучение






[IMAGE]














Перейти к основному содержимому
[IMAGE]
Машинное обучение
Глубокое обучение
Обозначения
Лицензия
Машинное обучение
Введение
Основы машинного обучения
Подготовка данных
Классификаторы в общем виде
Метрические методы прогнозирования
Линейная регрессия и её обобщения
Линейная регрессия
Аналитическое решение для линейной регрессии
Регуляризация в линейной регрессии
Аналитическое решение для гребневой регрессии
Линейный ансамбль моделей
Регрессия опорных векторов
Orthogonal matching pursuit
Локально-линейная регрессия
Дополнительная литература
Вопросы
Оценка качества регрессии
Линейная классификация
Многоклассовая классификация набором бинарных классификаторов
Численная оптимизация
Оценка качества классификации
Решающие деревья
Переобучение и недообучение
Ансамбли моделей
Бустинг
Интерпретация простых моделей
Интерпретация сложных моделей
Заключение
Линейная регрессия и её обобщения
Аналитическое решение для гребневой регрессии
Аналитическое решение для гребневой регрессии


В гребневой регрессии вектор весов 
$\mathbf{w}$
w
 находится из условия:


$L(\mathbf{w})=\sum_{n=1}^{N}\left(\mathbf{x}_{n}^{T}\mathbf{w}-y_{n}\right)^{2}+\lambda \mathbf{w}^{T}\mathbf{w}\to\min_{\mathbf{w}}$
L
(
w
)
=
n
=
1
∑
N
​
(
x
n
T
​
w
−
y
n
​
)
2
+
λ
w
T
w
→
w
min
​


Поскольку этот критерий выпуклый (докажите!), минимум является глобальным минимумом и находится из условия покомпонентного равенства градиента функции потерь нулю, т.е. 
$\nabla L(\mathbf{w})=0$
∇
L
(
w
)
=
0
:


$2\sum_{n=1}^{N}\mathbf{x}_{n}\left(\mathbf{x}_{n}^{T}\widehat{\mathbf{w}}-y_{n}\right)+2\lambda\widehat{\mathbf{w}}=0$
2
n
=
1
∑
N
​
x
n
​
(
x
n
T
​
w
−
y
n
​
)
+
2
λ
w
=
0


Отсюда, используя обозначение 
$I\in\mathbb{R}^{D\times D}$
I
∈
R
D
×
D
 для единичной матрицы, получаем:


$\lparen \sum_{n=1}^N \mathbf{x}_n^T \mathbf{x}_n \rparen \hat{\mathbf{w}}+\lambda I \hat{\mathbf{w}} = \sum_{n=1}^N \mathbf{x}_n y_n$
(
n
=
1
∑
N
​
x
n
T
​
x
n
​
)
w
^
+
λ
I
w
^
=
n
=
1
∑
N
​
x
n
​
y
n
​


$\lparen \sum_{n=1}^N \mathbf{x}_n^T \mathbf{x}_n +\lambda I \rparen \hat{\mathbf{w}} = \sum_{n=1}^N \mathbf{x}_n y_n$
(
n
=
1
∑
N
​
x
n
T
​
x
n
​
+
λ
I
)
w
^
=
n
=
1
∑
N
​
x
n
​
y
n
​


Используя обозначения для матрицы объекты-признаки 
$X\in\mathbb{R}^{N\times D}$
X
∈
R
N
×
D
 и вектора откликов 
$Y\in \mathbb{R}^N$
Y
∈
R
N
, это можно переписать в виде:


$\left(X^{T}X+\lambda I\right)\widehat{\mathbf{w}}=X^{T}Y,$
(
X
T
X
+
λ
I
)
w
=
X
T
Y
,


откуда получим итоговый вид аналитического решения:


$\widehat{\mathbf{w}}=(X^{T}X+\lambda I)^{-1}X^{T}Y$
w
=
(
X
T
X
+
λ
I
)
−
1
X
T
Y


Существование решения
Обратим внимание, что матрица 
$X^{T}X+\lambda I$
X
T
X
+
λ
I
 положительно определена (докажите!), поэтому всегда невырождена для любых 
$\lambda>0$
λ
>
0
. Поэтому решение всегда существует - в отличие от линейной регрессии без регуляризации. Интуитивно решение существует, поскольку даже в случае линейной зависимости признаков регуляризация привносит в него однозначность - среди всех решений необходимо найти то, которое обладает максимальной простотой (минимизирует 
$\|\mathbf{w}\|_2^2$
∥
w
∥
2
2
​
).
Предыдущая страница
Регуляризация в линейной регрессии
Следующая страница
Линейный ансамбль моделей
© 2023-25 
Виктор Китов.
 
Новости проекта.

