





Обобщения решающих деревьев | Машинное и глубокое обучение






[IMAGE]














Перейти к основному содержимому
[IMAGE]
Машинное обучение
Глубокое обучение
Обозначения
Лицензия
Машинное обучение
Введение
Основы машинного обучения
Подготовка данных
Классификаторы в общем виде
Метрические методы прогнозирования
Линейная регрессия и её обобщения
Оценка качества регрессии
Линейная классификация
Многоклассовая классификация набором бинарных классификаторов
Численная оптимизация
Оценка качества классификации
Решающие деревья
Решающие деревья
Особенности прогнозов решающего дерева
Настройка решающего дерева
Функции неопределённости
Учёт пользовательской функции потерь
Обрезка решающих деревьев
Обработка пропущенных значений
Важность признаков
Анализ решающих деревьев
Обобщения решающих деревьев
Дополнительная литература
Вопросы
Переобучение и недообучение
Ансамбли моделей
Бустинг
Интерпретация простых моделей
Интерпретация сложных моделей
Заключение
Решающие деревья
Обобщения решающих деревьев
Содержание этой страницы
Обобщения решающих деревьев


Обобщение правил ветвления
​


Вместо правил 
$x^i\le h$
x
i
≤
h
 (признак меньше порога) во внутренних узлах дерева можно применять и другие правила:






Например, при использовании категориального признака 
$x^i$
x
i
 можно построить столько дочерних вершин, сколько есть уникальных категорий, и, в зависимости от категории 
$x^i$
x
i
, спускаться в соответствующую вершину. Дерево тогда уже не будет бинарным 
[1]
. Именно такой подход используется в решающем дереве ID3 
[2]
 и его более продвинутой версии C4.5 
[3]
.






Можно разбить множество значений признака на набор из 
$K$
K
 непересекающихся полуинтервалов: 
$(-\infty,h_1],(h_1,h_2],...(h_{K-2},h_{K-1}],(h_{K-1},+\infty)$
(
−
∞
,
h
1
​
]
,
(
h
1
​
,
h
2
​
]
,
...
(
h
K
−
2
​
,
h
K
−
1
​
]
,
(
h
K
−
1
​
,
+
∞
)
 и осуществлять спуск в  дочернюю вершину 
$j$
j
, если признак 
$x^i$
x
i
 попадает в 
$j$
j
-й полуинтервал.






В каждом внутреннем узле 
$t$
t
 можно спускаться в левую или правую дочернюю вершину на основе правила


$\mathbf{\mathbf{x}}^T \mathbf{w}_t\le w_{t0},$
x
T
w
t
​
≤
w
t
0
​
,


при этом вектора коэффициентов 
$\mathbf{w}_t$
w
t
​
 и пороги 
$w_{t0}$
w
t
0
​
 у каждого узла будут свои. Тогда каждый внутренний узел сможет разделять признаковое пространство не только перпендикулярно осям, но и под произвольным углом на основе линейной классификации.






В качестве проверяемой функции в узле 
$t$
t
 можно брать произвольную функцию 
$f_t(\mathbf{x})$
f
t
​
(
x
)
. Например, если взять 
$f_t(\mathbf{x})=\|\mathbf{x}\|$
f
t
​
(
x
)
=
∥
x
∥
, то правило 
$\|\mathbf{x}\|\le h$
∥
x
∥
≤
h
 будет направлять объекты в левую либо правую дочернюю вершину в зависимости от того, попал ли объект внутрь шара определённого радиуса или нет.






Обобщение правил прогнозирования в листьях
​


Вместо назначения константного прогноза в листьях дерева в каждом листе 
$t$
t
 можно строить прогноз по некоторой функции (например, линейной):


$\hat{y}_t(\mathbf{x})=f_t(\mathbf{x})$
y
^
​
t
​
(
x
)
=
f
t
​
(
x
)


Параметры этой функции можно настроить, используя обучающие объекты, попавшие в лист 
$t$
t
.


Более оптимальная настройка дерева
​


Стандартное решающее дерево строится последовательно сверху вниз, выбирая локально оптимальное разбиение 
на один шаг вперёд
. Поиск можно сделать более полным и точным (ценой увеличения вычислительной сложности), если настраивать правило разбиения в каждой вершине, заглядывая не на один шаг вперёд, а на два: для этого нужно перебирать всевозможные признаки и пороги не только в текущем узле, но и в образовавшихся левой и правой дочерних вершинах, максимизируя изменение неопределённости сразу на два шага вперёд между вершиной и потомками от её потомков. Можно анализировать влияние разбиений, заглядывая и на большее число шагов вперёд. Детально с алгоритмом можно ознакомиться в 
[4]
.


Мягкие решающие деревья
​


В стандартном решающем дереве спуск каждого объекта производится только по одному пути. В мягких решающих деревьях (soft decision trees 
[5]
) объект спускается одновременно по всем путям сразу с вероятностями, рассчитываемыму логистической регрессией в каждом узле.


Литература
​






Wikipedia: двоичное дерево.






Wikipedia: ID3 algorithm.






Wikipedia: C4.5 algorithm.






Esmeir S., Markovitch S. Lookahead-based algorithms for anytime induction of decision trees //Proceedings of the 21 international conference on Machine learning. – 2004. – С. 33.






Irsoy O., Yıldız O. T., Alpaydın E. Soft decision trees //Proceedings of the 21st international conference on pattern recognition (ICPR2012). – IEEE, 2012. – С. 1819-1822.




Предыдущая страница
Анализ решающих деревьев
Следующая страница
Дополнительная литература
Обобщение правил ветвления
Обобщение правил прогнозирования в листьях
Более оптимальная настройка дерева
Мягкие решающие деревья
Литература
© 2023-25 
Виктор Китов.
 
Новости проекта.

