





Градиентный бустинг второго порядка | Машинное и глубокое обучение






[IMAGE]














Перейти к основному содержимому
[IMAGE]
Машинное обучение
Глубокое обучение
Обозначения
Лицензия
Машинное обучение
Введение
Основы машинного обучения
Подготовка данных
Классификаторы в общем виде
Метрические методы прогнозирования
Линейная регрессия и её обобщения
Оценка качества регрессии
Линейная классификация
Многоклассовая классификация набором бинарных классификаторов
Численная оптимизация
Оценка качества классификации
Решающие деревья
Переобучение и недообучение
Ансамбли моделей
Бустинг
Бустинг
Сравнение бустинга с другими ансамблями моделей
Алгоритм AdaBoost
Градиентный бустинг
Алгоритм градиентного бустинга
Улучшения градиентного бустинга
Иллюстрация работы
Градиентный бустинг второго порядка
Популярные реализации
Точность градиентного бустинга
Дополнительная литература
Вопросы
Интерпретация простых моделей
Интерпретация сложных моделей
Заключение
Бустинг
Градиентный бустинг второго порядка
Содержание этой страницы
Градиентный бустинг второго порядка


Мы вывели 
алгоритм градиентного бустинга
 из 
линейного приближения
 функции потерь. Но можно было бы применить ту же самую идею, используя более точное квадратичное приближение!


Рассмотрим для объекта 
$(\mathbf{x},y)$
(
x
,
y
)
 функцию потерь 
$\mathcal{L}(\mathbf{x},y)$
L
(
x
,
y
)
 и введём обозначения для её первой и второй производной по значению прогноза:


$\begin{aligned}
g(\mathbf{x}) &= \frac{\partial\mathcal{L}(G(\mathbf{x}),y)}{\partial G} \\
h(\mathbf{x}) &= \frac{\partial^{2}\mathcal{L}(G(\mathbf{x}),y)}{\partial G^{2}}
\end{aligned}$
g
(
x
)
h
(
x
)
​
=
∂
G
∂
L
(
G
(
x
)
,
y
)
​
=
∂
G
2
∂
2
L
(
G
(
x
)
,
y
)
​
​


Тогда из разложения Тейлора второго порядка 
[1]
 получим следующую квадратичную аппроксимацию для функции потерь:


$\begin{gathered}\mathcal{L}(G(\mathbf{x})+f(\mathbf{x}),\,y)\approx\mathcal{L}(G(\mathbf{x}),y)+g(\mathbf{x})f(\mathbf{x})+\frac{1}{2}h(\mathbf{x})\left(f(\mathbf{x})\right)^{2}=\\
\frac{1}{2}h(\mathbf{x})\left(f(\mathbf{x})+\frac{g(\mathbf{x})}{h(\mathbf{x})}\right)^{2}+\text{const}(f(\mathbf{x})),
\end{gathered}$
L
(
G
(
x
)
+
f
(
x
)
,
y
)
≈
L
(
G
(
x
)
,
y
)
+
g
(
x
)
f
(
x
)
+
2
1
​
h
(
x
)
(
f
(
x
)
)
2
=
2
1
​
h
(
x
)
(
f
(
x
)
+
h
(
x
)
g
(
x
)
​
)
2
+
const
(
f
(
x
))
,
​


где 
$\text{const}(f(\mathbf{x}))$
const
(
f
(
x
))
 обозначает некоторое выражение, не зависящее от базовой модели 
$f(\mathbf{x})$
f
(
x
)
, по которой нам необходимо производить минимизацию.


Отсюда следует, что для минимизации функции потерь для объекта 
$\mathbf{x}$
x
 базовая модель 
$f(\mathbf{x})$
f
(
x
)
 должна приближать 
$-g(\mathbf{x})/h(\mathbf{x})$
−
g
(
x
)
/
h
(
x
)
 с 
весом
 
$h(\mathbf{x})$
h
(
x
)
. То есть должна настраиваться на следующей обучающей выборке:


$\{ \mathbf{x}_n, -g(\mathbf{x}_{n})/h(\mathbf{x}_{n}) \}_{n=1}^N$
{
x
n
​
,
−
g
(
x
n
​
)
/
h
(
x
n
​
)
}
n
=
1
N
​


с соответствующими весами 
$\{h(\mathbf{x}_{n})\}_{n=1}^N$
{
h
(
x
n
​
)
}
n
=
1
N
​
, которые будут неотрицательны в окрестности локального минимума.


На приближении второго порядка основан алгоритм LogitBoost, подробно описанный в 
[2]
, а также алгоритм xgBoost 
[3]
.


Литература
​






Викиконспекты ИТМО: формула Тейлора для произвольной функции.






Мерков А. Б. Распознавание образов: введение в методы статистического обучения. // Москва: Едиториал УРСС. – 2019.






Chen T., Guestrin C. Xgboost: A scalable tree boosting system //Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining. – 2016. – С. 785-794.




Предыдущая страница
Иллюстрация работы
Следующая страница
Популярные реализации
Литература
© 2023-25 
Виктор Китов.
 
Новости проекта.

