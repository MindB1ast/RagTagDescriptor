





Многоклассовая логистическая регрессия | Машинное и глубокое обучение






[IMAGE]














Перейти к основному содержимому
[IMAGE]
Машинное обучение
Глубокое обучение
Обозначения
Лицензия
Машинное обучение
Введение
Основы машинного обучения
Подготовка данных
Классификаторы в общем виде
Метрические методы прогнозирования
Линейная регрессия и её обобщения
Оценка качества регрессии
Линейная классификация
Линейная классификация
Оценка весов линейного классификатора
Бинарная логистическая регрессия
Многоклассовая логистическая регрессия
Метод опорных векторов
Дополнительная литература
Вопросы
Многоклассовая классификация набором бинарных классификаторов
Численная оптимизация
Оценка качества классификации
Решающие деревья
Переобучение и недообучение
Ансамбли моделей
Бустинг
Интерпретация простых моделей
Интерпретация сложных моделей
Заключение
Линейная классификация
Многоклассовая логистическая регрессия
Многоклассовая логистическая регрессия


Существует обобщение логистической регрессии и для решения задачи 
многоклассовой классификации
, в которой прогнозируемая величина принадлежит одному из 
$C$
C
 классов:


$y\in\{1,2,...C\}$
y
∈
{
1
,
2
,
...
C
}


Многоклассовый линейный классификатор
, как известно, определяется 
$C$
C
 дискриминантными функциями, измеряющими рейтинг каждого из классов:


$\begin{array}{cc}
y=1: & g_{1}(\mathbf{x})=\mathbf{w}_{1}^{T}\mathbf{x}\\
y=2: & g_{2}(\mathbf{x})=\mathbf{w}_{2}^{T}\mathbf{x}\\
\cdots & \cdots\\
y=C: & g_{C}(\mathbf{x})=\mathbf{w}_{C}^{T}\mathbf{x}
\end{array}$
y
=
1
:
y
=
2
:
⋯
y
=
C
:
​
g
1
​
(
x
)
=
w
1
T
​
x
g
2
​
(
x
)
=
w
2
T
​
x
⋯
g
C
​
(
x
)
=
w
C
T
​
x
​




Для компактности записи смещение не указано, поскольку здесь мы добавили константную единицу в число признаков.




В модели 
многоклассовой логистической регрессии
 предполагается связь 
$g_{1}(\mathbf{x}),...g_{C}(\mathbf{x})$
g
1
​
(
x
)
,
...
g
C
​
(
x
)
 с вероятностями классов через 
SoftMax-преобразование
:


$p(y=c|\mathbf{x})=\text{SoftMax}(\mathbf{w}_{c}^{T}\mathbf{x}|\mathbf{w}_{1}^{T}\mathbf{x},...\mathbf{w}_{C}^{T}\mathbf{x})=\frac{e^{\mathbf{w}_{c}^{T}\mathbf{x}}}{\sum_{i=1}^{C}e^{\mathbf{w}_{i}^{T}\mathbf{x}}}$
p
(
y
=
c
∣
x
)
=
SoftMax
(
w
c
T
​
x
∣
w
1
T
​
x
,
...
w
C
T
​
x
)
=
∑
i
=
1
C
​
e
w
i
T
​
x
e
w
c
T
​
x
​


для каждого класса 
$c=1,2,...C$
c
=
1
,
2
,
...
C
.


Как видим, SoftMax-преобразование переводит 
$C$
C
-мерный вектор рейтингов классов в 
$C$
C
-мерный вектор выходов, которые:






неотрицательны;






суммируются в единицу.






Таким образом, выходы SoftMax можно трактовать как 
вероятности классов
.


Веса полученной вероятностной модели нужно оценивать 
методом максимального правдоподобия
.


Более компактное представление
Поскольку дискриминантные функции определены с точностью до сдвига на общую функцию (обоснуйте!), то, сдвигая каждый раз найденные дискриминантные функции на 
$\mathbf{w}_C^T \mathbf{x}$
w
C
T
​
x
, получим эквивалентный классификатор с рейтингом последнего класса, равным тождественному нулю.
Поэтому, не ограничивая общности, можно сразу положить 
$g_C(\mathbf{x})\equiv 0$
g
C
​
(
x
)
≡
0
 и находить только вектора весов 
$\mathbf{w}_1,\mathbf{w}_2,...\mathbf{w}_{C-1}$
w
1
​
,
w
2
​
,
...
w
C
−
1
​
. Число параметров модели тогда снизится с 
$C(D+1)$
C
(
D
+
1
)
 до 
$(C-1)(D+1)$
(
C
−
1
)
(
D
+
1
)
.
Предыдущая страница
Бинарная логистическая регрессия
Следующая страница
Метод опорных векторов
© 2023-25 
Виктор Китов.
 
Новости проекта.

