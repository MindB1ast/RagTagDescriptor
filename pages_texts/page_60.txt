





Численные методы оптимизации | Машинное и глубокое обучение






[IMAGE]














Перейти к основному содержимому
[IMAGE]
Машинное обучение
Глубокое обучение
Обозначения
Лицензия
Машинное обучение
Введение
Основы машинного обучения
Подготовка данных
Классификаторы в общем виде
Метрические методы прогнозирования
Линейная регрессия и её обобщения
Оценка качества регрессии
Линейная классификация
Многоклассовая классификация набором бинарных классификаторов
Численная оптимизация
Численные методы оптимизации
Метод градиентного спуска
Метод стохастического градиентного спуска
Мониторинг сходимости
Стохастический градиентный спуск с инерцией
Метод Ньютона
Вопросы
Оценка качества классификации
Решающие деревья
Переобучение и недообучение
Ансамбли моделей
Бустинг
Интерпретация простых моделей
Интерпретация сложных моделей
Заключение
Численная оптимизация
Численные методы оптимизации
Содержание этой страницы
Численные методы оптимизации


Оптимальные веса модели находятся 
методом минимизации средних потерь
 на обучающей выборке. Для функции потерь 
$L(\mathbf{w})$
L
(
w
)
 общего вида это 
нельзя сделать аналитически
 и приходится использовать численные методы оптимизации.


Применение, даже если аналитическое решение существует
Интересно, что применение численных методов может быть вычислительно эффективнее, даже когда аналитическое решение существует! Например, решение линейной регрессии
$\hat{\beta}=(X^T X)^{-1}X^T Y$
β
^
​
=
(
X
T
X
)
−
1
X
T
Y
хоть и можно выразить аналитически, но для расчёта оно требует обращения матрицы 
$D\times D$
D
×
D
, требующего вычислений порядка 
$O(D^3)$
O
(
D
3
)
. Численные методы могут найти приближённое решение, используя меньший объём вычислений!


Численные методы, описанные в этом разделе, основаны на вычислении 
градиента
 
$\nabla L(\mathbf{w})$
∇
L
(
w
)
, представляющего собой вектор из частных производных потерь по каждому параметру модели:


$\nabla L(\mathbf{w})=
\begin{pmatrix}
   \frac{\partial L}{\partial w_0} \\
   \frac{\partial L}{\partial w_1} \\
   \frac{\partial L}{\partial w_2} \\
   \cdots \\
   \frac{\partial L}{\partial w_D} \\
\end{pmatrix}$
∇
L
(
w
)
=
​
∂
w
0
​
∂
L
​
∂
w
1
​
∂
L
​
∂
w
2
​
∂
L
​
⋯
∂
w
D
​
∂
L
​
​
​


Градиент полезен, поскольку он указывает направление максимального возрастания функции 
$L(\mathbf{w})$
L
(
w
)
 в пространстве весов, а 
антиградиент
 
$-\nabla L(\mathbf{w})$
−
∇
L
(
w
)
 - направление максимального убывания 
$L(\mathbf{w})$
L
(
w
)
.


Почему?
Разложим 
$L(\mathbf{w})$
L
(
w
)
 в ряд Тейлора (см. 
[1]
) первого порядка, чтобы получить локально линейную аппроксимацию функции потерь:
$L(\mathbf{w}+\varepsilon\Delta \mathbf{w}) = L(\mathbf{w})+\varepsilon\nabla L(\mathbf{w})^T(\Delta \mathbf{w}) + o(\varepsilon)$
L
(
w
+
ε
Δ
w
)
=
L
(
w
)
+
ε
∇
L
(
w
)
T
(
Δ
w
)
+
o
(
ε
)
Если перебирать всевозможные направления 
$\Delta \mathbf{w}$
Δ
w
 единичной длины, то из неравенства Коши-Буняковского (см. 
[2]
) получим, что наибольший локальный рост (для малых 
$\varepsilon$
ε
) достигается, когда 
$\Delta \mathbf{w}$
Δ
w
 сонаправлен 
$\nabla L(\mathbf{w})$
∇
L
(
w
)
, а максимальный спад - когда 
$\Delta \mathbf{w}$
Δ
w
 и 
$\nabla L(\mathbf{w})$
∇
L
(
w
)
 направлены в противоположные стороны.
Используя это свойство, градиентные методы оптимизации уменьшают функцию потерь, 
многократно сдвигая веса в направлении антиградиента функции потерь с малым шагом
. Такие методы называются 
градиентными методами оптимизации
 (gradient based, см. 
[3]
) в противоположность 
безградиентным методам
 (gradient free).


Оптимизация без градиента
Иногда приходится оптимизировать функцию, 
не располагая её градиентом
. Такая ситуация возникает, когда мы можем функцию вычислить в каждой точке, но не можем представить в аналитическом виде, чтобы её можно было продифференцировать.
Например, это может быть выручка магазина при различных расположениях товаров на полках или качество решения задачи машинного обучения при различных значениях гиперпараметров. В этом случае используются безградиентные методы оптимизации, такие как Байесовская оптимизация, генетические (эволюционные) алгоритмы, метод Tree-structured Parzen Estimator, алгоритм имитации отжига, случайный поиск или перебор по сетке значений - см. 
[4]
.


Литература
​






Викиконспекты ИТМО: формула Тейлора для произвольной функции.






Wikipedia: Cauchy–Schwarz inequality.






Wikipedia: gradient method.






Wikipedia: derivative-free optimization.




Предыдущая страница
Численная оптимизация
Следующая страница
Метод градиентного спуска
Литература
© 2023-25 
Виктор Китов.
 
Новости проекта.

