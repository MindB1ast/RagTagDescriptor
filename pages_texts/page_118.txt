





Интерпретация решающего дерева | Машинное и глубокое обучение






[IMAGE]














Перейти к основному содержимому
[IMAGE]
Машинное обучение
Глубокое обучение
Обозначения
Лицензия
Машинное обучение
Введение
Основы машинного обучения
Подготовка данных
Классификаторы в общем виде
Метрические методы прогнозирования
Линейная регрессия и её обобщения
Оценка качества регрессии
Линейная классификация
Многоклассовая классификация набором бинарных классификаторов
Численная оптимизация
Оценка качества классификации
Решающие деревья
Переобучение и недообучение
Ансамбли моделей
Бустинг
Интерпретация простых моделей
Интерпретируемое машинное обучение
Интерпретация метрических методов
Метод наивного Байеса
Интерпретация линейной регрессии
Интерпретация логистической регрессии
Интерпретация решающего дерева
Вопросы
Интерпретация сложных моделей
Заключение
Интерпретация простых моделей
Интерпретация решающего дерева
Содержание этой страницы
Интерпретация решающего дерева


Решающее дерево является интерпретируемым алгоритмом машинного обучения. Интерпретация возможна:






визуализацией решающего дерева;






оценкой степени влияния каждого признака на прогнозы в среднем по выборке;






оценкой степени влияния каждого признака на прогноз интересующего объекта.






Ниже мы рассмотрим каждый из подходов детально.


Визуализация деревьев
​


Решающее дерево небольшой глубины можно визуализировать и анализировать напрямую. В этом смысле это метод, обладающий 
глобальной интерпретируемостью
.


Ниже приведён простой пример решающего дерева для задачи кредитного скоринга в банке, работу которого может понять даже не-специалист:


[IMAGE]


Глобальная важность признаков
​


Можно оценивать значимость каждого признака для прогнозов решающего дерева в целом по выборке, используя ранее уже изученное 
среднее изменение неопределённости
 (mean decrease in impurity, MDI).


Рассмотрим задачу wine 
[1]
, в которой по характеристикам вина требуется предсказать его класс. Значимости каждого признака приведены ниже 
[2]
:


[IMAGE]


Из графика сразу видно, что уровень пролина оказывается самым важным признаком.


Эту же методику можно применять для ансамбля над решающими деревьями (бэггинг, случайный лес, бустинг и др.) - нужно лишь усреднить важности признаков каждого дерева с теми коэффициентами, с которыми оно учитываются в ансамбле.




Поскольку ансамбли дают более точные прогнозы, расчёт важности по ансамблю деревьев даст более надёжную оценку влияния признаков на отклик!




Первичный анализ данных
Анализ самых значимых признаков по ансамблю решающих деревьев - важный этап 
первичного анализа данных
, который стоит применять, даже если вы не собираетесь впоследствии использовать сами решающие деревья!


Вклад признака в отдельный прогноз
​


Нас может интересовать вклад каждого признака не для всех объектов выборки, а для одного интересующего объекта 
$\mathbf{x}$
x
. Рассмотрим, как это можно рассчитать.


Как известно в решающих деревьях прогноз приписывается каждому листу дерева 
простым усреднением откликов объектов, попавших в лист
. В случае классификации усредняются one-hot закодированные метки классов, что на выходе даёт вектор предсказанных вероятностей классов. Но аналогично можно сопоставить прогноз и каждому промежуточному узлу, усредняя отклики объектов, прошедших через соответствующий узел.


Введём обозначения:






$t$
t
 - узел дерева,






$\text{Parent}\left(t\right)$
Parent
(
t
)
 - соответствующий родительский узел,






$\text{root}$
root
 - корень дерева,






$\text{path}\left(\mathbf{x}\right)$
path
(
x
)
 - путь от корня до листа, по которому объект 
$\mathbf{x}$
x
 спустился вниз по дереву.






Посчитаем для объекта 
$\mathbf{x}$
x
 прогноз 
$\widehat{y}\left(t\right)$
y
​
(
t
)
 в каждом промежуточном узле дерева 
$t$
t
 вдоль пути 
$\text{path}\left(\mathbf{x}\right)$
path
(
x
)
. Итоговый прогноз можно декомпозировать по вкладу в него каждого узла:


$\widehat{y}(\mathbf{x})=\sum_{t\in\text{path}\left(\mathbf{x}\right)\backslash\left\{ \text{root}\right\} }|\widehat{y}\left(t\right)-\widehat{y}\left(\text{Parent}\left(t\right)\right)|$
y
​
(
x
)
=
t
∈
path
(
x
)
\
{
root
}
∑
​
∣
y
​
(
t
)
−
y
​
(
Parent
(
t
)
)
∣


Но нам нужен не вклад каждого узла, а 
вклад каждого признака
 в прогноз для интересующего объекта. Для этого для каждого признака 
$i$
i
 найдём множество тех узлов 
$T_i$
T
i
​
, где этот признак использовался в решающем правиле дерева.


Тогда вклад 
$i$
i
-го признака в прогноз 
$\hat{y}(\mathbf{x})$
y
^
​
(
x
)
 считается как суммарный вклад по узлам, учитывающим 
$i$
i
-й признак:


$\text{Importance}(i|\mathbf{x})=\sum_{t\in\text{path}\left(\mathbf{x}\right)\backslash\left\{ \text{root}\right\} \textcolor{red}{\cap T_i} }|\widehat{y}\left(t\right)-\widehat{y}\left(\text{Parent}\left(t\right)\right)|$
Importance
(
i
∣
x
)
=
t
∈
path
(
x
)
\
{
root
}
∩
T
i
​
∑
​
∣
y
​
(
t
)
−
y
​
(
Parent
(
t
)
)
∣


Так мы рассчитаем вклад каждого признака в прогноз определённого объекта 
$\mathbf{x}$
x
. Метод был предложен в 
[3]
.


Обратим внимание, что


$\sum_i \text{Importance}(i|\mathbf{x}) = \hat{y}(\mathbf{x}),$
i
∑
​
Importance
(
i
∣
x
)
=
y
^
​
(
x
)
,


причём часть признаков будут оказывать положительное, а часть - отрицательное влияние на величину прогноза.




Метод обобщается на ансамбль решающих деревьев (бэггинг, бустинг, решающий лес и др.): для этого нужно усреднить вклады признаков по деревьям ансамбля.




Глобальная важность признака
Усредняя оценку важности по всем объектам выборки, получим 
глобальную важность признака
, похожую по смыслу на меру 
среднего изменения неопределённости
. Первая мера оценивает среднее изменение прогноза при учёте признака, а вторая - среднее влияние признака на снижение 
неопределённости прогнозов
.




Вы также можете прочитать про интерпретацию решающих деревьев в 
[4]
. А в 
следующем разделе учебника
 мы изучим подходы интерпретации прогнозов сложных моделей (balck-box models), таких как ансамбли алгоритмов и нейронные сети.


Литература
​






UC Irvine Machine Learning Repository: wine dataset.






Kanoki.org: decision tree in sklearn.






Blog.datadive.net: interpreting random forests.






Molnar C. Interpretable machine learning. – Lulu. com, 2020: Decision Tree.




Предыдущая страница
Интерпретация логистической регрессии
Следующая страница
Вопросы
Визуализация деревьев
Глобальная важность признаков
Вклад признака в отдельный прогноз
Литература
© 2023-25 
Виктор Китов.
 
Новости проекта.

