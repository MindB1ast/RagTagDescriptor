





Аналитическое решение для линейной регрессии | Машинное и глубокое обучение






[IMAGE]














Перейти к основному содержимому
[IMAGE]
Машинное обучение
Глубокое обучение
Обозначения
Лицензия
Машинное обучение
Введение
Основы машинного обучения
Подготовка данных
Классификаторы в общем виде
Метрические методы прогнозирования
Линейная регрессия и её обобщения
Линейная регрессия
Аналитическое решение для линейной регрессии
Регуляризация в линейной регрессии
Аналитическое решение для гребневой регрессии
Линейный ансамбль моделей
Регрессия опорных векторов
Orthogonal matching pursuit
Локально-линейная регрессия
Дополнительная литература
Вопросы
Оценка качества регрессии
Линейная классификация
Многоклассовая классификация набором бинарных классификаторов
Численная оптимизация
Оценка качества классификации
Решающие деревья
Переобучение и недообучение
Ансамбли моделей
Бустинг
Интерпретация простых моделей
Интерпретация сложных моделей
Заключение
Линейная регрессия и её обобщения
Аналитическое решение для линейной регрессии
Аналитическое решение для линейной регрессии


Веса линейной регрессии находятся 
из принципа минимизации наименьших квадратов
 (МНК, ordinary least squares, OLS):


$L(\mathbf{w})=\sum_{n=1}^N (\mathbf{x}_n^T\mathbf{w}-y_n)^2\to\min_{\mathbf{w}}$
L
(
w
)
=
n
=
1
∑
N
​
(
x
n
T
​
w
−
y
n
​
)
2
→
w
min
​


Этот критерий по 
$\mathbf{w}$
w
 является выпуклым как суперпозиция линейной и выпуклой функции.


Тип векторов
Все вектора в этом разделе, как и глобально в книге, будем считать векторами-столбцами (а не строками). Это важно для понимания аналитических выкладок.


Будет ли суперпозиция двух выпуклых функций выпуклой?
Суперпозиция двух выпуклых функций может и не быть выпуклой, например, 
$f(g(x))=(\vert x \vert-1)^2$
f
(
g
(
x
))
=
(
∣
x
∣
−
1
)
2
 для
$g(x)=\vert x \vert - 1,\quad f(x)=x^2$
g
(
x
)
=
∣
x
∣
−
1
,
f
(
x
)
=
x
2
Однако суперпозиция 
линейной
 и выпуклой функции всегда выпукла - докажите самостоятельно.


Поэтому не только необходимым, но и достаточным условием минимума потерь будет покомпонентное равенство нулю 
градиента
 функции потерь. Причем, также в силу выпуклости функции, будет гарантия, что найденный минимум будет 
глобальным минимумом
 функционала, т.е. обеспечивать наименьшее значение функции потерь среди всех возможных.


Мы говорим про градиент, а не про производную, поскольку при дифференцировании скалярной функции 
$L(\mathbf{w})\in\mathbb{R}$
L
(
w
)
∈
R
 по вектору 
$\mathbf{w}\in \mathbb{R}^{D+1}$
w
∈
R
D
+
1
 получим вектор частных производных по каждой компоненте вектора 
$\mathbf{w}$
w
:


$\nabla L(\mathbf{w})=
\begin{pmatrix}
   \frac{\partial L}{\partial w_0} \\
   \frac{\partial L}{\partial w_1} \\
   \frac{\partial L}{\partial w_2} \\
   \cdots \\
   \frac{\partial L}{\partial w_D} \\
\end{pmatrix}$
∇
L
(
w
)
=
​
∂
w
0
​
∂
L
​
∂
w
1
​
∂
L
​
∂
w
2
​
∂
L
​
⋯
∂
w
D
​
∂
L
​
​
​


Найдем оптимальные веса аналитически. Поскольку функция потерь является выпуклой по весам, то не только необходимым, но и 
достаточным
 условием оптимальности будет покомпонентное равенство нулю градиента функции потерь:


$\nabla L(\widehat{\mathbf{w}})=2\sum_{n=1}^{N}\mathbf{x}_{n}\left(\mathbf{x}_{n}^{T}\widehat{\mathbf{w}}-y_{n}\right)=0$
∇
L
(
w
)
=
2
n
=
1
∑
N
​
x
n
​
(
x
n
T
​
w
−
y
n
​
)
=
0


$\left(\sum_{n=1}^{N}\mathbf{x}_{n}\mathbf{x}_{n}^{T}\right)\widehat{\mathbf{w}}=\sum_{n=1}^{N}\mathbf{x}_{n}y_{n}$
(
n
=
1
∑
N
​
x
n
​
x
n
T
​
)
w
=
n
=
1
∑
N
​
x
n
​
y
n
​


Перепишем условие, используя обозначения для матрицы объекты-признаки 
$X\in\mathbb{R}^{N\times D}$
X
∈
R
N
×
D
 и вектора откликов 
$Y\in \mathbb{R}^N$
Y
∈
R
N
:


$X^{T}X\widehat{\mathbf{w}}=X^T Y$
X
T
X
w
=
X
T
Y


Откуда получаем аналитическое итоговое решение для линейной регрессии:


$\widehat{\mathbf{w}}=(X^{T}X)^{-1}X^{T}Y$
w
=
(
X
T
X
)
−
1
X
T
Y


Обратим внимание, что решение не будет существовать, если матрица 
$X^T X$
X
T
X
 вырождена, что эквивалентно тому, что 
ранг матрицы
 
$\text{rg}(X^T X)=\text{rg}(X)<D$
rg
(
X
T
X
)
=
rg
(
X
)
<
D
.


Задача
Докажите, что 
$\text{rg}(X^T X)=\text{rg}(X)$
rg
(
X
T
X
)
=
rg
(
X
)
 для любой матрицы 
$X$
X
.


Последнее, в свою очередь, означает линейную зависимость между признаками, т.е. что найдётся такой набор весов 
$\bm{a}=[a_0,a_1,...a_D]$
a
=
[
a
0
​
,
a
1
​
,
...
a
D
​
]
, что


$\mathbf{x}^T \bm{a}=0 \quad \forall \mathbf{x} \,(\text{выполнено для любых векторов признаков } \mathbf{x})$
x
T
a
=
0
∀
x
(
выполнено
 
для
 
любых
 
векторов
 
признаков
 
x
)


В этом случае один из признаков лишний, поскольку может быть получен как линейная комбинация других признаков, а само решение неоднозначно, поскольку если 
$\hat{\mathbf{w}}$
w
^
 - решение, то и 
$\mathbf{w}+k\bm{a}$
w
+
k
a
 - тоже решение для любого 
$k\in \mathbb{R}$
k
∈
R
, поскольку


$\hat{y}(\mathbf{x})=\mathbf{x}^T\hat{\mathbf{w}}=\mathbf{x}^T\hat{\mathbf{w}}+0=\mathbf{x}^T\hat{\mathbf{w}}+k\mathbf{x}^T\bm{a}=\mathbf{x}^T(\mathbf{w}+k\bm{a})$
y
^
​
(
x
)
=
x
T
w
^
=
x
T
w
^
+
0
=
x
T
w
^
+
k
x
T
a
=
x
T
(
w
+
k
a
)


Следовательно, если матрица 
$X^T X$
X
T
X
 необратима и аналитическая оценка весов не определена, то нужно уменьшить число признаков через 
отбор признаков/снижение размерности
.
Предыдущая страница
Линейная регрессия
Следующая страница
Регуляризация в линейной регрессии
© 2023-25 
Виктор Китов.
 
Новости проекта.

