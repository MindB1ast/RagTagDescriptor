





Линейная классификация | Машинное и глубокое обучение






[IMAGE]














Перейти к основному содержимому
[IMAGE]
Машинное обучение
Глубокое обучение
Обозначения
Лицензия
Машинное обучение
Введение
Основы машинного обучения
Подготовка данных
Классификаторы в общем виде
Метрические методы прогнозирования
Линейная регрессия и её обобщения
Оценка качества регрессии
Линейная классификация
Линейная классификация
Оценка весов линейного классификатора
Бинарная логистическая регрессия
Многоклассовая логистическая регрессия
Метод опорных векторов
Дополнительная литература
Вопросы
Многоклассовая классификация набором бинарных классификаторов
Численная оптимизация
Оценка качества классификации
Решающие деревья
Переобучение и недообучение
Ансамбли моделей
Бустинг
Интерпретация простых моделей
Интерпретация сложных моделей
Заключение
Линейная классификация
Линейная классификация
Содержание этой страницы
Линейная классификация


Напомним из 
более ранней главы
, что задача классификации состоит в предсказании дискретного отклика:


$y \in \{1,2,....C\}$
y
∈
{
1
,
2
,
....
C
}


и осуществляется по правилу


$\hat{y}=\arg\max_{c\in \{1,2,...C\}} g_c(\mathbf{x}),$
y
^
​
=
ar
g
c
∈
{
1
,
2
,
...
C
}
max
​
g
c
​
(
x
)
,


где 
$g_1(\mathbf{x}),...g_C(\mathbf{x})$
g
1
​
(
x
)
,
...
g
C
​
(
x
)
 - дискриминантные функции, измеряющие рейтинг класса для объекта 
$\mathbf{x}$
x
.


Линейный многоклассовый классификатор
​


Линейный многоклассовый классификатор
 (linear multiclass classifier) - классификатор, у которого все дискриминантные функции могут быть представлены в виде линейных функций:


$\begin{matrix}
   g_1(\mathbf{x})=w_{01}+\mathbf{x}^T \mathbf{w}_1 \\
   g_2(\mathbf{x})=w_{02}+\mathbf{x}^T \mathbf{w}_2 \\
   \cdots \\
   g_C(\mathbf{x})=w_{0C}+\mathbf{x}^T \mathbf{w}_С \\
\end{matrix}$
g
1
​
(
x
)
=
w
01
​
+
x
T
w
1
​
g
2
​
(
x
)
=
w
02
​
+
x
T
w
2
​
⋯
g
C
​
(
x
)
=
w
0
C
​
+
x
T
w
С
​
​


Для спецификации линейного классификатора нужно задать 
$C$
C
 смещений 
$w_{01},w_{02},...w_{0C}\in \mathbb{R}$
w
01
​
,
w
02
​
,
...
w
0
C
​
∈
R
 и 
$C$
C
 векторов из коэффициентов при каждом признаке 
$\mathbf{w}_1,\mathbf{w}_2,...\mathbf{w}_C\in\mathbb{R}^D$
w
1
​
,
w
2
​
,
...
w
C
​
∈
R
D
. Таким образом, для спецификации достаточно 
$C(D+1)$
C
(
D
+
1
)
 параметров.


На самом деле меньше...
Поскольку дискриминантные функции определены с точностью до сдвига на произвольную функцию (докажите!), то можно всегда смещать на 
$g_C(x)$
g
C
​
(
x
)
, получая, что рейтинг последнего класса будет равен тождественному нулю. И для эквивалентной спецификации линейного классификатора будет достаточно всего 
$(C-1)(D+1)$
(
C
−
1
)
(
D
+
1
)
 параметров.


Граница между 
$i$
i
-м и 
$j$
j
-м классом определяется из условия:


$\{\mathbf{x}: g_i(\mathbf{x})=g_j(\mathbf{x})\} = \{\mathbf{x}: w_{0i}+\mathbf{x}^T \mathbf{w}_i = w_{0j}+\mathbf{x}^T \mathbf{w}_j\}$
{
x
:
g
i
​
(
x
)
=
g
j
​
(
x
)}
=
{
x
:
w
0
i
​
+
x
T
w
i
​
=
w
0
j
​
+
x
T
w
j
​
}


Поскольку это линейное уравнение, то границы между парой классов всегда будет линейной гиперплоскостью, а областью отнесения объектов к определённому классу будет выпуклый многогранник как пересечение выделяющих этот класс полуплоскостей относительно каждого из альтернативных классов.


Линейный бинарный классификатор
​


Линейный бинарный классификатор
 (linear binary classifier) решает задачу классификации на два класса, называемые положительным и отрицательным:


$y\in\{+1,-1\}$
y
∈
{
+
1
,
−
1
}


Как правило, в качестве положительного класса выбирают целевой класс, представляющий интерес и требующий дальнейшей обработки, а в качестве отрицательного - фоновый. Например, при классификации, болен ли пациент или здоров, больных относят к положительному классу, а здоровых - к отрицательному. Обычно положительный класс встречается реже, чем отрицательный.


Прогноз для линейного бинарного классификатора строится по правилу:


$\widehat{y}(\mathbf{x})=\arg\max_{c\in\{+1,-1\}}\{w_{0c}+\mathbf{w}_{c}^{T}\mathbf{x}\}=\text{sign}\left(w_{0,+1}+\mathbf{w}_{+1}^{T}\mathbf{x}-w_{0,-1}-\mathbf{w}_{-1}^{T}\mathbf{x}\right)=\text{sign}\left(w_0+\mathbf{w}^{T}\mathbf{x}\right),$
y
​
(
x
)
=
ar
g
c
∈
{
+
1
,
−
1
}
max
​
{
w
0
c
​
+
w
c
T
​
x
}
=
sign
(
w
0
,
+
1
​
+
w
+
1
T
​
x
−
w
0
,
−
1
​
−
w
−
1
T
​
x
)
=
sign
(
w
0
​
+
w
T
x
)
,


где мы ввели обозначения:


$\begin{align*}
w_0&=w_{0,+1}-w_{0,-1}\\
\mathbf{w}&=\mathbf{w}_{+1}-\mathbf{w}_{-1}
\end{align*}$
w
0
​
w
​
=
w
0
,
+
1
​
−
w
0
,
−
1
​
=
w
+
1
​
−
w
−
1
​
​


а функция 
$\text{sign}(u)$
sign
(
u
)
 извлекает знак аргумента:


$\text{sign}(u)=
\begin{cases}
+1, \quad u\ge 0 \\
-1, \quad u<0 \\
\end{cases}\\$
sign
(
u
)
=
{
+
1
,
u
≥
0
−
1
,
u
<
0
​


Величина 
$w_0+\mathbf{w}^{T}\mathbf{x}$
w
0
​
+
w
T
x
 является 
относительной дискриминантной функцией
 или 
относительным рейтингом
. Она характеризует насколько положительный класс лучше подходит для объекта 
$\mathbf{x}$
x
, чем отрицательный.


Геометрическая интерпретация
​


Расстояние от 
$\mathbf{x}$
x
 до гиперплоскости 
$H$
H
, задаваемым уравнением


$\{\mathbf{x}: w_0+\mathbf{w}^T \mathbf{x}=0\}$
{
x
:
w
0
​
+
w
T
x
=
0
}


можно посчитать как


$\rho(\mathbf{x},H)=\frac{w_0+\mathbf{w}^T \mathbf{x}}{\|\mathbf{w}\|}$
ρ
(
x
,
H
)
=
∥
w
∥
w
0
​
+
w
T
x
​


Доказательство этого факта приведено ниже.


Обратим внимание, что это расстояние 
со знаком
, то есть оно может быть как положительным, так и отрицательным, в зависимости от того, 
с какой именно стороны точка лежит от гиперплоскости
.


Из последней формулы видно, что относительная дискриминантная функция пропорционально с коэффициентом 
$1/\|\mathbf{w}\|$
1/∥
w
∥
 расстоянию (со знаком) от точки 
$\mathbf{x}$
x
 до разделяющей гиперплоскости.






Если она принимает большие по модулю значения, то 
$\mathbf{x}$
x
 лежит в глубине того или иного класса, а если малые - то у границы между классами.






По знаку дискриминантной функции можно судить о том, с какой именно стороны от разделяющей гиперплоскости оказался объект.






Если дискриминантная функция равна нулю, то объект попадает строго на границу между классами.






Вектор нормали
​


Утверждение:


Вектор 
$\mathbf{w}$
w
, задающий гиперплоскость


$H = \{\mathbf{x} \in \mathbb{R}^n : w_0 + \mathbf{w}^\top \mathbf{x} = 0\}$
H
=
{
x
∈
R
n
:
w
0
​
+
w
⊤
x
=
0
}


является нормалью (перпендикулярным вектором) к этой гиперплоскости.


Доказательство:


Пусть 
$\mathbf{x}_1, \mathbf{x}_2 \in H$
x
1
​
,
x
2
​
∈
H
 — две произвольные точки гиперплоскости. По определению гиперплоскости имеем:


$w_0 + \mathbf{w}^\top \mathbf{x}_1 = 0, \quad w_0 + \mathbf{w}^\top \mathbf{x}_2 = 0.$
w
0
​
+
w
⊤
x
1
​
=
0
,
w
0
​
+
w
⊤
x
2
​
=
0.


Вычислим разность:


$\mathbf{w}^\top \mathbf{x}_1 - \mathbf{w}^\top \mathbf{x}_2 = 0.$
w
⊤
x
1
​
−
w
⊤
x
2
​
=
0.


Перепишем:


$\mathbf{w}^\top (\mathbf{x}_1 - \mathbf{x}_2) = 0.$
w
⊤
(
x
1
​
−
x
2
​
)
=
0.


Вектор 
$\mathbf{x}_1 - \mathbf{x}_2$
x
1
​
−
x
2
​
 лежит в гиперплоскости 
$H$
H
 и является произвольным направляющим вектором этой плоскости. Из равенства выше следует, что 
$\mathbf{w}$
w
 ортогонален любому вектору, лежащему в 
$H$
H
.


Следовательно, 
$\mathbf{w}$
w
 является нормалью к гиперплоскости 
$H$
H
.


$\square$
□


Расстояние до гиперплоскости
​


Утверждение:


Расстояние от точки 
$\mathbf{x}$
x
 до гиперплоскости


$H = \{\mathbf{x} \in \mathbb{R}^n : w_0 + \mathbf{w}^\top \mathbf{x} = 0\}$
H
=
{
x
∈
R
n
:
w
0
​
+
w
⊤
x
=
0
}


выражается формулой


$\rho(\mathbf{x}, H) = \frac{|w_0 + \mathbf{w}^\top \mathbf{x}|}{\|\mathbf{w}\|}.$
ρ
(
x
,
H
)
=
∥
w
∥
∣
w
0
​
+
w
⊤
x
∣
​
.


Доказательство:


Пусть 
$\mathbf{p}$
p
 — ортогональная проекция точки 
$\mathbf{x}$
x
 на гиперплоскость 
$H$
H
. Тогда, поскольку 
$\mathbf{w}$
w
 - вектор нормали, существует число 
$t \in \mathbb{R}$
t
∈
R
 такое, что


$\mathbf{p} = \mathbf{x} - t \mathbf{w}.$
p
=
x
−
t
w
.


Поскольку 
$\mathbf{p} \in H$
p
∈
H
, по определению гиперплоскости выполняется


$w_0 + \mathbf{w}^\top \mathbf{p} = 0.$
w
0
​
+
w
⊤
p
=
0.


Подставим выражение для 
$\mathbf{p}$
p
:


$w_0 + \mathbf{w}^\top (\mathbf{x} - t \mathbf{w}) = 0.$
w
0
​
+
w
⊤
(
x
−
t
w
)
=
0.


Раскроем скобки:


$w_0 + \mathbf{w}^\top \mathbf{x} - t \mathbf{w}^\top \mathbf{w} = 0.$
w
0
​
+
w
⊤
x
−
t
w
⊤
w
=
0.


Так как 
$\mathbf{w}^\top \mathbf{w} = \|\mathbf{w}\|^2$
w
⊤
w
=
∥
w
∥
2
, выразим 
$t$
t
:


$t = \frac{w_0 + \mathbf{w}^\top \mathbf{x}}{\|\mathbf{w}\|^2}.$
t
=
∥
w
∥
2
w
0
​
+
w
⊤
x
​
.


Расстояние от точки 
$\mathbf{x}$
x
 до гиперплоскости равно длине вектора 
$\mathbf{x} - \mathbf{p}$
x
−
p
:


$\rho(\mathbf{x}, H) = \|\mathbf{x} - \mathbf{p}\| = \|t \mathbf{w}\| = |t| \cdot \|\mathbf{w}\| = \frac{|w_0 + \mathbf{w}^\top \mathbf{x}|}{\|\mathbf{w}\|}.$
ρ
(
x
,
H
)
=
∥
x
−
p
∥
=
∥
t
w
∥
=
∣
t
∣
⋅
∥
w
∥
=
∥
w
∥
∣
w
0
​
+
w
⊤
x
∣
​
.


$\square$
□


Литература
​




Math.stackexchange: distance from a point to a hyperplane.


Предыдущая страница
Линейная классификация
Следующая страница
Оценка весов линейного классификатора
Линейный многоклассовый классификатор
Линейный бинарный классификатор
Геометрическая интерпретация
Вектор нормали
Расстояние до гиперплоскости
Литература
© 2023-25 
Виктор Китов.
 
Новости проекта.

