





Методы построения базовых моделей | Машинное и глубокое обучение






[IMAGE]














Перейти к основному содержимому
[IMAGE]
Машинное обучение
Глубокое обучение
Обозначения
Лицензия
Машинное обучение
Введение
Основы машинного обучения
Подготовка данных
Классификаторы в общем виде
Метрические методы прогнозирования
Линейная регрессия и её обобщения
Оценка качества регрессии
Линейная классификация
Многоклассовая классификация набором бинарных классификаторов
Численная оптимизация
Оценка качества классификации
Решающие деревья
Переобучение и недообучение
Ансамбли моделей
Ансамбли моделей
Математическое обоснование ансамблей
Простая агрегация в ансамблях
Методы построения базовых моделей
Настройка на разных фрагментах обучающей выборки
Ансамбли рандомизированных деревьев
Стэкинг
Дополнительная литература
Вопросы
Бустинг
Интерпретация простых моделей
Интерпретация сложных моделей
Заключение
Ансамбли моделей
Методы построения базовых моделей
Методы построения базовых моделей


Для построения ансамбля необходимо строить много базовых моделей. Можно брать модели из разных семейств (решающие деревья, линейные модели, метод K ближайших соседей и др.), но базовые модели можно строить и в рамках одного семейства следующими способами:






Использовать разные гиперпараметры
. Например, разное 
$K$
K
 в методе 
$K$
K
 ближайших соседей или разную допустимую глубину при настройке решающих деревьев.






Использовать разные начальные инициализации при настройке градиентными методами
. Этот метод приводит к разным прогнозирующим алгоритмам, если целевая функция невыпукла, например, при настройке нейросетей.






Использовать разную инициализацию для генератора случайных чисел (random seed)
. Этот метод применим только для для рандомизированных моделей, то есть моделей, использующих случайность в процессе своей настройки, такими как 
случайный лес или особо случайные деревья
.






Предсказывать целевую переменную с разными функциями потерь
. Использование различных функций потерь приведёт к настройке разных окончательных моделей даже по одинаковым данным. Например, можно настраивать классификаторы по одинаковым данным, но с экспоненциальной, логистической и hinge функцией потерь!






Настраивать модель предсказывать различные преобразования целевой переменной
, например 
$\ln y, y^2, \sqrt{y}$
ln
y
,
y
2
,
y
​
. За счёт аппроксимации по-разному преобразованной целевой переменной базовые модели будут получаться разными. К прогнозам базовых моделей нужно потом применить обратное преобразование для возврата к исходной шкале 
$y$
y
 перед последующим усреднением.






Настраивать одну модель на разных фрагментах обучающей выборки
. Фрагменты можно выбирать по-разному, и этому посвящён следующий раздел.






Параллелизация вычислений
Каждая базовая модель настраивается 
независимо от остальных
, поэтому их можно 
настраивать параллельно
, используя разные ядра процессора либо даже на разных компьютерах.
Предыдущая страница
Простая агрегация в ансамблях
Следующая страница
Настройка на разных фрагментах обучающей выборки
© 2023-25 
Виктор Китов.
 
Новости проекта.

