





Стохастический градиентный спуск с инерцией | Машинное и глубокое обучение






[IMAGE]














Перейти к основному содержимому
[IMAGE]
Машинное обучение
Глубокое обучение
Обозначения
Лицензия
Машинное обучение
Введение
Основы машинного обучения
Подготовка данных
Классификаторы в общем виде
Метрические методы прогнозирования
Линейная регрессия и её обобщения
Оценка качества регрессии
Линейная классификация
Многоклассовая классификация набором бинарных классификаторов
Численная оптимизация
Численные методы оптимизации
Метод градиентного спуска
Метод стохастического градиентного спуска
Мониторинг сходимости
Стохастический градиентный спуск с инерцией
Метод Ньютона
Вопросы
Оценка качества классификации
Решающие деревья
Переобучение и недообучение
Ансамбли моделей
Бустинг
Интерпретация простых моделей
Интерпретация сложных моделей
Заключение
Численная оптимизация
Стохастический градиентный спуск с инерцией
Содержание этой страницы
Стохастический градиентный спуск с инерцией


Метод стохастического градиентного спуска с инерцией
 (stochastic gradient descent with momentum, SGD+momentum 
[1]
) - это небольшое усложнение 
метода стохастического градиентного спуска
, которое позволяет ускорить сходимость и предотвратить застревание в локальных минимумах с небольшой окрестностью. Также он позволяет быстрее проскакивать точки перегиба и другие области медленных изменений функции потерь.


Напомним метод обычного стохастического градиентного спуска:




инициализируем 
$t=0$
t
=
0
, а начальные веса 
$\mathbf{w}_0$
w
0
​
 случайно


пока не выполнено условие остановки:


            сэмплируем случайные объекты 
$I=\{n_1,...n_K\}$
I
=
{
n
1
​
,
...
n
K
​
}
 из 
$\{1,2,...N\}$
{
1
,
2
,
...
N
}


            
$\Delta \mathbf{w}_{t+1} := \varepsilon_t \frac{1}{K} \sum_{n\in I} \nabla_\mathbf{w} \mathcal{L}(\mathbf{x}_n,y_n|\mathbf{w}_t)$
Δ
w
t
+
1
​
:=
ε
t
​
K
1
​
∑
n
∈
I
​
∇
w
​
L
(
x
n
​
,
y
n
​
∣
w
t
​
)


            
$\mathbf{w}_{t+1}:= \mathbf{w}_t- \Delta \mathbf{w}_{t+1}$
w
t
+
1
​
:=
w
t
​
−
Δ
w
t
+
1
​


            
$t:=t+1$
t
:=
t
+
1




Метод стохастического градиентного спуска с инерцией считается аналогично, но в качестве вектора сдвига весов используется не только градиент по объектам минибатча, но и ранее посчитанные градиенты с небольшим весом, задаваемым гиперпараметром 
$\mu\in (0,1)$
μ
∈
(
0
,
1
)
:




инициализируем 
$t=0$
t
=
0
, а начальные веса 
$\mathbf{w}_0$
w
0
​
 случайно


пока не выполнено условие остановки:


            сэмплируем случайные объекты 
$I=\{n_1,...n_K\}$
I
=
{
n
1
​
,
...
n
K
​
}
 из 
$\{1,2,...N\}$
{
1
,
2
,
...
N
}


            
$\Delta \mathbf{w}_{t+1} := \textcolor{red}{\mu\Delta \mathbf{w}_t}+\varepsilon_t \frac{1}{K} \sum_{n\in I} \nabla_\mathbf{w} \mathcal{L}(\mathbf{x}_n,y_n|\mathbf{w}_t)$
Δ
w
t
+
1
​
:=
μ
Δ
w
t
​
+
ε
t
​
K
1
​
∑
n
∈
I
​
∇
w
​
L
(
x
n
​
,
y
n
​
∣
w
t
​
)


            
$\mathbf{w}_{t+1}:= \mathbf{w}_t- \Delta \mathbf{w}_{t+1}$
w
t
+
1
​
:=
w
t
​
−
Δ
w
t
+
1
​


            
$t:=t+1$
t
:=
t
+
1




Компонента 
$\mu\Delta \mathbf{w}_t$
μ
Δ
w
t
​
 называется 
инерцией
 (momentum) и позволяет ускорить сходимость за счёт объединения информации о текущем градиенте с ранее посчитанными градиентами. За счёт этого градиент получается менее зашумлённым случайностью выбора объектов текущего минибатча, что позволяет использовать более высокий шаг обучения и ускорить сходимость.




Отметим, однако, что выбор высокого 
$\mu$
μ
 делает метод менее чувствительным к текущему направлению максимального снижения функции, что может, наоборот, замедлить сходимость. На практике чаще всего используют 
$\mu=0.9$
μ
=
0.9
.




Рекурсивно подставляя вместо 
$\Delta \mathbf{w}_{t},\Delta \mathbf{w}_{t-1},\Delta \mathbf{w}_{t-2},...$
Δ
w
t
​
,
Δ
w
t
−
1
​
,
Δ
w
t
−
2
​
,
...
 формулу их расчёта, получим, что изменение весов 
$\Delta \mathbf{w}_{t+1}$
Δ
w
t
+
1
​
 зависит от всех ранее посчитанных градиентов с экспоненциально убывающими весами по номеру итерации (докажите!).


Учёт инерции позволяет проскакивать локальные минимумы с малой окрестностью, сходясь к более устойчивым минимумам с большей окрестностью, как показано на рисунке:


[IMAGE]


Детальное обоснование и анализ метода стохастического градиентного спуска представлено в 
[2]
.


Инерция Нестерова
​


Метод инерции Нестерова (Nestrov momentum 
[3]
) позволяет ещё немного ускорить сходимость за счёт того, что градиент вычисляется в точке, 
более близкой к новой оценке весов
 на следующей итерации:




инициализируем 
$t=0$
t
=
0
, а начальные веса 
$\mathbf{w}_0$
w
0
​
 случайно


пока не выполнено условие остановки:


            сэмплируем случайные объекты 
$I=\{n_1,...n_K\}$
I
=
{
n
1
​
,
...
n
K
​
}
 из 
$\{1,2,...N\}$
{
1
,
2
,
...
N
}


            
$\Delta \mathbf{w}_{t+1} := \mu\Delta \mathbf{w}_t+\varepsilon_t\frac{1}{K} \sum_{n\in I} \nabla_\mathbf{w} \mathcal{L}(\mathbf{x}_n,y_n|\mathbf{w}_t\textcolor{red}{-\mu\Delta \mathbf{w}_t})$
Δ
w
t
+
1
​
:=
μ
Δ
w
t
​
+
ε
t
​
K
1
​
∑
n
∈
I
​
∇
w
​
L
(
x
n
​
,
y
n
​
∣
w
t
​
−
μ
Δ
w
t
​
)


            
$\mathbf{w}_{t+1}:= \mathbf{w}_t-\Delta \mathbf{w}_{t+1}$
w
t
+
1
​
:=
w
t
​
−
Δ
w
t
+
1
​


            
$t:=t+1$
t
:=
t
+
1




На шаге 
$t$
t
 мы уже заранее знаем, что веса сдвинутся на величину инерции, поэтому можем считать градиент уже в сдвинутой точке. Подобное заглядывание вперёд позволяет 
заранее сместить градиент
, если на следующей итерации предвидится изменение рельефа функции потерь.


Более продвинутые техники
Для оптимизируемых функций со сложным рельефом (особенно в настройке нейросетей) применяются более продвинутые методы, такие как RMSprop и Adam, ключевая идея которых заключается в том, что шаг обучения 
$\varepsilon_t$
ε
t
​
 вычисляется независимо для каждой компоненты вектора весов. Это связано с тем, что вдоль одних осей в пространстве весов функция потерь меняется быстро, а вдоль других - медленно, поэтому, для ускорения сходимости, целесообразно адаптировать шаг обучения индивидуально для каждой оси. Эти методы 
описаны
 во второй части учебника, посвящённой нейросетям.


Литература
​






Поляк Б. Т. О некоторых способах ускорения сходимости итерационных методов //Журнал вычислительной математики и математической физики. – 1964. – Т. 4. – №. 5. – С. 791-803.






Shi B. On the hyperparameters in stochastic gradient descent with momentum //Journal of Machine Learning Research. – 2024. – Т. 25. – №. 236. – С. 1-40.






Нестеров Ю. Е. Метод минимизации выпуклых функций со скоростью
сходимости O(1/k2) // Доклады АН СССР. 1983. Т. 269, № 3. С. 543–547.




Предыдущая страница
Мониторинг сходимости
Следующая страница
Метод Ньютона
Инерция Нестерова
Литература
© 2023-25 
Виктор Китов.
 
Новости проекта.

