





Метод один-против-одного | Машинное и глубокое обучение






[IMAGE]














Перейти к основному содержимому
[IMAGE]
Машинное обучение
Глубокое обучение
Обозначения
Лицензия
Машинное обучение
Введение
Основы машинного обучения
Подготовка данных
Классификаторы в общем виде
Метрические методы прогнозирования
Линейная регрессия и её обобщения
Оценка качества регрессии
Линейная классификация
Многоклассовая классификация набором бинарных классификаторов
Метод один-против-всех
Метод один-против-одного
Кодирование с исправлением ошибок
Численная оптимизация
Оценка качества классификации
Решающие деревья
Переобучение и недообучение
Ансамбли моделей
Бустинг
Интерпретация простых моделей
Интерпретация сложных моделей
Заключение
Многоклассовая классификация набором бинарных классификаторов
Метод один-против-одного
Содержание этой страницы
Метод один-против-одного


Метод один-против-одного
 (one-vs-one) представляет собой альтернативный метод многоклассовой классификации с помощью набора бинарных классификаторов.


Для начала рассмотрим похожую задачу, когда у нас есть 
$C$
C
 футбольных команд, и нам нужно определить среди них самую сильную. Мы не можем заставить играть одновременно больше двух команд, поэтому проведём матчи между каждой парой команд. Всего таких пар будет 
$C(C-1)/2$
C
(
C
−
1
)
/2
. Самой сильной командой тогда можно назначить ту команду, которая победила в максимальном числе матчей. Если лучшие команды побеждали одинаковое число раз, то назначим самой лучшей среди них ту, которая при этом ещё забила в соревнованиях наибольшее число голов.


Аналогично работает и метод один-против-одного. Настраивается 
$C(C-1)/2$
C
(
C
−
1
)
/2
 бинарных классификаторов, определяющих, какой класс лучше подходит объекту, 
если выбирать только между 
$i$
i
-м и 
$j$
j
-м классом
:


$\hat{y}=\text{sign}(g_{ij}(\mathbf{x}))$
y
^
​
=
sign
(
g
ij
​
(
x
))


Настройка каждого такого классификатора производится по подвыборке объектов, принадлежащих либо 
$i$
i
-му, либо 
$j$
j
-му классу.


Далее для нового объекта тестовой выборки назначается тот класс, 
который побеждает в максимальном числе попарных сравнений этого класса с другими классами
. Если несколько классов побеждают другие одинаковое число раз, то среди них выбирается тот, который побеждает остальных 
с максимальной суммой относительных дискриминантных функций соответствующих классификаторов.


Сложность оценивания
Метод один-против-одного требует оценки 
$C(C-1)/2$
C
(
C
−
1
)
/2
 бинарных классификаторов, в то время как метод один-против-всех - только 
$C$
C
 классификаторов. Однако в первом методе оценка производится каждый раз 
по подвыборке объектов
 двух соответствующих классов, в том время как во втором методе - каждый раз по всем объектам выборки.
Поэтому, несмотря на то, что классификаторов нужно оценить больше, совокупное время оценивания может получиться меньше, если сложность настройки модели нелинейно возрастает с объемом обучающей выборки, как, например, в 
ядерно обобщённом методе опорных векторов
.


Каким методом, одним-против-одного или одним-против-всех, вычислительно сложнее строить прогноз? 
Если считать, что сложность прогнозирования каждым бинарным классификатором примерно одинаковая, то метод один-против-одного сторит прогнозы быстрее, поскольку для него нужно пропустить объект через 
$C$
C
 классификаторов, в то время как для метода один-против-одного - уже через 
$C(C-1)/2$
C
(
C
−
1
)
/2
 классификаторов.


Реализацию метода на python см. в документации sklearn 
[1]
.


Литература
​




Документация sklearn: OneVsOneClassifier.


Предыдущая страница
Метод один-против-всех
Следующая страница
Кодирование с исправлением ошибок
Литература
© 2023-25 
Виктор Китов.
 
Новости проекта.

