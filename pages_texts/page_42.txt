





Orthogonal matching pursuit | Машинное и глубокое обучение






[IMAGE]














Перейти к основному содержимому
[IMAGE]
Машинное обучение
Глубокое обучение
Обозначения
Лицензия
Машинное обучение
Введение
Основы машинного обучения
Подготовка данных
Классификаторы в общем виде
Метрические методы прогнозирования
Линейная регрессия и её обобщения
Линейная регрессия
Аналитическое решение для линейной регрессии
Регуляризация в линейной регрессии
Аналитическое решение для гребневой регрессии
Линейный ансамбль моделей
Регрессия опорных векторов
Orthogonal matching pursuit
Локально-линейная регрессия
Дополнительная литература
Вопросы
Оценка качества регрессии
Линейная классификация
Многоклассовая классификация набором бинарных классификаторов
Численная оптимизация
Оценка качества классификации
Решающие деревья
Переобучение и недообучение
Ансамбли моделей
Бустинг
Интерпретация простых моделей
Интерпретация сложных моделей
Заключение
Линейная регрессия и её обобщения
Orthogonal matching pursuit
Содержание этой страницы
Orthogonal matching pursuit


Orthogonal Matching Pursuit
  регрессия (OMP regression, 
[1]
) - это комбинация отбора признаков и линейной регрессии, в которой строится максимально точная линейная модель с числом признаков, равным 
$K$
K
.


Алгоритм применяется, когда число признаков слишком велико и требуется построить компактную модель, зависящую лишь от небольшого их числа. Это полезно для повышения интерпретируемости модели, повышения скорости её работы и уменьшения переобучения.


Будем использовать следующие обозначения:






$A=\{1,2,...D\}$
A
=
{
1
,
2
,
...
D
}
 - множество всех признаков,






$A\setminus S$
A
∖
S
 - разность множеств (множество элементов 
$A$
A
, не содержащихся в 
$S$
S
),






$|S|$
∣
S
∣
 - число элементов множества 
$S$
S
.






Алгоритм итеративный, в котором на каждой итерации расширяется число используемых признаков 
$S$
S
 на единицу, а вектор ошибок текущей итерации на всех объектах выборки обозначим через 
$E\in \mathbb{R}^N$
E
∈
R
N
.


Алгоритм OMP-регрессии работает следующим образом:




Начальное множество признаков - пустое множество, а начальная модель - тождественный ноль:

$S:=\{\}$
S
:=
{
}


Поскольку начальная модель - тождественный ноль, то ошибки в начале совпадают с верными ответами:

$E:=Y$
E
:=
Y


пока 
$|S|<K$
∣
S
∣
<
K
:






выбрать признак 
$i\in A\setminus S$
i
∈
A
∖
S
, у которого максимальная корреляция с ошибками текущей модели 
$E$
E
.






добавить этот признак в число отобранных: 
$S:=S\cup\{i\}$
S
:=
S
∪
{
i
}






обучить линейную регрессию предсказывать 
$Y$
Y
, используя только отобранные признаки из 
$S$
S






обновить вектор 
$E$
E
 ошибками обновлённой модели








Таким образом, алгоритм OMP 
жадным образом
 (greedy search) добавляет максимально скоррелированные признаки с откликом по одному, пока не наберёт необходимое количество. В число признаков должна входить константа 1, чтобы модель могла выучить смещение.


Можно досрочно прерывать алгоритм, как только средние потери модели становятся ниже заданного порога. В этом случае достаточное число признаков может быть и меньше 
$K$
K
.


Альтернативный подход
Для построения модели с минимальным числом признаков также можно использовать 
лассо-регрессию
 с гиперпараметром 
$\lambda$
λ
 подбираемым таким образом, чтобы настроенная модель зависела только от 
$K$
K
 признаков.


Литература
​




Rubinstein R., Zibulevsky M., Elad M. Efficient implementation of the K-SVD algorithm using batch orthogonal matching pursuit //Cs Technion. – 2008. – Т. 40. – №. 8. – С. 1-15.


Предыдущая страница
Регрессия опорных векторов
Следующая страница
Локально-линейная регрессия
Литература
© 2023-25 
Виктор Китов.
 
Новости проекта.

