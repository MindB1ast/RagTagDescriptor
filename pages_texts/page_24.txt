





Общий вид прогнозирующих функций | Машинное и глубокое обучение






[IMAGE]














Перейти к основному содержимому
[IMAGE]
Машинное обучение
Глубокое обучение
Обозначения
Лицензия
Машинное обучение
Введение
Основы машинного обучения
Подготовка данных
Классификаторы в общем виде
Общий вид прогнозирующих функций
Отступ классификации
Предсказание вероятностей и преобразование SoftMax
Метрические методы прогнозирования
Линейная регрессия и её обобщения
Оценка качества регрессии
Линейная классификация
Многоклассовая классификация набором бинарных классификаторов
Численная оптимизация
Оценка качества классификации
Решающие деревья
Переобучение и недообучение
Ансамбли моделей
Бустинг
Интерпретация простых моделей
Интерпретация сложных моделей
Заключение
Классификаторы в общем виде
Общий вид прогнозирующих функций
Содержание этой страницы
Общий вид прогнозирующих функций


Регрессия в общем виде
​


Регрессионная зависимость (в которой откликом является вещественное число) в общем виде записывается в виде некоторой функции, параметризованной вектором параметров 
$\mathbf{w}$
w
:


$\hat{y}=f_{\mathbf{w}}(\mathbf{x})$
y
^
​
=
f
w
​
(
x
)


Многоклассовый классификатор в общем виде
​


В многоклассовой классификации отклик принимает одно из 
$C$
C
 дискретных значений:


$y \in \{1,2,....C\}$
y
∈
{
1
,
2
,
....
C
}


Соответственно, каждый многоклассовый классификатор определяет внутри себя 
$C$
C
 
дискриминантных функций
 (discriminant functions) или 
функций рейтинга класса
 
$g_c(\mathbf{x})$
g
c
​
(
x
)
, 
$c=1,2,...C$
c
=
1
,
2
,
...
C
 - свою для каждого класса. По смыслу дискриминантная функция 
$g_c(\mathbf{x})$
g
c
​
(
x
)
 определяет, насколько хорошо объект 
$\mathbf{x}$
x
 подходит под  соответствующий класс 
$c$
c
.


В качестве прогноза классификатор выдаёт класс, обладающий максимальным рейтингом:


$\hat{y}=\arg\max_c g_c(\mathbf{x})$
y
^
​
=
ar
g
c
max
​
g
c
​
(
x
)




Часто дискриминатные функции определяют как линейные функции от признаков либо как условные вероятности классов при условии вектора признаков 
$\mathbf{x}$
x
, однако в общем случае это могут быть произвольные функции!




Пусть прогнозы классификатора определены в каждой точке 
$\mathbf{x}$
x
. Однозначно ли по классификатору определяются его дискриминантные функции?
Нет, не однозначно. Например, мы можем прибавить или вычесть любую константу одновременно из всех дискриминантных функций, и на прогноз это не окажет никакого влияния: максимум будет достигаться на том же самом классе.
На самом деле дискриминантные функции определены даже с точностью до монотонно возрастающего преобразования. Например, мы можем одновременно возвести в куб или экспоненцировать все дискриминантные функции, и это не окажет влияния на прогноз класса!


Каким уравнением задаётся граница между i-м и j-м классом?
Граница между i-м и j-м классом - это множество всех таких объектов, для которых рейтинг для i-го класса совпадает с рейтингом для j-го класса, т.е. это множество точек:
$\{\mathbf{x}: g_i(\mathbf{x})=g_j(\mathbf{x})\}$
{
x
:
g
i
​
(
x
)
=
g
j
​
(
x
)}


Подробнее о дискриминантных функциях можно прочитать в [1].


Бинарный классификатор в общем виде
​


В бинарной классификации возможных классов всего два, один из которых называется положительным, а другой - отрицательным:


$y\in \{+1,-1\}$
y
∈
{
+
1
,
−
1
}




В качестве положительного обычно выбирают 
целевой класс
, который встречается реже. Например, больной пациент при классификации людей на больных и здоровых или мошенническая транзакция при классификации на регулярные и мошеннические.




Прогноз строится по формуле


$\hat{y}=\arg\max_{c\in\{+1,-1\}}g_c(\mathbf{x})=\text{sign}(g_{+1}(\mathbf{x})-g_{-1}(\mathbf{x}))=\text{sign}(g(\mathbf{x})),$
y
^
​
=
ar
g
c
∈
{
+
1
,
−
1
}
max
​
g
c
​
(
x
)
=
sign
(
g
+
1
​
(
x
)
−
g
−
1
​
(
x
))
=
sign
(
g
(
x
))
,


где 
$g(\mathbf{x})=g_{+1}(\mathbf{x})-g_{-1}(\mathbf{x})$
g
(
x
)
=
g
+
1
​
(
x
)
−
g
−
1
​
(
x
)
 - 
относительная дискриминантная функция
, по смыслу определяющая, насколько положительный класс лучше подходит для объекта 
$\mathbf{x}$
x
, чем отрицательный, а функция знака возвращает знак аргумента:


$\text{sign}(u)=\begin{cases}
   +1, &\text{если } u>0 \\
   -1, &\text{если } u<0
\end{cases}$
sign
(
u
)
=
{
+
1
,
−
1
,
​
если
 
u
>
0
если
 
u
<
0
​


При 
$u=0$
u
=
0
 функция знака не определена, и её можно доопределить возвращать либо +1, либо -1.


Литература
​




Webb A. R., Copsey K.D. Statistical pattern recognition. – John Wiley & Sons, 2011: Discriminant functions.


Предыдущая страница
Классификаторы в общем виде
Следующая страница
Отступ классификации
Регрессия в общем виде
Многоклассовый классификатор в общем виде
Бинарный классификатор в общем виде
Литература
© 2023-25 
Виктор Китов.
 
Новости проекта.

