





Функции неопределённости решающих деревьев | Машинное и глубокое обучение






[IMAGE]














Перейти к основному содержимому
[IMAGE]
Машинное обучение
Глубокое обучение
Обозначения
Лицензия
Машинное обучение
Введение
Основы машинного обучения
Подготовка данных
Классификаторы в общем виде
Метрические методы прогнозирования
Линейная регрессия и её обобщения
Оценка качества регрессии
Линейная классификация
Многоклассовая классификация набором бинарных классификаторов
Численная оптимизация
Оценка качества классификации
Решающие деревья
Решающие деревья
Особенности прогнозов решающего дерева
Настройка решающего дерева
Функции неопределённости
Учёт пользовательской функции потерь
Обрезка решающих деревьев
Обработка пропущенных значений
Важность признаков
Анализ решающих деревьев
Обобщения решающих деревьев
Дополнительная литература
Вопросы
Переобучение и недообучение
Ансамбли моделей
Бустинг
Интерпретация простых моделей
Интерпретация сложных моделей
Заключение
Решающие деревья
Функции неопределённости
Содержание этой страницы
Функции неопределённости решающих деревьев


Задача регрессии
​


При предсказании вещественного отклика 
$y\in\mathbb{R}$
y
∈
R
 в качестве функции неопределённости (impurity function) используется 
дисперсия откликов
:


$\phi(t)=\frac{1}{\left|I_{t}\right|}\sum_{i\in I_{t}}\left(y_{i}-\text{mean}_{i\in I_{t}}(y_{i})\right)^{2}$
ϕ
(
t
)
=
∣
I
t
​
∣
1
​
i
∈
I
t
​
∑
​
(
y
i
​
−
mean
i
∈
I
t
​
​
(
y
i
​
)
)
2


где






$I_{t}=\{i_{1},...i_{K}\}$
I
t
​
=
{
i
1
​
,
...
i
K
​
}
 - множество индексов объектов, попадающих в узел 
$t$
t
,






$K=|I_t|$
K
=
∣
I
t
​
∣
 - количество таких объектов,






$\text{mean}_{i\in I_{t}}(y_{i})$
mean
i
∈
I
t
​
​
(
y
i
​
)
 - выборочное среднее по откликам объектов, попадающих в узел 
$t$
t
.






Также используется 
среднее абсолютное отклонение
 (mean absolute deviation):


$\phi(t)=\frac{1}{\left|I_{t}\right|}\sum_{i\in I_{t}}|y_{i}-\text{median}_{i\in I_{t}}(y_{i})|,$
ϕ
(
t
)
=
∣
I
t
​
∣
1
​
i
∈
I
t
​
∑
​
∣
y
i
​
−
median
i
∈
I
t
​
​
(
y
i
​
)
∣
,


где 
$\text{median}_{i\in I_{t}}(y_{i})$
median
i
∈
I
t
​
​
(
y
i
​
)
 - выборочная 
медиана
 откликов объектов узла 
$t$
t
.




Для более неопределённых данных с изменяющимся откликом обе меры неопределённости будут давать высокие значения. А в случае одинаковых откликов они будут равны нулю.




Задача классификации
​


Для классификации функции неопределённости 
$\phi(t)$
ϕ
(
t
)
 будут зависеть 
от вероятностей классов
 для объектов, попавших в узел 
$t$
t
. Ниже представлены популярные варианты этих функций:


название
на английском
формула
классификационная ошибка
classification error
$1-\max\{p_1,p_2,...p_C\}$
1
−
max
{
p
1
​
,
p
2
​
,
...
p
C
​
}
критерий Джини
Gini
$\sum_{i=1}^{C}p_{i}(1-p_{i})$
∑
i
=
1
C
​
p
i
​
(
1
−
p
i
​
)
энтропийный критерий
entropy
$-\sum_{i=1}^{C}p_{i}\ln p_{i}$
−
∑
i
=
1
C
​
p
i
​
ln
p
i
​


Обоснование функций неопределённости
​


Рассмотрим бинарную классификацию и некоторый узел дерева 
$t$
t
.


Пусть вероятность положительного класса  
$p(y=+1|t)=p$
p
(
y
=
+
1∣
t
)
=
p
, а отрицательного  
$p(y=-1|t)=1-p$
p
(
y
=
−
1∣
t
)
=
1
−
p
.


Зависимости функций неопределённости от 
$p$
p
 приведены ниже:


[IMAGE]




Как видим, максимальная неопределённость функций достигается в наиболее неопределённом случае, когда 
$p=0.5$
p
=
0.5
 и оба класса равновероятны. А минимум неопределённости достигается, когда 
$p=0$
p
=
0
 либо 
$p=1$
p
=
1
. В этих случаях все объекты узла принадлежат одному из классов, и неопределённость классификации отсутствует.




Для многоклассового случая представленные функции также измеряют степень неопределённости классов, достигая максимума при равномерном распределении классов, когда 
$p_1=p_2=...=p_C=\frac{1}{C}$
p
1
​
=
p
2
​
=
...
=
p
C
​
=
C
1
​
, а минимума, когда все объекты принадлежат одному из классов:


$p_1=0,\, ...\, p_{i-1}=0,\, p_i=1,\, p_{i+1}=0,\, ...\, p_C=0$
p
1
​
=
0
,
...
p
i
−
1
​
=
0
,
p
i
​
=
1
,
p
i
+
1
​
=
0
,
...
p
C
​
=
0


Интуитивно это можно понять следующим образом:


Классификационная ошибка
 измеряет ожидаемое число ошибок при классификации всех объектов 
максимально вероятным классом
, у которого вероятность появления 
$p_{max}=\max \{p_1,p_2,...p_C\}$
p
ma
x
​
=
max
{
p
1
​
,
p
2
​
,
...
p
C
​
}
. Очевидно, что вероятность ошибки при такой классификации будет 
$1-p_{max}$
1
−
p
ma
x
​
. Классификация будет безошибочной, когда в узле присутствуют объекты только одного класса, а максимум ошибок будет достигаться, когда все классы будут равновероятны.


Критерий Джини
 (Gini criterion) измеряет вероятность ошибки 
при случайном угадывании класса
 по правилу:


$\hat{y}=
\begin{cases}
1,& \text{ с вероятностью }p_1, \\
2,& \text{ с вероятностью }p_2,  \\
\cdots & \cdots \\
C, & \text{ с вероятностью }p_C.  \\
\end{cases}$
y
^
​
=
⎩
⎨
⎧
​
1
,
2
,
⋯
C
,
​
 
с
 
вероятностью
 
p
1
​
,
 
с
 
вероятностью
 
p
2
​
,
⋯
 
с
 
вероятностью
 
p
C
​
.
​


Тогда, расписывая вероятность ошибки по формуле полной вероятности 
[1]
, как раз и получим критерий Джини:


$p(\hat{y}\ne y)= \sum_{c=1}^C p(\hat{y}\ne y|y=1)p(y=1)= \sum_{c=1}^C (1-p_c)p_c$
p
(
y
^
​

=
y
)
=
c
=
1
∑
C
​
p
(
y
^
​

=
y
∣
y
=
1
)
p
(
y
=
1
)
=
c
=
1
∑
C
​
(
1
−
p
c
​
)
p
c
​


Эта ошибка будет максимальной, когда все классы равновероятны, и равняться нулю, когда все объекты принадлежат одному из классов.


Энтропийный критерий
 (entropy criterion) вычисляет энтропию случайной величины 
$y$
y
:


$y=
\begin{cases}
1,& \text{ с вероятностью }p_1,   \\
2,& \text{ с вероятностью }p_2,   \\
\cdots & \cdots \\
C, & \text{ с вероятностью }p_C.  \\
\end{cases}$
y
=
⎩
⎨
⎧
​
1
,
2
,
⋯
C
,
​
 
с
 
вероятностью
 
p
1
​
,
 
с
 
вероятностью
 
p
2
​
,
⋯
 
с
 
вероятностью
 
p
C
​
.
​


которая служит 
мерой её неопределённости
. Покажем это.


Определим количество информации, которую мы получаем при случайном событии, реализующимся с вероятностью 
$p$
p
, по формуле


$\text{Info(событие с вероятностью p)}=-\ln p$
Info(
событие
 
с
 
вероятностью
 p)
=
−
ln
p


График этой зависимости показан ниже:


[IMAGE]


Именно так определить получаемую информацию разумно, поскольку такая функция:






будет выдавать нулевую информацию при реализации события, у которого вероятность наступления равна 1 (происходит всегда);






стремится к бесконечности при 
$p\to 0$
p
→
0
 (событие происходит редко);






для двух независимых событий 
$A$
A
 и 
$B$
B
 будет выполнено свойство аддитивности:






$\text{Info}(A,B)=\text{Info}(A)+\text{Info}(B)$
Info
(
A
,
B
)
=
Info
(
A
)
+
Info
(
B
)


Тогда энтропия случайной величины 
$y$
y
 будет равна 
ожидаемому количеству информации
, которую мы получим, узнав реализацию этой случайной величины:


$\text{entropy(y)}=\mathbb{E}\{\text{Info(y)}\} = -\sum_{c=1}^C p_c \ln p_c$
entropy(y)
=
E
{
Info(y)
}
=
−
c
=
1
∑
C
​
p
c
​
ln
p
c
​


Максимум информации мы будем получать для случая, когда все классы равновероятны, а минимум - когда реализуется всегда только один из классов.


Задача
Докажите формально, что энтропия максимизируется, когда все классы равновероятны. Для этого нужно её промаксимизировать при ограничении, что вероятности суммируются в единицу. Технически для этого используется метод множителей Лагранжа 
[2]
.


Литература
​






Wikipedia: формула полной вероятности.






Wikipedia: метод множителей Лагранжа.




Предыдущая страница
Настройка решающего дерева
Следующая страница
Учёт пользовательской функции потерь
Задача регрессии
Задача классификации
Обоснование функций неопределённости
Литература
© 2023-25 
Виктор Китов.
 
Новости проекта.

