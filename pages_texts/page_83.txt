





Важность признаков | Машинное и глубокое обучение






[IMAGE]














Перейти к основному содержимому
[IMAGE]
Машинное обучение
Глубокое обучение
Обозначения
Лицензия
Машинное обучение
Введение
Основы машинного обучения
Подготовка данных
Классификаторы в общем виде
Метрические методы прогнозирования
Линейная регрессия и её обобщения
Оценка качества регрессии
Линейная классификация
Многоклассовая классификация набором бинарных классификаторов
Численная оптимизация
Оценка качества классификации
Решающие деревья
Решающие деревья
Особенности прогнозов решающего дерева
Настройка решающего дерева
Функции неопределённости
Учёт пользовательской функции потерь
Обрезка решающих деревьев
Обработка пропущенных значений
Важность признаков
Анализ решающих деревьев
Обобщения решающих деревьев
Дополнительная литература
Вопросы
Переобучение и недообучение
Ансамбли моделей
Бустинг
Интерпретация простых моделей
Интерпретация сложных моделей
Заключение
Решающие деревья
Важность признаков
Содержание этой страницы
Важность признаков


Важность в решающем дереве
​


Работу решающего дерева можно проинтерпретировать непосредственно, если дерево не слишком глубокое. Для интерпретации деревьев произвольной глубины можно анализировать, какое влияние оказывает каждый из признаков на его прогнозы. На значимость каждого признака влияют:






как часто признак использовался в правилах во внутренних узлах дерева;






какое число обучающих объектов прошло через узлы, использующие признак;






насколько правилам в этих узлах удавалось снизить неопределённость прогнозов.






Объединяя эти факторы вместе, важность признака 
$f$
f
 для решающего дерева (feature importance) рассчитывается по формуле:


$\text{Importance}(f) = \frac{1}{N}\sum_{t\in T(f)}N(t)\Delta\phi(t),$
Importance
(
f
)
=
N
1
​
t
∈
T
(
f
)
∑
​
N
(
t
)
Δ
ϕ
(
t
)
,


где:






$T(f)$
T
(
f
)
 - множество всех узлов дерева, использовавших признак 
$f$
f
 в своих правилах ветвления;






$N(t)$
N
(
t
)
 - число объектов выборки, проходящих через узел 
$t$
t
;






$\Delta\phi(t)$
Δ
ϕ
(
t
)
 - 
изменение функции неопределённости
 после применения правила ветвления в узле 
$t$
t
;






$N$
N
 - общее число объектов в обучающей выборке.






Эта мера важности признака называется 
средним изменением неопределённости
 (
mean decrease in impurity
 или MDI 
[1]
, 
[2]
).


MDI-важность рассчитывается на этапе первичного анализа данных и на этапе отбора признаков перед применением других нелинейных моделей. Причём эту меру считают не по единичному дереву, а по 
ансамблю случайных деревьев
, поскольку единичное дерево склонно переобучаться при построении до самого низа. С методом расчёта MDI-меры в библиотеке sklearn, используя алгоритм случайного леса, можно ознакомиться в 
[3]
 и 
[4]
, где этот метод сравнивается с 
перестановочным методом оценки важности
.


Важность в линейных моделях
​


Для измерения важности признаков также можно настроить линейную модель и анализировать полученные веса при признаках - чем они больше по модулю, тем признак важнее.




Поскольку веса при признаках обратно пропорциональны масштабу признаков, важно предварительно приводить признаки к единой шкале 
нормализацией
.




Однако эта мера покажет важность признака только в контексте 
линейного влияния на отклик
! Если признак оказывает существенное нелинейное влияние, то MDI-важность даст более адекватную оценку степени его влияния на прогноз.


Литература
​






Louppe G. et al. Understanding variable importances in forests of randomized trees //Advances in neural information processing systems. – 2013. – Т. 26.






Breiman L. Random forests //Machine learning. – 2001. – Т. 45. – С. 5-32.






Документация sklearn: feature importances with a forest of trees.






Документация sklearn: permutation Importance vs random forest feature Importance (MDI).




Предыдущая страница
Обработка пропущенных значений
Следующая страница
Анализ решающих деревьев
Важность в решающем дереве
Важность в линейных моделях
Литература
© 2023-25 
Виктор Китов.
 
Новости проекта.

