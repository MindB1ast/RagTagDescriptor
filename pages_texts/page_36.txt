





Линейная регрессия | Машинное и глубокое обучение






[IMAGE]














Перейти к основному содержимому
[IMAGE]
Машинное обучение
Глубокое обучение
Обозначения
Лицензия
Машинное обучение
Введение
Основы машинного обучения
Подготовка данных
Классификаторы в общем виде
Метрические методы прогнозирования
Линейная регрессия и её обобщения
Линейная регрессия
Аналитическое решение для линейной регрессии
Регуляризация в линейной регрессии
Аналитическое решение для гребневой регрессии
Линейный ансамбль моделей
Регрессия опорных векторов
Orthogonal matching pursuit
Локально-линейная регрессия
Дополнительная литература
Вопросы
Оценка качества регрессии
Линейная классификация
Многоклассовая классификация набором бинарных классификаторов
Численная оптимизация
Оценка качества классификации
Решающие деревья
Переобучение и недообучение
Ансамбли моделей
Бустинг
Интерпретация простых моделей
Интерпретация сложных моделей
Заключение
Линейная регрессия и её обобщения
Линейная регрессия
Содержание этой страницы
Линейная регрессия


Идея метода
​


В 
линейной регрессии
 (linear regression) прогноз строится как линейная комбинация признаков:


$\hat{y}(\mathbf{x})=w_0+w_1 x^1+w_2 x^2+...+w_D x^D$
y
^
​
(
x
)
=
w
0
​
+
w
1
​
x
1
+
w
2
​
x
2
+
...
+
w
D
​
x
D


Если ввести в признаки константную единицу и использовать векторное обозначение


$\begin{align*}
   \mathbf{x}&=[1,x^1,x^2,...x^D]^T, \\
   \mathbf{\mathbf{w}}&=[w_0,w_1,w_2,...w_D]^T,
\end{align*}$
x
w
​
=
[
1
,
x
1
,
x
2
,
...
x
D
]
T
,
=
[
w
0
​
,
w
1
​
,
w
2
​
,
...
w
D
​
]
T
,
​


то прогноз линейной регрессии можно записать в компактном виде:


$\hat{y}(\mathbf{x})=\mathbf{x}^T \mathbf{w}$
y
^
​
(
x
)
=
x
T
w


Модель основана на следующих предположениях:






каждый признак 
$x^{i}$
x
i
 линейно влияет на отклик с коэффициентом 
$w_{i}$
w
i
​
;






характер влияния каждого признака постоянен и не зависит от значений других признаков.






Достоинства линейной регрессии
​


Модель линейной регрессии простая и интерпретируемая, быстро работает и содержит мало параметров, поэтому наименее склонна к переобучению.




Если число признаков 
$D$
D
 велико, а число объектов в обучающей выборке 
$N$
N
 мало, то при тщательно подобранной 
регуляризации
 модель может оказаться наилучшей среди всех возможных!




Другим достоинством линейной регрессии является наличие 
аналитического решения
 (при минимизации суммы квадратов прогнозов), которое является глобальным минимумом функции потерь, т.е. наилучшим значением из всех возможных.


Недостатки линейной регрессии
​


Предположения линейной регрессии довольно просты и на практике, скорее всего, выполняться не будут: признаки влияют скорее всего нелинейно и характер этого влияния зависит от значений других признаков.




Например, при оценке стоимости квартиры по её характеристикам, стоимость нелинейным образом зависит от увеличения расстояния до метро, причем характер этого влияния разный в зависимости от того, есть ли рядом остановка общественного транспорта или нет.




Нарушение модельных предположений будет приводить к меньшей точности прогнозов.


Моделирование более сложных зависимостей
Ограничения линейной регрессии можно исправить вручную, если в качестве признаков добавлять 
нелинейные трансформации
 исходных признаков. Тогда модель в терминах исходных признаков будет получаться нелинейной! Если трансформации будут зависеть не от каждого признака по отдельности, а сразу от нескольких признаков, то характер влияния признаков также будет варьироваться в зависимости от значений других признаков.


Ранее мы обсуждали, что для многих методов машинного обучения масштаб признаков (диапазон принимаемых значений) влияет на прогнозы. Например, признак [вес] мы можем измерять в килограммах, граммах или тоннах, и это будет оказывать влияние на прогноз.


Будет ли изменение масштаба признаков (после перенастройки модели с новым масштабом) влиять на прогнозы линейной регрессии?
Нет, не будет. Например, если признак уменьшить в 100 раз, то соответствующий коэффициент при признаке увеличится в 100 раз, а итоговый прогноз не изменится.


Связь с вероятностной моделью
Минимизация суммы квадратов отклонений соответствует применению метода максимального правдоподобия к вероятностной модели
$\hat{y}(\mathbf{x})=w_0+w_1 x^1+w_2 x^2+...+w_D x^D+\varepsilon$
y
^
​
(
x
)
=
w
0
​
+
w
1
​
x
1
+
w
2
​
x
2
+
...
+
w
D
​
x
D
+
ε
где 
$\varepsilon\sim\mathcal{N}(0,\sigma^2)$
ε
∼
N
(
0
,
σ
2
)
 - шум, распределённый согласно 
нормальному распределению
 (докажите).


А если бы мы взяли 
$\varepsilon$
ε
 из 
распределения Лапласа
, то чему бы соответствовал метод максимального правдоподобия?
Минимизации суммы модулей ошибок прогнозов (докажите).


Пример запуска в Python
​


Пример использования линейной регрессии в библиотеке sklearn:


from
 sklearn
.
linear_model 
import
 LinearRegression
from
 sklearn
.
metrics 
import
 mean_absolute_error
X_train
,
 X_test
,
 Y_train
,
 Y_test 
=
 get_demo_regression_data
(
)
  
model 
=
 LinearRegression
(
)
     
# инициализация модели
model
.
fit
(
X_train
,
Y_train
)
     
# обучение модели                
Y_hat 
=
 model
.
predict
(
X_test
)
  
# построение прогнозов
print
(
f'Средний модуль ошибки 
(
MAE
)
:
 \
        
{
mean_absolute_error
(
Y_test
,
 Y_hat
)
:
.
2f
}
'
)
    




Больше информации
. 
Полный код
.


В следующих главах мы рассмотрим аналитический вывод весов для линейной регрессии, введение регуляризации в модель и всевозможные усложнения и варианты применения модели.


Дополнительно о линейной регрессии можно прочитать в википедии 
[1]
, а также в документации sklearn 
[2]
.


Литература
​






Wikipedia: linear regression.






Документация sklearn: linear models.




Предыдущая страница
Линейная регрессия и её обобщения
Следующая страница
Аналитическое решение для линейной регрессии
Идея метода
Достоинства линейной регрессии
Недостатки линейной регрессии
Пример запуска в Python
Литература
© 2023-25 
Виктор Китов.
 
Новости проекта.

