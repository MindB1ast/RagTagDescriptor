





Генерация признаков | Машинное и глубокое обучение






[IMAGE]














Перейти к основному содержимому
[IMAGE]
Машинное обучение
Глубокое обучение
Обозначения
Лицензия
Машинное обучение
Введение
Основы машинного обучения
Подготовка данных
Фильтрация выбросов
Заполнение пропусков
Обработка временного признака
Обработка категориальных признаков
Нормализация признаков
Генерация признаков
Сокращение числа признаков
Преобразование целевой переменной
Вопросы
Классификаторы в общем виде
Метрические методы прогнозирования
Линейная регрессия и её обобщения
Оценка качества регрессии
Линейная классификация
Многоклассовая классификация набором бинарных классификаторов
Численная оптимизация
Оценка качества классификации
Решающие деревья
Переобучение и недообучение
Ансамбли моделей
Бустинг
Интерпретация простых моделей
Интерпретация сложных моделей
Заключение
Подготовка данных
Генерация признаков
Содержание этой страницы
Генерация признаков


Исходных признаков может оказаться недостаточно для построения точной модели. Например, линейные методы моделируют только линейные зависимости отклика от признаков, а реальная зависимость может быть нелинейной, поэтому модель будет работать неточно. Для повышения точности моделей 
генерируют новые признаки
 из существующих (feature engineering). Признаки желательно генерировать так, чтобы они как можно точнее соответствовали фрагментам реальной зависимости в данных, из которых модель уже соберёт итоговое решение. Например, в задаче автоматической оценки стоимости земельного участка нам может быть известна его длина и ширина. Логично сгенерировать новый признак, равный их произведению и соответствующий площади участка.


Подходы для генерации новых признаков
​






Можно генерировать признаки, соответствующие интерпретируемым характеристикам, влияющим на отклик. Зная, например, длину и ширину участка, можно взять их произведение, что будет соответствовать его площади. При  оценке квартир это может быть отношение жилой площади к числу комнат, чтобы определить средний размер каждой комнаты и т.д. Можно использовать признаки, исходящие из общей логики, например, предварительно кластеризовать все объекты и в качестве дополнительного признака подставлять номер класса, в который попал объект. При работе с людьми по их возрасту можно сгенерировать три бинарных признака


$\mathbb{I}\{age < 18\}, \mathbb{I}\{18 \le age < 65\}, \mathbb{I}\{age\ge 65\}.$
I
{
a
g
e
<
18
}
,
I
{
18
≤
a
g
e
<
65
}
,
I
{
a
g
e
≥
65
}
.


Это позволит линейной модели по-разному обрабатывать несовершеннолетних, работающих людей и пенсионеров. Чаще границы отрезков дискретизации не подбирают вручную, а разбивают на равномерные отрезки. Такие алгоритмы, как 
решающие деревья
, способны самостоятельно извлекать подобные признаки, но им сложно аппроксимировать линейные комбинации над признаками, поэтому им на вход можно подавать эти комбинации в явном виде как разности и суммы исходных признаков. 
Метрические алгоритмы
 предсказывают отклик только на основе попарных расстояний между объектами. Поэтому им может подойти расширение числа признаков как их разумной дискретизацией, так и линейными комбинациями над признаками. В свою очередь, линейным моделям и решающим деревьям можно дополнительно подавать на вход расстояния от каждого объекта до некоторого набора характерных объектов выборки (эталонов). Например, это может быть расстояние до центров кластеров при предварительной 
кластеризации
 объектов.






В качестве признаков модели могут выступать даже прогнозы других моделей. Тогда итоговая модель будет учиться агрегировать прогнозы этих моделей наилучшим образом и будет называться 
ансамблем
 или 
композицией моделей
 (model ensemble). Чтобы избежать переобучения за счёт повторного переиспользования информации об откликах, базовые и агрегирующая разных модель должны обучаться на разных выборках, что реализуется в 
блендинге и стэкинге
.






Больше не всегда означает лучше!
Увеличение числа признаков повышает вероятность настроиться на 
ложную зависимость
 (false dependency) между одним из признаков и откликом, которая будет соответствовать не реальной зависимости, а случайному совпадению в данных. Например, если зафиксировать вектор откликов и по нему сгенерировать очень много случайных признаков из равномерного распределения, то некоторые из них, в силу случайности, окажутся сильно скоррелироваными с целевой величиной! Но это будет ложная зависимость, поскольку, в силу случайности, нельзя будет рассчитывать на подобную скоррелированность на новых тестовых данных.


Обзор частых приёмов генерации новых признаков с кодом реализации можно прочитать в 
[1]
.


Литература
​




Kaggle: a reference guide to feature engineering methods.


Предыдущая страница
Нормализация признаков
Следующая страница
Сокращение числа признаков
Подходы для генерации новых признаков
Литература
© 2023-25 
Виктор Китов.
 
Новости проекта.

