





Линейный ансамбль моделей | Машинное и глубокое обучение






[IMAGE]














Перейти к основному содержимому
[IMAGE]
Машинное обучение
Глубокое обучение
Обозначения
Лицензия
Машинное обучение
Введение
Основы машинного обучения
Подготовка данных
Классификаторы в общем виде
Метрические методы прогнозирования
Линейная регрессия и её обобщения
Линейная регрессия
Аналитическое решение для линейной регрессии
Регуляризация в линейной регрессии
Аналитическое решение для гребневой регрессии
Линейный ансамбль моделей
Регрессия опорных векторов
Orthogonal matching pursuit
Локально-линейная регрессия
Дополнительная литература
Вопросы
Оценка качества регрессии
Линейная классификация
Многоклассовая классификация набором бинарных классификаторов
Численная оптимизация
Оценка качества классификации
Решающие деревья
Переобучение и недообучение
Ансамбли моделей
Бустинг
Интерпретация простых моделей
Интерпретация сложных моделей
Заключение
Линейная регрессия и её обобщения
Линейный ансамбль моделей
Линейный ансамбль моделей


Для повышения точности прогнозов часто используют 
ансамбли моделей
 (model ensemble), называемые также 
композициями моделей
. Для этого используют прогнозы 
$M$
M
 базовых моделей 
$f_{1}\left(\mathbf{x}\right),...f_{M}\left(\mathbf{x}\right)$
f
1
​
(
x
)
,
...
f
M
​
(
x
)
, а итоговый прогноз строят через 
агрегирующую модель
 (называемую также 
мета-моделью
) 
$G(f_{1}\left(\mathbf{x}\right),...f_{M}\left(\mathbf{x}\right))$
G
(
f
1
​
(
x
)
,
...
f
M
​
(
x
)
)
. Таким образом, прогнозы базовых моделей выступают признаками для агрегирующей модели.


Часто используется линейная комбинация прогнозов различных моделей:


$\widehat{y}(\mathbf{x})=f\left(\mathbf{x}\right)^{T}\widehat{\mathbf{w}}=\hat{w}_1 f_1(\mathbf{x})+...+\hat{w}_M f_M(\mathbf{x})$
y
​
(
x
)
=
f
(
x
)
T
w
=
w
^
1
​
f
1
​
(
x
)
+
...
+
w
^
M
​
f
M
​
(
x
)


Веса 
$\mathbf{w}$
w
 этой модели настраиваются через линейную регрессию, признаками которой выступают прогнозы базовых моделей. Чтобы избежать переобучения, коэффициенты  настраиваются 
на отдельной обучающей выборке
, а не выборке, по которой настраивались параметры базовых моделей.




Построение прогнозов, агрегируя ансамбли моделей и используя при этом 
произвольную агрегирующую функцию
, будет впоследствии рассматриваться в 
алгоритме стэкнига и блендинга
.




Веса можно настраивать обычным способом, а можно ввести дополнительные требования из логики задачи:


$\begin{cases}
\sum_{n=1}^{N}\left(f\left(\mathbf{x}_{n}\right)^{T}\mathbf{w}-y_{n}\right)^{2}+\lambda\sum_{m=1}^{M}\left(w_{m}-\frac{1}{M}\right)^{2}\to\min_{\mathbf{w}}\\
w_{m}\ge 0,\quad m=1,2,...M.
\end{cases}$
⎩
⎨
⎧
​
∑
n
=
1
N
​
(
f
(
x
n
​
)
T
w
−
y
n
​
)
2
+
λ
∑
m
=
1
M
​
(
w
m
​
−
M
1
​
)
2
→
min
w
​
w
m
​
≥
0
,
m
=
1
,
2
,
...
M
.
​


Вместо классической L2-регуляризации мы приближаем веса к равномерным 
$(\frac{1}{M},\frac{1}{M},...\frac{1}{M})$
(
M
1
​
,
M
1
​
,
...
M
1
​
)
. Тогда даже для больших 
$\lambda$
λ
 модель будет сдвигаться к разумной стратегии - равномерному усреднению прогнозов базовых моделей, а не к константному нулю.


Добавление смещения
Если базовые модели систематически переоценивают или недооценивают прогноз, то в агрегирующую модель можно добавить смещение 
$w_0$
w
0
​
, которое мы не будем подвергать регуляризации.
Предыдущая страница
Аналитическое решение для гребневой регрессии
Следующая страница
Регрессия опорных векторов
© 2023-25 
Виктор Китов.
 
Новости проекта.

