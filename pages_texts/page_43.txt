





Локально-линейная регрессия | Машинное и глубокое обучение






[IMAGE]














Перейти к основному содержимому
[IMAGE]
Машинное обучение
Глубокое обучение
Обозначения
Лицензия
Машинное обучение
Введение
Основы машинного обучения
Подготовка данных
Классификаторы в общем виде
Метрические методы прогнозирования
Линейная регрессия и её обобщения
Линейная регрессия
Аналитическое решение для линейной регрессии
Регуляризация в линейной регрессии
Аналитическое решение для гребневой регрессии
Линейный ансамбль моделей
Регрессия опорных векторов
Orthogonal matching pursuit
Локально-линейная регрессия
Дополнительная литература
Вопросы
Оценка качества регрессии
Линейная классификация
Многоклассовая классификация набором бинарных классификаторов
Численная оптимизация
Оценка качества классификации
Решающие деревья
Переобучение и недообучение
Ансамбли моделей
Бустинг
Интерпретация простых моделей
Интерпретация сложных моделей
Заключение
Линейная регрессия и её обобщения
Локально-линейная регрессия
Содержание этой страницы
Локально-линейная регрессия


В традиционной линейной регрессии прогноз строится как линейная комбинация признаков:


$\hat{y}(\mathbf{x})=w_0+w_1 x^1+w_2 x^2+...+w_D x^D=w_0+\mathbf{w}^T \mathbf{x}, \tag{1}$
y
^
​
(
x
)
=
w
0
​
+
w
1
​
x
1
+
w
2
​
x
2
+
...
+
w
D
​
x
D
=
w
0
​
+
w
T
x
,
(
1
)


где веса находятся 
из принципа минимизации наименьших квадратов
:


$\sum_{n=1}^N (w_0+\mathbf{w}^T \mathbf{x}_n-y_n)^2\to\min_{w_0,\mathbf{w}}$
n
=
1
∑
N
​
(
w
0
​
+
w
T
x
n
​
−
y
n
​
)
2
→
w
0
​
,
w
min
​


Обратим внимание, что формула (1) предполагает глобальную линейную связь между признаками и откликами. Но как быть, если реальная зависимость нелинейна? Один из вариантов - добавлять нелинейные трансформации в число признаков. Другой подход - использовать линейную зависимость, но 
со своими коэффициентами для каждого объекта
 
$\mathbf{x}$
x
, что реализуется в алгоритме 
локально-линейной регрессии
 (local linear regression, locally weighted scatterplot smoothing, LOWESS 
[1]
), в которой прогноз 
$\hat{y}(\mathbf{x})$
y
^
​
(
x
)
 также строится по формуле (1), однако параметры 
$w_0,\mathbf{w}$
w
0
​
,
w
 находятся по объектам, лежащим недалеко от 
$\mathbf{x}$
x
, за счёт минимизации 
взвешенной
 суммы квадратов ошибок:


$\sum_{n=1}^N \alpha_n(\mathbf{x})(w_0+\mathbf{w}^T \mathbf{x}_n-y_n)^2\to\min_{w_0,\mathbf{w}} \tag{2}$
n
=
1
∑
N
​
α
n
​
(
x
)
(
w
0
​
+
w
T
x
n
​
−
y
n
​
)
2
→
w
0
​
,
w
min
​
(
2
)


Как видим, объекты 
$(\mathbf{x}_1,y_1),(\mathbf{x}_2,y_2),...(\mathbf{x}_N,y_N)$
(
x
1
​
,
y
1
​
)
,
(
x
2
​
,
y
2
​
)
,
...
(
x
N
​
,
y
N
​
)
 учитываются с весами 
$\alpha_1,\alpha_2,...\alpha_N$
α
1
​
,
α
2
​
,
...
α
N
​
, где вес каждого объекта определяется 
близостью
 к прогнозируемому объекту 
$\mathbf{x}$
x
: чем он ближе, тем его вес больше. Это позволяет вычислять коэффициенты линейной регрессии 
адаптивно к той точке, в которой нужно построить прогноз
. В другой точке 
$\mathbf{x}$
x
 зависимость также будет линейной, но уже с другими коэффициентами.




Поэтому прогнозная функция для всевозможных 
$\mathbf{x}$
x
 уже будет получаться 
нелинейной
.




Веса 
$\alpha_n(\mathbf{x})\ge 0$
α
n
​
(
x
)
≥
0
 вычисляются по тем же формулам, что и веса 
локально-постоянной регрессии
.


По сравнению с 
локально-постоянной регрессией
, локально-линейный вариант более вычислительно трудоёмкий, поскольку необходимо заново находить минимум (2) для каждого тестового объекта 
$\mathbf{x}$
x
. Зато он более гибкий. В частности локально-линейная регрессия лучше экстраполирует зависимости в областях, где обучающих примеров мало, как показано на рисунке по краям:


[IMAGE]


Литература
​




Wikipedia: local regression.


Предыдущая страница
Orthogonal matching pursuit
Следующая страница
Дополнительная литература
Литература
© 2023-25 
Виктор Китов.
 
Новости проекта.

