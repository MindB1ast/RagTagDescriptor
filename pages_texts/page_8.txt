





Связь с принципом максимального правдоподобия | Машинное и глубокое обучение






[IMAGE]














Перейти к основному содержимому
[IMAGE]
Машинное обучение
Глубокое обучение
Обозначения
Лицензия
Машинное обучение
Введение
Основы машинного обучения
Обучение с учителем
Настройка параметров модели
Выпуклость потерь
Регуляризация модели
Взвешенный учёт наблюдений
Связь с принципом максимального правдоподобия
Обобщающая способность
Оценка качества прогнозов
Этапы решения задачи машинного обучения
Обучение без учителя
Частичное обучение
Вопросы
Подготовка данных
Классификаторы в общем виде
Метрические методы прогнозирования
Линейная регрессия и её обобщения
Оценка качества регрессии
Линейная классификация
Многоклассовая классификация набором бинарных классификаторов
Численная оптимизация
Оценка качества классификации
Решающие деревья
Переобучение и недообучение
Ансамбли моделей
Бустинг
Интерпретация простых моделей
Интерпретация сложных моделей
Заключение
Основы машинного обучения
Связь с принципом максимального правдоподобия
Содержание этой страницы
Связь с принципом максимального правдоподобия


Многие модели машинного обучения 
$f_\mathbf{w}(\mathbf{x})$
f
w
​
(
x
)
 могут сопоставлять целевой величине 
вероятностное распределение
 (probability distribution).


В случае задачи классификации - это вероятности каждого из классов при условии объекта 
$\mathbf{x}$
x
:


$[\hat{p}_\mathbf{w}(y=1|\mathbf{x}),\hat{p}_\mathbf{w}(y=2|\mathbf{x}),...\hat{p}_\mathbf{w}(y=C|\mathbf{x})]$
[
p
^
​
w
​
(
y
=
1∣
x
)
,
p
^
​
w
​
(
y
=
2∣
x
)
,
...
p
^
​
w
​
(
y
=
C
∣
x
)]


В случае задачи регрессии предсказывается условная 
плотность вероятностного распределения
 
$\hat{p}_\mathbf{w}(y|x)$
p
^
​
w
​
(
y
∣
x
)
, позволяющая рассчитать, с какой вероятностью вещественная величина принадлежит каждому интервалу.


В этих случаях модель машинного обучения представляет собой 
вероятностную модель
. При предположении, что объекты выборки распределены 
независимо
,  вероятность пронаблюдать ответы на всей выборке 
$(X,Y)$
(
X
,
Y
)
 факторизуется в произведение вероятностей пронаблюдать ответ на каждом объекте выборки:


$\hat{p}_\mathbf{w}(Y|X)=\prod_{n=1}^N \hat{p}_\mathbf{w}(y_n|\mathbf{x}_n)$
p
^
​
w
​
(
Y
∣
X
)
=
n
=
1
∏
N
​
p
^
​
w
​
(
y
n
​
∣
x
n
​
)


Задача регрессии
В случае задачи регрессии получается аналогичная факторизация, но состоящая не из вероятностей отдельных классов, а из значений условной 
плотности
.


Принцип максимума правдоподобия
 (maximum likelihood estimation) назначает такие параметры модели 
$\hat{\mathbf{w}}$
w
^
, которые бы максимизировали вероятность пронаблюдать верные отклики на всех объектах обучающей выборки. Таким образом, параметры вероятностной модели находятся из условия


$\hat{\mathbf{w}} = \arg\max_\mathbf{w} \prod_{n=1}^N \hat{p}_\mathbf{w}(y_n|\mathbf{x}_n)$
w
^
=
ar
g
w
max
​
n
=
1
∏
N
​
p
^
​
w
​
(
y
n
​
∣
x
n
​
)


Вычислительно неудобно максимизировать произведение большого количества малых чисел (получим машинный ноль), поэтому на практике максимизируют средний логарифм правдоподобия, дающий тот же самый результат:


$\hat{\mathbf{w}} = \arg\max_\mathbf{w} \frac{1}{N}\sum_{n=1}^N \log \hat{p}_\mathbf{w}(y_n|\mathbf{x}_n)$
w
^
=
ar
g
w
max
​
N
1
​
n
=
1
∑
N
​
lo
g
p
^
​
w
​
(
y
n
​
∣
x
n
​
)


Это очень похоже на принцип минимизации эмпирического риска, согласно которому параметры модели должны находиться из условия:


$\hat{\mathbf{w}}=\arg\min_\mathbf{w}\frac{1}{N}\sum_{n=1}^{N}\mathcal{L}(f_{\mathbf{w}}(\mathbf{x}_{n}),\,y_{n})$
w
^
=
ar
g
w
min
​
N
1
​
n
=
1
∑
N
​
L
(
f
w
​
(
x
n
​
)
,
y
n
​
)


Отсюда видна взаимосвязь принципа минимизации эмпирического риска и принципа максимального правдоподобия. При выборке такой функции потерь и прогнозирующей функции, что


$\mathcal{L}(f_{\mathbf{w}}(\mathbf{x_n}),y_{n})=-\log \hat{p}_\mathbf{w}(y_n|\mathbf{x}_n),$
L
(
f
w
​
(
x
n
​
)
,
y
n
​
)
=
−
lo
g
p
^
​
w
​
(
y
n
​
∣
x
n
​
)
,


модель машинного обучения настраивается точно так же, как соответствующая вероятностная модель. Верно и в обратную сторону. Если вероятностную модель выбрать таким образом, что равенство выше выполняется, то модель машинного обучения можно эквивалентно описать вероятностной моделью. Знак минус взят, поскольку в одном случае целевой функционал нужно 
максимизировать
, а в другом - 
минимизировать
.


Вы также можете прочитать о связи принципов минимизации эмпирического риска и принципа максимума правдоподобия при использовании разных типов регуляризации в 
[1]
.


Литература
​




Воронцов К. В. Математические методы обучения по прецедентам (теория обучения машин) // Москва. – 2011.


Предыдущая страница
Взвешенный учёт наблюдений
Следующая страница
Обобщающая способность
Литература
© 2023-25 
Виктор Китов.
 
Новости проекта.

