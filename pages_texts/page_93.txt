





Математическое обоснование ансамблей | Машинное и глубокое обучение






[IMAGE]














Перейти к основному содержимому
[IMAGE]
Машинное обучение
Глубокое обучение
Обозначения
Лицензия
Машинное обучение
Введение
Основы машинного обучения
Подготовка данных
Классификаторы в общем виде
Метрические методы прогнозирования
Линейная регрессия и её обобщения
Оценка качества регрессии
Линейная классификация
Многоклассовая классификация набором бинарных классификаторов
Численная оптимизация
Оценка качества классификации
Решающие деревья
Переобучение и недообучение
Ансамбли моделей
Ансамбли моделей
Математическое обоснование ансамблей
Простая агрегация в ансамблях
Методы построения базовых моделей
Настройка на разных фрагментах обучающей выборки
Ансамбли рандомизированных деревьев
Стэкинг
Дополнительная литература
Вопросы
Бустинг
Интерпретация простых моделей
Интерпретация сложных моделей
Заключение
Ансамбли моделей
Математическое обоснование ансамблей
Содержание этой страницы
Математическое обоснование ансамблей


Уточнение прогнозов при равномерном усреднении
​


Рассмотрим задачу регрессии с усредняющим ансамблем:


$\widehat{y}(\mathbf{x})=\frac{1}{M}\sum_{m=1}^{M}f_{m}(\mathbf{x})$
y
​
(
x
)
=
M
1
​
m
=
1
∑
M
​
f
m
​
(
x
)


Пусть 
$\varepsilon_{1},...\varepsilon_{M}$
ε
1
​
,
...
ε
M
​
 - ошибки прогнозирования базовыми моделями 
$f_{1}(\mathbf{x}),...f_{M}(\mathbf{x})$
f
1
​
(
x
)
,
...
f
M
​
(
x
)
 со средним ноль, дисперсией 
$\mathbb{D}\varepsilon_{i}=\sigma^{2}$
D
ε
i
​
=
σ
2
, и пусть эти ошибки имеют попарную корреляцию 
$\rho$
ρ
. Тогда ковариации ошибок будут:


$\mathbb{E}\varepsilon_{i}\varepsilon_{j}=\rho\sigma^{2},$
E
ε
i
​
ε
j
​
=
ρ
σ
2
,


а ожидаемый квадрат ошибки отдельной базовой модели:


$\mathbb{E}\left\{ \left(f_{i}(\mathbf{x})-y(\mathbf{x})\right)^{2}\right\} =\mathbb{E}\varepsilon_{i}^{2}=\sigma^{2}, \quad i=1,2,...M$
E
{
(
f
i
​
(
x
)
−
y
(
x
)
)
2
}
=
E
ε
i
2
​
=
σ
2
,
i
=
1
,
2
,
...
M


Можно показать, что ожидаемый квадрат ошибки усредняющего ансамбля будет следующим:


$\mathbb{E}\left\{ \left(\widehat{y}(\mathbf{x})-y(\mathbf{x})\right)^{2}\right\} = \frac{\sigma^{2}}{M}+\left(1-\frac{1}{M}\right)\rho\sigma^{2}$
E
{
(
y
​
(
x
)
−
y
(
x
)
)
2
}
=
M
σ
2
​
+
(
1
−
M
1
​
)
ρ
σ
2


Доказательство
$\begin{aligned}\mathbb{E}\left\{ \left(\widehat{y}(\mathbf{x})-y(\mathbf{x})\right)^{2}\right\}  & =\mathbb{E}\left\{ \left(\frac{\sum_{m=1}^{M}\left(f_{m}(\mathbf{x})-y(\mathbf{x})\right)}{M}\right)^{2}\right\} \\
 & =\frac{1}{M^{2}}\mathbb{E}\left\{ \left(\sum_{m=1}^{M}\varepsilon_{m}\right)^{2}\right\} \\
 & =\frac{1}{M^{2}}\mathbb{E}\left\{ \sum_{m=1}^{M}\varepsilon_{m}^{2}+\sum_{i\ne j}\varepsilon_{i}\varepsilon_{j}\right\} \\
 & =\frac{M}{M^{2}}\sigma^{2}+\frac{M^{2}-M}{M^{2}}\rho\sigma^{2}\\
 & =\frac{\sigma^{2}}{M}+\left(1-\frac{1}{M}\right)\rho\sigma^{2}
\end{aligned}$
E
{
(
y
​
(
x
)
−
y
(
x
)
)
2
}
​
=
E
⎩
⎨
⎧
​
(
M
∑
m
=
1
M
​
(
f
m
​
(
x
)
−
y
(
x
)
)
​
)
2
⎭
⎬
⎫
​
=
M
2
1
​
E
⎩
⎨
⎧
​
(
m
=
1
∑
M
​
ε
m
​
)
2
⎭
⎬
⎫
​
=
M
2
1
​
E
⎩
⎨
⎧
​
m
=
1
∑
M
​
ε
m
2
​
+
i

=
j
∑
​
ε
i
​
ε
j
​
⎭
⎬
⎫
​
=
M
2
M
​
σ
2
+
M
2
M
2
−
M
​
ρ
σ
2
=
M
σ
2
​
+
(
1
−
M
1
​
)
ρ
σ
2
​


Анализ
​


При 
$\rho=0$
ρ
=
0
 ожидаемый квадрат ошибки ансамбля будет в 
$M$
M
 раз меньше, чем квадрат ошибки отдельной базовой модели, а при 
$\rho<0$
ρ
<
0
 он может быть ещё меньше за счёт рассогласованности прогнозов и взаимной компенсации ошибок разных моделей!


При максимальной скоррелированности ошибок (
$\rho=1$
ρ
=
1
) ожидаемо получим, что квадрат ошибки ансамбля совпадёт с квадратом ошибки отдельной базовой модели, и использование ансамбля преимущества не даст.


На практике базовые модели будут различными, а их прогнозы в общем случае будут обладать положительной корреляцией 
$\rho\in (0,1)$
ρ
∈
(
0
,
1
)
, поэтому квадрат ошибки ансамбля будет 
меньше
, чем квадрат ошибки одной базовой модели.




Положительная скоррелированность вызвана тем, что эти базовые модели будут обучаться предсказывать одну и ту же целевую переменную на одной и той же обучающей выборке. Чтобы уменьшить корреляцию, рекомендуется выбирать базовые модели из разных классов (линейные, метрические, решающие деревья).




Уточнение прогнозов при неравномерном усреднении
​


Рассмотрим задачу регрессии, решаемую ансамблем


$G(\mathbf{x})=\sum_{m=1}^{M}w_{m}f_{m}(\mathbf{x}),$
G
(
x
)
=
m
=
1
∑
M
​
w
m
​
f
m
​
(
x
)
,


где 
$w_{m}\ge 0, \sum_{m=1}^M w_{m}=1$
w
m
​
≥
0
,
∑
m
=
1
M
​
w
m
​
=
1
 - веса, с которыми мы учитываем базовые модели.


В простейшем случае можно взять равномерное усреднение:


$w_1=w_2=...=w_M=\frac{1}{M},$
w
1
​
=
w
2
​
=
...
=
w
M
​
=
M
1
​
,


однако взвешенный учёт позволяет 
более точным базовым моделям назначать больший вес
.


Результат выше (для ожидаемого квадрата ошибки) можно обобщить и на случай взвешенного учёта базовых моделей (выведите!) с обоснованием эффективности ансамблей. Однако здесь мы продемонстрируем другое иллюстративное разложение квадрата ошибки, справедливое даже не для мат. ожидания, а для отдельного объекта 
$(\mathbf{x},y)$
(
x
,
y
)
, называемое 
разложением неоднозначности
 (ambiguity decomposition 
[1]
):


$\underset{\text{ошибка ансамбля}}{\underbrace{\left(G(\mathbf{x})-y\right)^{2}}}=\underset{\text{ошибка базовых моделей}}{\underbrace{\sum_{m}w_{m}\left(f_{m}(\mathbf{x})-y\right)^{2}}}-\underset{\text{неоднозначность}}{\underbrace{\sum_{m}w_{m}\left(f_{m}(\mathbf{x})-G(\mathbf{x})\right)^{2}}}$
ошибка
 
ансамбля
(
G
(
x
)
−
y
)
2
​
​
=
ошибка
 
базовых
 
моделей
m
∑
​
w
m
​
(
f
m
​
(
x
)
−
y
)
2
​
​
−
неоднозначность
m
∑
​
w
m
​
(
f
m
​
(
x
)
−
G
(
x
)
)
2
​
​


Доказательство
$\begin{gather*}
\sum_{m}w_{m}\left(f_{m}(\mathbf{x})-G(\mathbf{x})\right)^{2}=\sum_{m}w_{m}\left(f_{m}(\mathbf{x})-y+y-G(\mathbf{x})\right)^{2}\\
=\sum_{m}w_{m}\left(f_{m}(\mathbf{x})-y\right)^{2}+\sum_{m}w_{m}\left(y-G(\mathbf{x})\right)^{2}+2\sum_{m}w_{m}\left(f_{m}(\mathbf{x})-y\right)\left(y-G(\mathbf{x})\right) \\
=\sum_{m}w_{m}\left(f_{m}(\mathbf{x})-y\right)^{2}+\left(G(\mathbf{x})-y\right)^{2}+2\left(y-G(\mathbf{x})\right)\sum_{m}w_{m}\left(f_{m}(\mathbf{x})-y\right)\\
=\sum_{m}w_{m}\left(f_{m}(\mathbf{x})-y\right)^{2}+\left(G(\mathbf{x})-y\right)^{2}+2\left(y-G(\mathbf{x})\right)\left(G(\mathbf{x})-y\right)\\
=\sum_{m}w_{m}\left(f_{m}(\mathbf{x})-y\right)^{2}+\left(G(\mathbf{x})-y\right)^{2}-2\left(G(\mathbf{x})-y\right)^{2}\\
=\sum_{m}w_{m}\left(f_{m}(\mathbf{x})-y\right)^{2}-\left(G(\mathbf{x})-y\right)^{2} 
\end{gather*}$
m
∑
​
w
m
​
(
f
m
​
(
x
)
−
G
(
x
)
)
2
=
m
∑
​
w
m
​
(
f
m
​
(
x
)
−
y
+
y
−
G
(
x
)
)
2
=
m
∑
​
w
m
​
(
f
m
​
(
x
)
−
y
)
2
+
m
∑
​
w
m
​
(
y
−
G
(
x
)
)
2
+
2
m
∑
​
w
m
​
(
f
m
​
(
x
)
−
y
)
(
y
−
G
(
x
)
)
=
m
∑
​
w
m
​
(
f
m
​
(
x
)
−
y
)
2
+
(
G
(
x
)
−
y
)
2
+
2
(
y
−
G
(
x
)
)
m
∑
​
w
m
​
(
f
m
​
(
x
)
−
y
)
=
m
∑
​
w
m
​
(
f
m
​
(
x
)
−
y
)
2
+
(
G
(
x
)
−
y
)
2
+
2
(
y
−
G
(
x
)
)
(
G
(
x
)
−
y
)
=
m
∑
​
w
m
​
(
f
m
​
(
x
)
−
y
)
2
+
(
G
(
x
)
−
y
)
2
−
2
(
G
(
x
)
−
y
)
2
=
m
∑
​
w
m
​
(
f
m
​
(
x
)
−
y
)
2
−
(
G
(
x
)
−
y
)
2
​


Из этого разложения видно, что ансамбль даёт точные прогнозы, когда базовые модели достаточно точны. Но даже если они не точны, мы всё равно имеем возможность уменьшить квадрат ошибки за счёт использования базовых моделей, дающих 
более рассогласованные прогнозы
 с высоким значением неоднозначности. Отсюда следует, как и раньше, что выгодно усреднять как можно более непохожие друг на друга базовые модели: модели из разных классов с разными значениями гиперпараметров!




Также при настройке новой модели, помимо критерия средних потерь, можно включать регуляризацию, 
в явном виде поощряющую рассогласованность прогнозов
 базовых моделей. Например, в 
[2]
 предложено обучать несколько моделей, штрафуя не только квадраты отклонений от целевой переменной, но и корреляции между прогнозами.




Классификация
​


Рассмотрим задачу бинарной классификации базовыми моделями 
$f_{1}(\mathbf{x}),...f_{M}(\mathbf{x})$
f
1
​
(
x
)
,
...
f
M
​
(
x
)
. Пусть для каждой базовой модели вероятность предсказать правильный класс равна 
$p>0.5$
p
>
0.5
. Предположим, что модели ошибаются независимо друг от друга, а агрегирующая модель 
$G(\cdot)$
G
(
⋅
)
 - 
голосование по большинству
 (majority voting), в котором итоговым классом назначается тот класс, за который проголосовало большинство базовых моделей.


При указанных предположениях оказывается, что, увеличивая число базовых моделей, вероятность ошибки можно сделать сколь угодно малой:


$p(G(\mathbf{x})\ne y)\to 0 \text{ при } M\to\infty$
p
(
G
(
x
)

=
y
)
→
0
 
при
 
M
→
∞




Это переформулировка теоремы Кондорсе 
[3]
 о присяжных в терминах машинного обучения.




Доказательство
Рассмотрим случайную величину 
$\xi_m\in \{0,1\}$
ξ
m
​
∈
{
0
,
1
}
, принимающую значение 1, если m-й классификатор верно угадал класс, и значение 0, если неверно.
Тогда среднее этих случайных величин 
$\eta=\frac{\xi_{1}+...+\xi_{M}}{M}$
η
=
M
ξ
1
​
+
...
+
ξ
M
​
​
 будет больше 0.5 тогда и только тогда, когда прогноз всего ансамбля будет верным, поскольку это условие характеризует ситуацию, когда большинство базовых классификаторов не ошиблись и назначили верный класс. Вероятность верного прогноза для всего ансамбля будет 
$P(\eta>0.5)$
P
(
η
>
0.5
)
.
По центральной предельной теореме
$\begin{gather*}  
\frac{\sum_{i=1}^{M}\left[\xi_{i}-\mathbb{E}\xi_{i}\right]}{\sqrt{M\mathbb{D}(\xi_{1})}}=\frac{\sum_{i=1}^{M}\left[\xi_{i}-p\right]}{\sqrt{Mp(1-p)}}=\\
\frac{\sqrt{M}}{\sqrt{p(1-p)}}\frac{\sum_{i=1}^{M}\xi_{i}-Mp}{M}=\frac{\sqrt{M}}{\sqrt{p(1-p)}}(\eta-p)\to\mathcal{N}(0,1)
\end{gather*}$
M
D
(
ξ
1
​
)
​
∑
i
=
1
M
​
[
ξ
i
​
−
E
ξ
i
​
]
​
=
Mp
(
1
−
p
)
​
∑
i
=
1
M
​
[
ξ
i
​
−
p
]
​
=
p
(
1
−
p
)
​
M
​
​
M
∑
i
=
1
M
​
ξ
i
​
−
Mp
​
=
p
(
1
−
p
)
​
M
​
​
(
η
−
p
)
→
N
(
0
,
1
)
​
Поэтому 
$\eta\to\mathcal{N}(p,\frac{p(1-p)}{M})$
η
→
N
(
p
,
M
p
(
1
−
p
)
​
)
 при 
$M\to\infty$
M
→
∞
.
Обратим внимание, что  
$\mathbb{E}\eta=p$
E
η
=
p
 и 
$\mathbb{D}[\eta]=\frac{p(1-p)}{M}\to0 \text{ при } M\to\infty$
D
[
η
]
=
M
p
(
1
−
p
)
​
→
0
 
при
 
M
→
∞
, откуда следует, что 
$P(\eta>0.5)\to 1 \text{ при } M\to\infty$
P
(
η
>
0.5
)
→
1
 
при
 
M
→
∞
 по 
неравенству Чебышева
.
Задача
Сформулируйте и докажите это утверждение для многоклассового случая.


Отметим, что мы потребовали, чтобы базовые модели угадывали верный класс при 
$p>0.5$
p
>
0.5
. Например, при 
$p=0.51$
p
=
0.51
. Для бинарного случая это означает, что базовые классификаторы могут быть лишь немного лучше случайного угадывания для получения сколь угодно точного прогноза ансамблем!


В чём проблема этого достичь на практике?
На практике ошибки классификаторов нельзя считать независимыми - они пытаются предсказывать одну и ту же целевую величину и настраиваются по одинаковой обучающей выборке. Но мы, тем не менее, можем снизить их зависимость, если будем брать базовые классификаторы из разных классов или с разными конфигурациями гиперпараметров.


Литература
​






Krogh A., Vedelsby J. Neural network ensembles, cross validation, and active learning //Advances in neural information processing systems. – 1994. – Т. 7.






Liu Y., Yao X. Ensemble learning via negative correlation //Neural networks. – 1999. – Т. 12. – №. 10. – С. 1399-1404.






Викиконспекты ИТМО: виды ансамблей.




Предыдущая страница
Ансамбли моделей
Следующая страница
Простая агрегация в ансамблях
Уточнение прогнозов при равномерном усреднении
Анализ
Уточнение прогнозов при неравномерном усреднении
Классификация
Литература
© 2023-25 
Виктор Китов.
 
Новости проекта.

