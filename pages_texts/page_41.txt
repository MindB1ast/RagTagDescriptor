





Регрессия опорных векторов | Машинное и глубокое обучение






[IMAGE]














Перейти к основному содержимому
[IMAGE]
Машинное обучение
Глубокое обучение
Обозначения
Лицензия
Машинное обучение
Введение
Основы машинного обучения
Подготовка данных
Классификаторы в общем виде
Метрические методы прогнозирования
Линейная регрессия и её обобщения
Линейная регрессия
Аналитическое решение для линейной регрессии
Регуляризация в линейной регрессии
Аналитическое решение для гребневой регрессии
Линейный ансамбль моделей
Регрессия опорных векторов
Orthogonal matching pursuit
Локально-линейная регрессия
Дополнительная литература
Вопросы
Оценка качества регрессии
Линейная классификация
Многоклассовая классификация набором бинарных классификаторов
Численная оптимизация
Оценка качества классификации
Решающие деревья
Переобучение и недообучение
Ансамбли моделей
Бустинг
Интерпретация простых моделей
Интерпретация сложных моделей
Заключение
Линейная регрессия и её обобщения
Регрессия опорных векторов
Содержание этой страницы
Регрессия опорных векторов


Ранее мы везде настраивали коэффициенты 
линейной регрессии
 минимизируя квадрат ошибки:


$\mathcal{L}(\hat{y},y)=(\hat{y}-y)^2$
L
(
y
^
​
,
y
)
=
(
y
^
​
−
y
)
2


Но их можно настраивать, используя и другие функции потерь, более соответствующие смыслу задачи!


Например, когда для нас несущественны небольшие отклонения в пределах 
$\varepsilon>0$
ε
>
0
, а сверх этого потери возрастают линейно, то можно использовать для настройки 
$\varepsilon$
ε
-нечувствительную функцию потерь (
$\varepsilon$
ε
-insensitive loss):


$\mathcal{L}(\hat{y},y)=
\begin{cases}
   0, & \text{если } |\hat{y}-y|\le \varepsilon \\
   |\hat{y}-y|-\varepsilon, & \text{если } |\hat{y}-y| > \varepsilon 
\end{cases}$
L
(
y
^
​
,
y
)
=
{
0
,
∣
y
^
​
−
y
∣
−
ε
,
​
если
 
∣
y
^
​
−
y
∣
≤
ε
если
 
∣
y
^
​
−
y
∣
>
ε
​


Регрессия опорных векторов
 (support vector regression) - это линейная регрессия, веса которой настраиваются, используя 
$\varepsilon$
ε
-нечувствительную функцию потерь с L2-регуляризацией. На графике ниже показан пример настроенной регрессии опорных векторов по точкам (слева) и 
$\varepsilon$
ε
-нечувствительная функция потерь (справа):


[IMAGE]


После настройки метода все объекты обучающей выборки разделятся на опорные (support vectors) и неинформативные (non-informative vectors).


Опорные объекты (выше обозначены желтыми кружками):






прогнозируются с ошибкой по модулю больше или равной 
$\varepsilon$
ε
;






влияют
 на наклон прогнозирующей гиперплоскости .






В свою очередь, неинформативные объекты (обозначены зелёными кружками):






прогнозируются с ошибкой по модулю меньше 
$\varepsilon$
ε
;






не влияют
 на наклон прогнозирующей гиперплоскости, поскольку на них функция потерь равна нулю.






Именно опорные объекты определяют решение задачи. Если же исключить неинформативные объекты из выборки, то итоговое решение не поменяется, поскольку потери на них равны нулю.




Если число опорных объектов невелико (а его можно контролировать, изменяя 
$\varepsilon$
ε
), то можно интерпретировать модель, анализируя объекты, повлиявшие на решение.




Обобщение метода (kernel trick)
Оптимизационную задачу для регрессии опорных векторов и формулу для построения прогноза можно переформулировать так, что прогноз будет зависеть только от скалярных произведений между векторами 
$\lang \mathbf{x},\mathbf{x}'\rang$
⟨
x
,
x
′
⟩
, а не от самих векторов 
$\mathbf{x},\mathbf{x}'$
x
,
x
′
. Методы, для которых выполнено это свойство, можно обобщить через ядра (применить так называемый 
kernel trick
 
[1]
), заменив все скалярные произведения на другую функцию, удовлетворяющую ряду свойств и называемую 
ядром Мерсера
:
$\lang \mathbf{x},\mathbf{x}'\rang \to K(\mathbf{x},\mathbf{x}').$
⟨
x
,
x
′
⟩
→
K
(
x
,
x
′
)
.
Регрессию опорных векторов таким образом можно обобщить и превратить из линейного метода в нелинейный!


Более детально с регрессией опорных векторов, её обобщениями и реализациями алгоритмов настройки параметров вы можете ознакомиться в 
[1]
.


Литература
​






Wikipedia: kernel method.






Smola A. J., Schölkopf B. A tutorial on support vector regression //Statistics and computing. – 2004. – Т. 14. – С. 199-222.




Предыдущая страница
Линейный ансамбль моделей
Следующая страница
Orthogonal matching pursuit
Литература
© 2023-25 
Виктор Китов.
 
Новости проекта.

