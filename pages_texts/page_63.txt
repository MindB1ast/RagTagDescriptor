





Мониторинг сходимости | Машинное и глубокое обучение






[IMAGE]














Перейти к основному содержимому
[IMAGE]
Машинное обучение
Глубокое обучение
Обозначения
Лицензия
Машинное обучение
Введение
Основы машинного обучения
Подготовка данных
Классификаторы в общем виде
Метрические методы прогнозирования
Линейная регрессия и её обобщения
Оценка качества регрессии
Линейная классификация
Многоклассовая классификация набором бинарных классификаторов
Численная оптимизация
Численные методы оптимизации
Метод градиентного спуска
Метод стохастического градиентного спуска
Мониторинг сходимости
Стохастический градиентный спуск с инерцией
Метод Ньютона
Вопросы
Оценка качества классификации
Решающие деревья
Переобучение и недообучение
Ансамбли моделей
Бустинг
Интерпретация простых моделей
Интерпретация сложных моделей
Заключение
Численная оптимизация
Мониторинг сходимости
Содержание этой страницы
Мониторинг сходимости


Для анализа сходимости метода 
стохастического градиентного спуска
 важно смотреть на динамику потерь на каждой итерации алгоритма. Особенно это важно при долгой и трудоёмкой настройке нейросетей,  чтобы на ранней стадии оптимизации увидеть некорректную настройку определённых параметров. Поскольку в стохастическом градиентном спуске сдвиг весов производится на антиградиент по случайному минибатчу объектов, то и величина этого сдвига будет подвержена случайным колебаниям, как в примере ниже:


[IMAGE]


Для большей наглядности нам хотелось бы отслеживать сглаженную версию этой динамики, показанную зелёной кривой. Для этого существуют два подхода - скользящее среднее и экспоненциальное сглаживание. Оба метода на вход принимают зашумлённый временной ряд 
$z_t$
z
t
​
 (в нашем случае - потерь на объектах минибатча), а на выходе выдают его сглаженную версию 
$s_t$
s
t
​
, причем сглаживание осуществляется динамически в каждый момент времени.


Скользящее среднее
​


Идея 
скользящего среднего
 заключается в выдаче усреднения по 
$K$
K
 последним наблюдениям:


$s_t=\frac{1}{K}\sum_{i=t-K+1}^t z_t$
s
t
​
=
K
1
​
i
=
t
−
K
+
1
∑
t
​
z
t
​


Это среднее можно эффективно пересчитывать по формуле


$s_t:=s_{t-1}+\frac{z_t}{K}-\frac{z_{t-K}}{K}$
s
t
​
:=
s
t
−
1
​
+
K
z
t
​
​
−
K
z
t
−
K
​
​


Вначале, пока 
$K$
K
 наблюдений еще не накоплены, нужно усреднять по всем располагаемым наблюдениям.


Экспоненциальное сглаживание
​


Экспоненциальное сглаживание
 вычисляет сглаженную версию временного ряда по следующей формуле:


$\begin{cases}
s_{1}=z_{1} & \\ 
s_{t}=(1-\alpha)z_{t}+\alpha s_{t-1} & 
\end{cases}$
{
s
1
​
=
z
1
​
s
t
​
=
(
1
−
α
)
z
t
​
+
α
s
t
−
1
​
​
​


Гиперпараметр 
$\alpha\in [0,1)$
α
∈
[
0
,
1
)
 управляет степенью сглаживания.


Как именно 
$\alpha$
α
 влияет на результат?
Увеличение 
$\alpha$
α
 приводит к более слабому учёту новых данных и к более сильному - исторических. Поэтому сглаженный временной ряд 
$s_t$
s
t
​
 будет получаться более гладким. Уменьшение 
$\alpha$
α
 уменьшает сглаживание. В частности, при 
$\alpha=0$
α
=
0
, сглаженный ряд совпадает с исходным.


Если рекуррентно переписать зависимость 
$s_t$
s
t
​
 только от 
$z_t,z_{t-1},z_{t-2},...$
z
t
​
,
z
t
−
1
​
,
z
t
−
2
​
,
...
, то получим, что экспоненциальное сглаживание выдаёт взвешенное усреднение по всем прошлым наблюдениям с экспоненциально убывающими весами:


$s_t = (1-\alpha)(z_{t}+\alpha z_{t-1}+\alpha^{2}z_{t-2}+\alpha^{3}z_{t-3}+...)$
s
t
​
=
(
1
−
α
)
(
z
t
​
+
α
z
t
−
1
​
+
α
2
z
t
−
2
​
+
α
3
z
t
−
3
​
+
...
)


Более детально об экспоненциальном сглаживании и его связи со скользящим средним можно прочитать в 
[1]
.


Литература
​




Wikipedia: exponential smoothing.


Предыдущая страница
Метод стохастического градиентного спуска
Следующая страница
Стохастический градиентный спуск с инерцией
Скользящее среднее
Экспоненциальное сглаживание
Литература
© 2023-25 
Виктор Китов.
 
Новости проекта.

